分类号 编号
U D C 密级
本科生毕业设计（论文）
题 目： 面向对抗鲁棒性的深度学习
可视化解释分析框架
姓 名： 朱旭飞
学 号： 12011432
系 别： 计算机科学与工程系
专 业： 计算机科学与技术
指导教师： 马昱欣 副教授
2024 年 6 月 7 日
诚信承诺书
1. 本人郑重承诺所呈交的毕业设计（论文），是在导师的指导下，
独立进行研究工作所取得的成果，所有数据、图片资料均真实可靠。
2. 除文中已经注明引用的内容外，本论文不包含任何其他人或
集体已经发表或撰写过的作品或成果。对本论文的研究作出重要贡
献的个人和集体，均已在文中以明确的方式标明。
3. 本人承诺在毕业论文（设计）选题和研究内容过程中没有抄袭
他人研究成果和伪造相关数据等行为。
4. 在毕业论文（设计）中对侵犯任何方面知识产权的行为，由本
人承担相应的法律责任。
作者签名:
2024 年 6 月 7 日
面向对抗鲁棒性的深度学习
可视化解释分析框架
朱旭飞
（计算机科学与工程系 指导教师：马昱欣）
[摘要]：对抗训练是保护深度学习模型抵御对抗攻击、提升模型鲁棒性
的一种极为有效的策略。然而，众多研究发现，尽管对抗训练能显著提
高模型的鲁棒性，但往往伴随着模型预测准确度的降低，揭示了模型鲁
棒性与预测准确率之间的潜在权衡关系。为了深入探索并理解这一现象，
本文提出了一个交互式可视分析框架。该框架通过细致地分析对抗训练
过程中，模型决策边界及其周边样本点的动态变化，旨在帮助领域专家
更好地理解对抗训练的行为。整个框架从数据样本层面，自顶向下地逐
层探究模型性能指标的变化，识别易受攻击的区域和样本点，并采用创
新的可视化方法展示决策边界的变化。
[关键词]：对抗鲁棒性; 对抗训练; 可视化
I
[ABSTRACT]:Adversarialtrainingisanextremelyeffectivestrategytopro-
tectdeeplearningmodelsagainstadversarialattacksandenhancemodelrobust-
ness. However, numerous studies have found that although adversarial train-
ing can significantly improve model robustness, it often comes with a decrease
in model prediction accuracy, revealing a potential trade-off relationship be-
tween model robustness and prediction accuracy. To further explore and un-
derstand this phenomenon, this paper proposes an interactive visual analysis
framework. The framework carefully analyzes the dynamic changes in model
decision boundaries and neighboring sample points during adversarial training,
aiming to help domain experts better understand the behavior of adversarial
training. The entire framework systematically explores changes in model per-
formance metrics from the data sample level, identifying vulnerable areas and
sample points prone to attacks, and employs innovative visualization methods
to showcase the changes in decision boundaries.
[Key words]: Adversarial Robustness; Adversarial Training; Visualization
II
目录
1. 引言和背景 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
2. 相关工作 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2.1 对抗训练方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2.2 机器学习的可视化方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2.3 时间序列的可视化方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3. 系统需求、研究挑战和主要贡献 . . . . . . . . . . . . . . . . . . . . . . 5
3.1 系统需求概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 研究挑战 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.3 解决方案和主要贡献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4. 模型和方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.1 对抗训练前置知识 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2 模型与具体算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.3 数据处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5. 可视分析系统设计与实现 . . . . . . . . . . . . . . . . . . . . . . . . . . 14
5.1 模型指标概览视图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
5.2 实例视图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
5.3 投影视图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5.3.1 预测投影 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5.3.2 时间序列投影 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
5.4 决策边界视图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
III
6. 实验结果 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
6.1 使用场景 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
6.2 案例分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
7. 讨论和未来展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
7.1 讨论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
7.2 未来展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
8. 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
参考文献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
致谢 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
IV
1. 引言和背景
近年来，深度神经网络（DNN）已被广泛应用于各个领域，包括自然语言处理、
计算机视觉、语音识别等。例如，在计算机视觉领域，DNN已经在图像分类、目标检
测、图像分割等任务中取得了巨大成功。一项由 Szegedy 等人在 2013 年提出的研究
GoogLeNet[1]取得了是2014年ILSVRC挑战赛冠军，这表明DNN已经成为了图像识
别领域的一种主导技术。
然而，研究表明，尽管在许多任务中表现出非凡的性能，DNN却容易受到各种形
式的攻击。以对抗攻击为例，即使对人眼几乎不可察觉的微小扰动，也能够引起DNN
预测错误。例如，Goodfellow 等人在 2014 年的研究[2]中展示了通过在图像中添加对
人类几乎不可察觉的扰动，可以欺骗当时具有最优分类性能的DNN模型GoogLeNet，
使其将一只熊猫识别为长臂猿。这种易受攻击性严重阻碍了DNN在一些安全关键领
域的应用，如自动驾驶、生物识别和医疗诊断。在自动驾驶领域，DNN 被广泛应用
于实现环境感知和决策制定，但其易受攻击性可能导致对环境的错误识别，从而引发
事故。在生物识别领域，DNN 被用于人脸识别、指纹识别等任务，然而，受到对抗
攻击的影响，可能导致认证系统受到攻击者的欺骗。在医疗诊断领域，DNN 在医学
图像分析和疾病诊断方面表现出色，但其易受攻击性可能导致错误的诊断，从而给患
者带来严重的健康风险。
为了增强模型对恶意攻击的鲁棒性，可以使用对抗训练、集成学习、迁移学习、
加入正则化等方法，而本研究关注的对抗训练目前被认为是其中最有效的方法。在过
去的几年里，对抗训练从各个方面进行了研究和讨论，但对抗训练对模型决策边界的
动态影响仍然需要进一步深入探索。因此，本文引入了一种交互式可视分析框架，对
对抗训练过程中模型和样本点的变化进行动态展示和对比分析，旨在辅助专家更好
地理解对抗训练的行为。
1
2. 相关工作
本文的工作旨在解释和分析对抗训练中涉及的动态过程。在本节中，我们回顾了
对抗训练和模型解释性可视化的相关工作。
2.1 对抗训练方法
对抗训练[2-3]已被证明是一种通过对对抗样本[4]进行训练来增强深度神经网络模
型鲁棒性的有效方法。这些示例是通过对原始示例应用精心设计的和不可察觉的扰
动来生成的，故意造成错误分类。许多研究从不同的角度探索了对抗训练。一种方法
是通过数据增强生成数据[5]来补充训练集，从而提高模型的鲁棒性。Wang 等人[6]提
出使用最新的扩散模型进行数据生成以增强对抗训练，实现了最先进的性能。虽然将
大量生成数据纳入对抗训练可以显著提高模型的鲁棒性，但计算成本的激增也会导
致训练过程的低效。
除了利用生成数据外，另一类主要方法侧重于改进对抗训练方案，包括损失函数
优化[7-9]、样本重加权[10-11]以及自适应调整攻击强度[12]。其中许多研究特别关注对抗
训练过程中决策边界的变化。TRADES[9] 引入了一个惩罚正则化项，用于接近决策边
界的实例，将数据点推离决策边界以增强对攻击的鲁棒性。Rade 等人[7]提出了 HAT
算法来缓解对抗训练造成的决策边缘过大的问题，这个问题被其证明会损害干净数
据分类的准确性。GAIRAT[11]为更靠近决策边界的数据点赋予更大的权重，优先增强
这些脆弱实例的鲁棒性。Xu等人[10]观察到在对抗训练中，决策边界可能会在接近其
他脆弱点的同时远离一些脆弱点，因此他们提出了DyART算法来优先增强较小的决
策边界。然而，这些工作是从宏观的理论角度进行分析，没有清晰地展示整个对抗训
练过程中决策边界的具体变化。本文旨在使用可视分析框架进行更详细的分析和探
索。
2.2 机器学习的可视化方法
在模型训练过程中，机器学习模型通过迭代更新参数来学习训练数据中的数据
事实。为了与训练过程进行交互，可视分析方法可以为人类提供三方面的帮助，即模
型理解、模型诊断和模型操纵[13]。
2
(1) 模型理解
理解模型如何工作对于模型评估和模型改进是必要的。可视分析方法通过总结
模型行为，促进了对模型行为的理解。为了展示神经元在深度卷积神经网络中
的作用，CNNVis[14]根据神经元在网络中的位置和激活情况对神经元进行聚类。
利用神经元簇，人类可以快速检查包含数百层的CNN 模型。TensorFlowGraph
Visualizer[15]通过描述组操作之间的数据流为人类提供了卷积网络的概述。为了
寻求直观的解释，What-If工具[16]允许用户根据指定的实例了解模型行为。
(2) 模型诊断
为了识别性能问题，需要监控训练过程并诊断模型性能。性能指标的变化，如准
确性和损失，展示了训练过程的高层次总结，可以通过折线图可视化[17-19]。当
出现训练问题时，折线图反映的波动可用于检测训练阶段。为了进一步诊断问
题，人们可以回顾训练阶段并检查模型更新。例如，ConceptExplorer[20]将性能
问题与参与某些轮在线学习的数据记录关联起来。AEVis[21]比较了用于模型训
练的正常记录和对抗记录的数据路径，以解释神经网络是如何被对抗样本愚弄
的。[22]还将性能问题与包括数据特征和模型结构在内的信息联系起来，以解释
投毒攻击如何影响模型。
(3) 模型操纵
人类可以根据对模型行为和模型诊断结果的理解，通过转向模型来提高模型性
能。具体来说，人类可以将他们的知识整合到模型中[16,23]，或用一个更好的模
型取代使用的模型[24-25]。
2.3 时间序列的可视化方法
从多维时间序列数据中识别模式是一个综合的过程。因此，整合可视化和数据分
析方法以提高效率是必不可少的。现有研究从交互式设置目标模式、基于聚类和频率
特征提取重复模式、利用机器学习方法检测异常模式三个方面实现了这一目标。
(1) 交互式设置目标模式
3
在可视化界面中，分析人员可以通过可视化分析系统来表达他们的需求。例如，
Thermalplot[26]支持通过为每个属性设置权重来指定需求，而 COPE[27]则需要分
析人员通过设置属性值的阈值来指定事件，以探索共现模式。另外，TimeNotes[28]提
供了用户层次化的时间轴，允许用户通过刷选逐步从一个大的时间范围中选择
一个或多个小的感兴趣的时间范围。
(2) 基于聚类和频率特征提取重复模式
当分析人员对数据集的了解有限时，必须借助自动化方法来增强分析。时间多
维标度（TMDS）[29] 通过用户定义的滑动窗口将时间维度离散化，并通过 MDS
将每个窗口中的多维数据投影到一维，从而帮助分析人员理解时间模式。除了
降维方法之外，提取重要时期也可以减少用户的工作量。StreamExplorer[30] 使
用子事件检测模型从社交流中识别重要时期，并建议用户分析与大量推文相关
的时期以及相关事件，TPFlow[31] 使用分段一级张量分解来检测具有最高优先
级的子张量。
(3) 利用机器学习方法检测异常模式
在识别异常模式方面，机器学习模型的高预测误差可能暗示着异常模式[32]的存
在。因此，概念漂移检测方法可以应用于动态环境中以定位有用的模式。可视
化技术如线图[33]、散点图[34]和平行坐标[35]等常见图表可用于描述概念漂移的
发展[36-37]，另一方面，模型生成的信息可以传达概念漂移的特征。每个属性值
的时变贡献可以用于分类。通过标记波动，可以从微观层面轻松地识别概念漂
移的发生。此外，用户可以从宏观层面对每个属性值的重要性进行评估。
4
3. 系统需求、研究挑战和主要贡献
3.1 系统需求概述
本文提出的系统目标是帮助专家更好的了解对抗训练的过程，因此我们将在模
型层面，实例层面，动态训练层面对系统提出需求。
模型层面上，我们需要关注原始模型与鲁棒模型的差异。最基础的是比较它们在
全局性能上的表现，包括标准精度、鲁棒精度、脆弱实例数量、间隔以及混淆矩阵。
更进一步的，我们还想观察模型在对抗训练过程中相邻时期的变化。
实例层面上，专家用户往往关注那些易被攻击的实例，系统需要通过一些度量指
标帮助用户识别易受攻击的实例，从而进一步分析其特征和行为。此处提到的行为包
括在训练过程中实例标签的变化，实例在决策边界附近相对移动时的距离变化等。
动态训练层面上，系统需要刻画训练过程中决策边界的变化来帮助专家理解鲁
棒性和准确性之间的权衡，并为决策边界的对抗训练行为提供更深入的认识。
3.2 研究挑战
根据系统需求，在研究中会遇到的挑战有以下几部分。
(1) 追踪对抗训练过程中的变化：在对抗训练中，数据点的标签和到决策边界的距
离会随着训练的进行而变化。数据点的标签很容易观察，但如何度量数据点到
决策边界的距离是有难度的。
(2) 模型比较和分析：对比连续时期的模型，分析它们之间的差异。这涉及评估模
型的鲁棒性准确性以及对抗训练过程中模型参数的变化。
(3) 对抗鲁棒性评估：仅仅依靠鲁棒性准确性作为评估模型对抗鲁棒性的唯一指标
是过于简单的，需要考虑所有数据点的分布以及决策边界附近数据点的距离特
征等。数据点可以依据模型训练过程中的行为聚类，如从训练一开始被分类错
误，训练过程中标签不断跳变，训练最后到达鲁棒等，评估不同类型的样本点
对增强模型鲁棒性的贡献和意义。
(4) 可视化决策边界和数据点分布：通过可视化方法描述决策边界及其附近的数据
5
点分布，保留高维数据点到决策边界的距离特征。在二维屏幕上展示高维决策
边界的基本特征和数据点之间的距离特征需要选择合适的算法和可视化编码。
3.3 解决方案和主要贡献
为解决上述挑战，本文提出了一个交互式可视分析框架。该框架通过细致地分析
对抗训练过程中，模型决策边界及其周边样本点的动态变化，旨在帮助领域专家更好
地理解对抗训练的行为。整个框架从数据样本层面，自顶向下地逐层探究模型性能指
标的变化，识别易受攻击的区域和样本点，并采用创新的可视化方法展示决策边界的
变化。
本系统主要由四个主要视图和其子视图组件组成：（1）模型指标概览视图，用户
选择感兴趣的对抗方法和数据集，观察基础指标变化趋势（2）实例视图，自定义过
滤样本，寻找有价值的样本点（3）投影视图，展示样本特征在高维空间到二维平面
的投影（4）决策边界视图，展示样本集群和决策边界的距离变化关系。具体视图设
计见第5章节。
本项目的贡献如下：
• 对抗训练调研 为了全面了解当前面向对抗鲁棒性的深度学习可视化解释分
析领域的不足和挑战，以及满足用户需求的潜在方向，本项目进行了对抗训练
调研。在调研过程中，对近年来发表的对抗鲁棒性深度学习相关论文进行了深
入研究，综合选取领域中经典和前沿的对抗训练方法作为系统支持的模型。
• 系统设计 本文提出并实现了面向对抗鲁棒性的深度学习可视化解释分析框
架，该框架通过细致地分析对抗训练过程中，模型决策边界及其周边样本点的
动态变化，旨在帮助领域专家更好地理解对抗训练的行为。系统位用提供了较
高的自由度，可以选择不同的对抗训练方法，选择训练过程的任意时刻，选择
数据集中任意样本点，启发式的引导用户寻找对用户有价值的样本点和训练区
间，通过观察模型、样本、决策边界的变化多维度理解对抗训练的过程。
6
4. 模型和方法
4.1 对抗训练前置知识
• 对抗攻击 对抗攻击是指为训练良好的模型寻找对抗样本。在本研究中，我们
只考虑在基于lp的攻击时，训练和测试数据为相同分布的情况。考虑一个分类
问题，可以用f(x;θ) : Rh×w×c → {1…k}表示一个图像分类器，它将输入图像x
映射到具有k个类别的离散标签集C，其中θ表示f 的参数，h,w,c分别表示图
像的高度，宽度和通道。给定扰动阈值ϵ，攻击者试图找到一个扰动δ ∈ Rh×w×c
使得损失函数最大化，使得 f(x+δ) ̸= f(x)。因此，我们可以将 δ 用以下公式
表示出来：
δ∗ := argmaxL (θ,x+δ,y) (1)
ce
|δ| p≤ϵ
这里的 y 是 x 的分类标签，p 可以是 0、1、2 和 ∞。在大多数情况下，ϵ 很小，
人眼无法察觉这些扰动。损失函数L 这里具体为交叉熵损失L 。
ce
x 对应生成的对抗副本x′ 可以表示为：
′ ∗
x := x+δ (2)
• 对抗鲁棒性 一般来说，对抗鲁棒性是模型对具有对抗攻击的测试数据的性
能，即在使用对抗样本进行测试的分类准确率。模型使用者期望知道模型在最
坏情况下的性能，因此攻击者应该足够强大和全知，即攻击者对模型有充分的
了解，例如架构、参数、训练数据等，这也被称为为白盒攻击。在实践中，投
影梯度下降攻击 (Projected Gradient Descent，PGD)[3]因为它在白盒设置下的攻
击有效性而被广泛用于评估。
4.2 模型与具体算法
本研究的模型结构基于 ResNet，这是一个深度残差网络，用于对抗训练。在该
基础网络上，我采用了不同的对抗训练算法，包括PGD（ProjectedGradientDescent）、
HAT（Helper-basedAdversarialTraining）和DyART（DynamicsAwareRobustTraining）。
7
这些算法在训练过程中使用不同的方法生成对抗样本或者优化了损失函数，以增强
模型对抗攻击的鲁棒性。除此上述几种对抗训练算法之外，我们使用fab攻击的方法
来模拟样本到决策边界的距离。本小节将介绍这几种算法的基本思想、数学原理和优
势。
• PGD Huang 等人[38]在 2015 年首次定义了一个 min-max 问题，他们认为对抗
训练的过程就是模型不断使自己分类错误最小化，以抵抗一个试图加入扰动使
分类错误最大化的敌手。
[ ]
minE max L (θ,x+δ,y) (3)
(x,y)∼D ce
θ δ∈B(x,ε)
式中(x,y) ∼ D 表示从分布D 中采样的训练数据，B(x,ε)为允许的扰动集，可
以被表示为B(x,ε) := {x+δ ∈ Rm| ∥ δ ∥ ≤ ε}。
p
Madry 等人[3]对这个 min-max 问题进一步解释：内部是一个最大化问题，即模
型找到最坏情况的样本，外部是一个最小化问题，即训练一个对对抗样本具有
鲁棒性的模型。基于这个理解，他们采用了基于多步梯度的PGD攻击来解决内
部问题，具体如下：
( ( ( )))
xt+1 = Proj xt +αsign ∇ L θ,xt,y (4)
x+B(x,ε) xt ce
这里的 t 是当前步数，α 是步长。此外，他们从对抗样本的角度研究了内部最
大化问题，并给出了局部极大值与PGD的可处理性的理论和经验证明。通过大
量的实验，他们的方法 (PGD-AT) 显著提高了深度学习模型对各种攻击的对抗
鲁棒性，这是对抗训练方法的一个里程碑。
• HAT Rade 和他的团队[7]在研究中发现，对抗训练会导致某些边缘间隔无端
增加，使得原本被分类正确的样本点训练过后被错误分类，从而损害模型的
准确性，基于这种现象，他们提出了一种新的的算法：Helper-basedAdversarial
Training(HAT)。通过在训练过程中加入额外的错误标记来减少这种影响，HAT
8
算法在不影响鲁棒性的情况下，提高了精度，与现有的其他对抗训练方法相比，
它在准确性和鲁棒性之间取得了更好的权衡。
具体来说，他们的目标是在学习一个稳健的模型的同时保留标准训练网络的某
些几何属性，即在对抗方向上的预测能力。为此，他们在对抗训练过程中添加
了额外的训练样本，称为帮手样本，这些样本是通过在训练过程中找到的对抗
扰动进行外推构造的，并由标准网络标记。助手样本可以被定义为：对于一个
输入样本 (x ,y )，一个标准网络 f ，一个训练迭代 k 次的鲁棒网络 f ，和
i i θ std θ rk ob
一个由敌手计算出的对抗样本x′，对应的帮手样本(x˜ ,y˜)可以通过如下公式得
i i i
到，
x˜ = x +2r , r = x′ −x , y˜ = argmax f (x′ ) (5)
i i i i i i i k θ std i k
将帮手样本定义为 x˜ = x +2r 是一种启发式选择，它提供了一个很好的折中
i i i
方案，一方面帮助样本与 x 有足够的不同，以便标准模型分配不同的标签，另
一方面又不太相似，以免影响其他干净样本的性能。
HAT算法的损失函数分为三部分，干净样本上的交叉熵损失，以标量β 加权的
对抗样本上的KL 散度损失，以γ 加权的帮手样本上的额外交叉熵损失。
• DyART Xu等人[10]在他们的研究中量化了训练过程中每个样本点到决策边界
的相对速度，并在训练过程中发现决策边界远离一些样本点的同时靠近了另一
些样本点，这导致鲁棒性的提升受限。为缓解这个冲突，他们提出动态感知的
鲁棒训练方法 (DyART)，通过精心设计软边缘间隔上的损失函数，鼓励决策边
界优先参与那些小边缘间隔的移动。与之前的工作相比，DyART直接在边缘间
隔而不是其的近似估计上操作，这允许更有针对性和有效的鲁棒性改进。
具体来说，在他们的研究中发现直接寻找边缘间隔最近的样本点的成本是极其
昂贵的，为解决这一问题，他们先提出了提出了一种有效的替代方法，通过将
边缘间隔替换为软边缘间隔来计算梯度。后者是前者的下界，其近似差距是可
控的。DyART 的目标是增加软边缘间隔并且达到一个高的干净准确率。为此
他们精心设计了损失函数，对于大小为 n 的批训练数据 B 和定义为 {i ∈ B :
9
Φyi(x ;β) > 0}的大小为m 的B+，损失函数如下：
θ i θ
∑ ∑
1 λ
L (B) = l(x ,y )+ h(Rsoft(x )) (6)
θ n i i n θ i
i∈B i∈B+
θ
其中第一项是自然数据点上的平均交叉熵损失，第二项是增加软边缘间隔的损
失。超参数λ 调节了干净准确率和鲁棒准确率之间的平衡。
• FAB 快速自适应边界攻击（FastAdaptiveBoundaryAttack，FAB）是一种用于
生成对抗样本的攻击方法。它由Croce等人[39]在2020年提出。FAB的主要目标
是在给定原始样本的情况下，找到一个与原始样本非常接近的对抗样本，即在
对抗样本与原始样本之间的距离尽可能小的情况下，使得对抗样本能够被误分
类。FAB 的基本思想是通过迭代地投影原始样本到线性近似的决策边界上，并
在投影的过程中偏向原始样本，以保持对抗样本与原始样本之间的相似性。这
种方法的优点是可以在相对较少的迭代次数内找到对抗样本，并且在处理高维
数据时表现良好。
具体来说，FAB 算法会在每次迭代中调整投影方向和步长，以保证尽可能地接
近原始样本。通过这种方式，FAB 可以有效地搜索对抗样本的空间，并在较短
的时间内找到对抗样本。然而，FAB 并不保证找到全局最优的对抗样本，而是
在局部范围内寻找最优解。
总的来说，FAB 是一种高效的对抗样本生成方法，特别适用于对抗鲁棒性的研
究和实践中，需要注意的是，在本项目所有情景中，原始样本到FAB攻击生成
的攻击样本将被视为原始样本到决策边界的距离。
4.3 数据处理
在如今对抗训练的研究中，主流使用的数据集为 CIFAR-10 数据集，因此本工
作同样使用该数据集进行模型训练测试以及数据处理。CIFAR-10 数据集由 60000 张
32x32 彩色图像组成，包含有 50000 张训练图像和 10000 张测试图像。分为 10 个类
别，每个类别有6000张图像。更具体的，数据集分为五个训练批次和一个测试批次，
10
每个批次包含 10000 张图像。测试批次包含每个类别中随机选择的 1000 张图像，训
练批次包含 5000 张图像，但每个类别的图片数量并不一定一致。与 MNIST 数据集
相比，CIFAR-10 是 3 通道的彩色 RGB 图像，且相比于手写字符，CIFAR-10 含有的
是现实世界中真实的物体，不仅噪声很大，而且物体的比例、特征都不尽相同，这为
分类工作和提高模型鲁棒性带来一定的困难。而与 ImageNet 数据集相比，ImageNet
则是一个更大更复杂的数据集，包含了 1000 个不同的类别，总共有 1.2 百万张图像。
这些图像的大小是 224x224 像素，并且需要手动标注。ImageNet 是一个更挑战性的
数据集，因为它包含了更多的类别，并且图像的大小更大。目前对抗训练最前沿的研
究在 ImageNet 数据集上的表现效果也都十分不佳，因此 CIFAR-10 是一个更适合当
前对抗训练研究的数据集。
图1 CIFAR-10数据集示例
• t-SNE
t-SNE降维，全称t-distributedStochasticNeighborEmbedding，是一种流行的降
维技术，常用于可视化高维数据。t-SNE 主要用于将高维数据映射到低维空间
（通常是2维或3维），以便将数据可视化并发现其中的结构。
t-SNE 的工作流程有几个关键的部分：首先计算高维数据中每对数据点之间的
相似度，通常使用高斯核函数或 t 分布来衡量相似度，距离较近的点在低维空
11
间中也应该保持较近的距离。然后将高维数据和对应的低维嵌入空间中的点表
示为概率分布。它试图使高维空间中的相似数据点在低维空间中拥有更高的概
率密度。t-SNE 的目标是最小化高维空间和低维空间概率分布之间的 KL 散度
（Kullback-Leibler divergence）。使用梯度下降等优化方法来调整低维空间中的
点的位置，直到达到最小化KL散度的目标。
选择 t-SNE 进行降维和投影是因为其擅长保留数据的局部结构，适用于发现数
据集中的聚类和子群。与PCA（PrincipalComponentAnalysis）等线性降维方法
相比，t-SNE可以更好的捕捉数据的非线性结构。
• LSTM
LSTM（LongShort-TermMemory，长短期记忆）是一种用于处理序列数据的深
度学习模型，特别适用于时间序列数据、自然语言处理和其他需要对长期依赖
关系进行建模的任务。在本系统中，样本在训练过程中到决策边界的距离是时
间序列数据，目标在于将庞大数据量的时序数据进行聚类处理，用LSTM提取
时间序列数据的特征是聚类的第一步。
LSTM 是一种循环神经网络（RNN）的变体，被用于解决一般递归神经网络中
普遍存在的长期依赖问题，使用LSTM可以有效的传递和表达长时间序列中的
信息并且不会导致长时间前的有用信息被忽略（遗忘）。与此同时，LSTM还可
以解决 RNN 中的梯度消失/爆炸问题。LSTM 包含了以下几个重要的组件：输
入门、遗忘门、细胞状态、输出门、隐藏状态。输入门控制输入的权重，决定哪
些信息将被加入到细胞状态中；遗忘门决定细胞状态中哪些信息将被遗忘，控
制细胞状态的保留程度；细胞状态用来传递信息，允许长期依赖关系得以维持；
输出门可以控制输出权重，根据输入和细胞状态，决定输出的内容；隐藏状态
根据输入和前一个隐藏状态计算得到的，用于在序列中保持信息。
• 聚类算法Kmeans&DBScan
聚类算法中最常见的就是KMeans和DBSCAN。本系统在将时序数据输入LSTM
后得到特征，对提取的特征进行聚类，期望用户发现样本实例和决策边界普适
12
性的行为表现。KMeans是一种基于距离的聚类算法，将距离比较近的数据点看
作相似的点，将它们归为一类。Kmeans需要指定聚类簇数，生成指定数量的簇
心，通过循环不断更新簇心的位置直到簇心的位置基本不变。Kmeans算法有几
个显著的缺点，第一就是对簇的数量的选择，我们希望指定一个簇数 K，以使
每个点和其最近的簇的距离之和最小，不同K值对聚类结果影响巨大。在本系
统需要的时间序列聚类任务中不知道应该将数据分为几类。第二，Kmeans算法
会把所有的数据点都进行分类，但是在大部分情况中会有一些离群点，这些点
应该被剔除。第三，Kmeans 聚类假设对每个簇来说，所有的方向都同等重要，
这也就意味着 k 均值聚类主要适用于球形分布的数据，对于其他分布的数据聚
类的效果可能不好。
相比于Kmeans，DBScan更适合系统需求。DBSCAN全称为Density-BasedSpatial
ClusteringAlgorithmwithNoise，基于密度的噪声空间聚类算法。DBSCAN算法
是基于密度对数据点进行处理的，主要是将特征空间中足够密集的点划分为同
一个簇，簇的形状可以是任意的，而且数据点中有噪声点的话，不会将这些点
划分给某个簇。DBScan有几个优点，不需要我们指定数据集中簇的个数K；可
以用于各种复杂形状的数据集；可以排除离群点的干扰。而它的缺点有，消耗
更大的计算成本；需要调整优化 eps 和 min_sample 这两个参数来达到想要的
效果。经过比较，最终采用DBscan作为时间序列聚类算法。
13
5. 可视分析系统设计与实现
为了将上一章节提到的模型和算法的结果更好的展现给专家用户，并方便其交
互以发现新的见解，本工作设计了一个面向对抗鲁棒性的深度学习可视化解释分析
框架。该系统分为四个子视图：模型指标概览视图，实例视图，投影视图，决策边界
视图。这些子视图的配合使用，可以帮助用户分析其所关注的模型以及感兴趣的样本
点在训练过程中的行为变化，从而加深对对抗训练的理解。
图2 系统概览
5.1 模型指标概览视图
模型指标概览视图旨在给用户提供一个对模型性能的全局认识，整个系统的使
用从模型指标概览视图开始。如图3所示，模型指标概览视图的功能有选择模型和数
据集，用户可以根据需求设置参数模块来选择上一章节提到的几种对抗训练算法和
感兴趣的数据集，这个选择是系统全局的，选择之后，视图将绘制训练过程中标准模
型和对抗训练模型的指标变化折线图，最基础的指标包括干净准确率和鲁棒准确率。
用户可以横向观察随着训练进行模型性能的变化，同时也能够纵向对比加入对抗训
练前后对干净准确率和鲁棒准确率之间的平衡关系。交互方面，本视图除了一开始就
要选择模型和数据集，在整个系统使用的过程中，用户根据后续其他视图的观察可以
点选指标变化折线图，全局选中一个特殊感兴趣的 epoch，一旦全局选中 epoch 的值
14
发生变化，投影视图会做出相应的更新。
图3 模型指标概览视图
5.2 实例视图
实例视图是为了让用户通过感兴趣的指标选择出可能有价值的样本点。图4展示
了整个实例视图。该视图由左右两个子视图构成，视图左侧是一个表格，展示实例到
决策边缘距离时序特征，右侧是对抗训练模型和基础模型中单个实例到和决策边界
距离的折线图。
图4 实例视图
如图5所示，表格的行头是系统提供的选择指标的密度直方图，包括预测标签变
化次数、平均PGD迭代次数，对抗训练模型样本到决策边界的平均距离，对抗训练模
15
图5 实例训练全过程时序特征自定义筛选
型和标准模型边界距离的差值，稳定差值，系统对这些重要指标做密度直方图，旨在
帮助用户加强对整个数据集训练过程的理解，并且在直方图中做区间刷选更加直观。
在表格中，每一行代表一个实例在对抗训练全过程中的各项指标值，每个单元格均用
数值附加代表数值大小的矩形表示，降低用户通过肉眼比较数据大小的难度。用户可
以对类别过滤，对表头的直方图进行刷选过滤、排序操作来获取感兴趣的样本实例。
找到感兴趣的样本实例后，点击该样本实例所在的行，即可系统全局的选中一
个感兴趣的样例，在该视图右侧将绘制训练过程中选中实例在标准模型和对抗训练
模型中相对决策边界的距离变化和标签变化折线，如图6 所示，假设用户此时选中
1-truck 为继续观察的实例，上下两个折线图分别对应基础模型和对抗训练模型。通
过上下对比，可以观察对抗训练对模型鲁棒性的作用。系统将距离值归一化到 [-1,1]
区间，需要注意的是，[-1,0] 区间的数值不具备数学意义，负值仅代表当前 epoch 的
模型预测错误。如区域C，当模型分类错误时，将折线与x轴围成的区域以灰色填充，
并用代表预测类的颜色画竖线和点。当数值处于[0,1)时，即区域A所示，模型分类
正确，数值即代表实例到决策边界的距离，用当前类的颜色填充折线和x轴围成的区
域。当数值为 1 时，即区域 B 所示，意味着样本处于鲁棒状态，不用颜色区域填充，
转而使用当前类颜色的竖线和点表示。用户可以进一步使用刷选工具具体观察感兴
趣的epoch区域，如果需要对某个epoch的样例行为进一步观察，可以在模型指标概
16
图6 实例到决策边界距离变化视图
览视图中点选epoch，进一步在投影视图中产生联动。
5.3 投影视图
实例视图是关注每个样本点训练全过程的行为，而投影视图主要用于观察所有
样本点在一个epoch中的分布情况，其目的和实例视图一致，希望能帮助用户观察训
练过程中模型的行为，同时找出有价值的样本点。投影视图可以分为两个子模块，预
测投影和时间序列投影。
投影视图支持点击投影样本点全局选中样本实例，因此实例视图和投影视图的
联动相当密切，在投影视图新选中实例，对应的训练全过程到决策边界距离的折线图
也会在实例视图中更新。
5.3.1 预测投影
• 预测分类投影 分类效果是评估模型最直接的依据，在训练过程中，仅仅通
过准确率变化来观察模型训练的结果是单一的，预测视图覆盖训练过程所有
epoch，展示所有样本实例从高维空间到二维空间的投影，借助这个视图，用户
17
a) 预测分类投影 b) 选中实例k近邻投影
图7 投影视图预测投影
可以更直观的观察到模型在训练过程中的变化。在模型指标概览的视图中，点
击折线图即可选中当前epoch，与此同时，在投影视图中的预测投影中使用当前
epoch模型提取出的特征，使用t-SNE投影方法，将数据集中的所有样本降维投
影到二维平面上。t-SNE 旨在保持高维空间中数据点之间的局部关系，使得原
始空间中相近的点在降维后的空间中也保持接近，这有助于识别数据中的细微
模式，从而帮助用户理解数据内在的模式和结构。如果在全局选中了一个感兴
趣的实例，用户可以在投影视图中着重观察这个实例点的行为特征。如图7a所
示，模型在训练初期和末期，投影视图的展示，可以看到对于模型提取的特征，
t-SNE投影的分类结果更加明晰。
• k 近邻投影 在预测视图的基础上，系统提供了 k 近邻投影模式，对于用户在
实例视图中全局选中的实例，系统将分别计算同类别和不同类别欧氏距离近邻
的10个实例，系统猜测这些近邻点是用户潜在关注的样本实例，这个功能可以
为用户进一步挖掘特殊样本实例提供引导。如图7b这些近邻点将在隐去其他样
本后进行特殊视觉编码将其投影出来，颜色和形状代表了实例当前 epoch 不同
的状态：鲁棒点，靠近决策边界，远离决策边界等。
18
5.3.2 时间序列投影
在实例视图右侧只能展示一个选中样本实例的对抗训练模型和基础模型样本到
决策边界的距离变化折线图，过于单一，时间序列投影功能旨在进一步帮助用户发
现样本到决策边界距离序列的共性，对所有数据集所有样本的两个序列的差值使用
LSTM提取特征并用DBScan聚类，并将提取的特征使用t-SNE方法投影。将时间序
列聚类后的结果如图8所示，用户能够更方便的找到行为相似的样本。
图8 投影视图时间序列聚类投影
5.4 决策边界视图
可视化决策边界一直是该领域的难题，决策边界是高维且抽象的，将其可视化到
人能理解的三维或者二维空间，必然会导致信息丢失。本系统使用虚拟决策边界作为
决策边界的可视表达，不赋予决策边界具象的形状，从样本实例到决策边界的距离和
行为来间接的刻画决策边界的变化，这个变化是相对的，距离变化可以视为样本实例
不动但决策边界运动，通过这种方式来帮助用户理解决策边界的行为变化。
本视图可以选择起始 epoch 和间隔步长，在视图中绘制 10 个子图，每个子图代
表的epoch由用户选择的起始epoch和步长计算得出，子图绘制包含了选中实例和其
19
同类别 10 个近邻样本相对于决策边界的距离示意图，视图只考虑当前类和其他类，
中间较粗的竖线代表了虚拟决策边界，竖线将子图分割为左右两部分，左边代表当前
选中样本实例的类空间，右侧代表其他类的总和空间。左侧的最左边有一条灰色竖
线，灰色竖线的左侧代表当前类的鲁棒空间，灰色竖线和中间竖线的区域代表当前样
本处于易被攻击的状态。出现在中间竖线右侧的红色十字是 PGD 攻击生成的攻击样
本，如果样本预测错误或者PGD攻击失败则右侧不绘制红色样本点。10个子图可以
看做是训练的连续间隔过程，调整起始epoch和步长，用户可以观察到这个相似集群
在训练过程中的行为变化，从而相对的观察决策边界行为变化。
图9 决策边界视图
20
6. 实验结果
6.1 使用场景
图10 使用场景流程
在本小节中，通过一个使用场景来展示本系统的使用流程，图10展示了用户使
用的全流程。X 是一名对抗训练领域的专家，他对对抗训练的传统方法如 PGD 的数
学原理和思想有较为全面的了解，但是对于 PGD 训练过程中模型的变化一直没有一
21
个清楚的认识，他想要从模型行为和实例行为层面对 PGD 训练的过程有更深入的了
解，于是 X 使用本项目提供的对抗训练可视化框架。首先进入系统，X 先操作系统
左上角的模型概览视图，选择系统提供的对抗训练方法中的PGD（SAT）和数据集中
的CIFAR-10训练集。选择之后，X看到了使用该数据集和对抗训练方法训练200个
epoch 的基础指标折线图。X 对比了基础模型和对抗训练模型的干净准确率和鲁棒准
确率的变化趋势。X 发现在干净准确率上，两个模型的准确率的变化趋势几乎一致，
但对抗训练模型始终低于基础模型；在鲁棒准确率上，对抗训练模型持续上升，最后
提升至 0.85 左右，过程中产生了两个拐点，但基础模型在对于攻击样本的准确率在
前100个epoch几乎为0，到训练末期略有上升，但也不过0.2。这些变化趋势符合对
抗训练的公共认知，X 觉得两个模型的鲁棒准确率的差异和对抗训练模型产生两个
明显拐点是可能是他进一步想观察的地方。
X 觉得样本对对抗训练模型的影响是至关重要的，特别是距离决策边界近的那
些样本，它们可能在训练过程中不是很稳定，他进一步对实例视图进行操作。系统提
供了样本训练过程中样本预测变化次数（flip）这个过滤排序选项，X 想观察该指标
值处于 50 左右的值，因为值处于该范围的样例可能是分类由不稳定到鲁棒的点，他
对数值 30 到 70 的区域进行刷选，然后升序排序，依次点击几个样例让系统绘制决
策边界距离折线，通过折线来找到他想观察现象的样例，观察该样例右侧生成的到
决策边界距离的变化曲线，X 着重关注两个拐点部分，于是刷选对应的区域进行放
大观察。然后，X 点击模型概览视图折线图点选 epoch，在投影视图中观察这个拐点
epoch 区间投影效果的变化。将几个投影做比较，X 发现在这个区间类与类投影效果
有可以辨别的微小提升，但明显狗和猫（绿色和红色）投影有较大重叠区域，此时模
型对狗和猫的分类能力不足，后续可以关注这个现象。X打开了knn模式，同样观察
这个 epoch 区间内选中的点和近邻点的关系，并结合最后一个决策边界视图观察这
个 epoch 区间内样本和其 10 个近邻点集群和决策边界距离的关系。X 在观察后发现
近邻点集群在对抗训练的过程中逐渐在预测准确和鲁棒两个方面做到平衡，在第107
个epoch时11个点有10个点处于分类正确但非鲁棒的状态，X认为自己需要进一步
观察更多的实例样本行为来确定这个推论是否正确。
22
至此，一个简单的使用本对抗训练可视化系统的使用场景完成了，该使用场景
涵盖了系统所有视图的使用，帮助了用户 X 在模型层面，实例层面，决策边界层面
对PGD对抗训练过程有了更清楚的了解，在使用系统的过程中，X产生了两个见解：
模型对猫和狗的分类敏感程度低于其他类；模型趋向于在鲁棒性和准确率做出平衡。
后续可以通过在投影视图中点击投影样本点来选择实例，进一步到实例视图和决策
边界视图中观察新的感兴趣实例的变化，系统在这个使用场景也完成了为用户启发
探索的任务，整个系统使用进入下一次循环，不断深入的探索有助于专家验证推论和
产生新的见解。
6.2 案例分析
为了更好地展示本系统是如何协助领域专家理解对抗训练所导致的模型预测准
确度和对抗鲁棒性之间的权衡，本研究通过两个案例分析来分别解释对抗训练是如
何提升模型的对抗鲁棒性以及对抗训练是如何使模型预测准确度下降的。
• 对抗训练提升模型对抗鲁棒性
图11 对抗训练提升模型对抗鲁棒性案例
领域专家通过比对观察模型指标概览视图中的折线图，可以直观地看出随着对
抗训练的进行，模型的鲁棒准确度即对抗鲁棒性在不断上升，且相较于正常训
23
练的模型，对抗训练后的模型在对抗鲁棒性上有显著的提升。然后根据11所示，
专家进行A操作，通过观测实例视图中的样本到原始模型决策边界的距离和到
对抗模型决策边界的距离差值diffDist的统计数据分布情况，可以看出大部分
的样本点在经过对抗训练后逐渐远离决策边界，从而提升了抗扰动攻击的能力，
也是模型对抗鲁棒性上升的原因所在。紧接着，领域专家进行操作B通过筛选
出距离差值分布中最密集的区域并选中样本 5 来进一步观测单个样本在对抗训
练过程中的行为变化，从实例到决策边界距离变化视图可以看出 epoch 4 → 7
有一个显著的远离决策边界的行为，并在之后一直保持足够鲁棒的状态。紧接
着，操作 C 中，通过设置决策边界视图的初始 epoch 和间隔步长精准定位到决
策边界距离发生变化的 epoch 阶段。通过观测样本点 5 和其 knn 邻域中的样本
点，可以进一步发现大部分样本点在这个阶段都在远离决策边界并逐渐成为稳
定的鲁棒点。
• 对抗训练降低模型预测准确性
图12 对抗训练降低模型预测准确性案例
首先，专家用户通过比较正常训练模型和经过对抗训练后的模型在训练集上的
准确度后发现：经过对抗训练后，模型预测准确度有一定程度的下降。为了进
一步探索是什么原因导致的这种现象，如图12操作A，用户通过筛选diffDist
差值为负值的区域并选中其中的 4449−cat 样本，通过比对观察决策边界距离
24
变化视图发现，该样本在原始模型的训练过程中几乎全部被分类正确，但是在
对抗训练过程中的绝大部分阶段被预测错误。用户进行操作 B，通过进一步研
究epoch156→162这一训练时期后，发现样本点在决策边界附近波动并最终越
过决策边界从而导致被错误分类，在观察其邻域数据点的行为后推断可能是邻
域内其他点对该区域决策边界的影响所导致的该样本点越过决策边界，从而导
致模型预测错误。
25
7. 讨论和未来展望
7.1 讨论
在本节中，我们将讨论我们实现的面向对抗鲁棒性的深度学习可视化解释分析
框架的一些重要问题，主要包括：系统的泛用性以及可解释性方面的局限。
• 系统泛用性： 目前，本框架主要支持针对目前主流和前沿的几种对抗训练方
法的解释和分析。然而，我们也意识到这种限制可能会影响到用户在不同模型
间的比较分析以及对其他模型的解释需求。在数据集方面，CIFAR-10 作为本
系统支持的数据集，支持 60000 张图片的数据量，但 CIFAR-10 相较于更大的
CIFAR-100或者ImageNet有很大差距，目前还没有办法支持更大数据集的可视
化需求。
• 可解释性局限： 虽然我们的框架旨在提供对对抗鲁棒性的深度学习模型进行
可视化解释的支持，但当前可能存在一定程度的可解释性局限。对于复杂模型
或者高维数据，解释模型的结果的可能仍然存在一定的困难，导致用户无法全
面理解模型的行为，如：t-SNE 投影结果在不同 epoch 之间可能不具有连续性；
决策边界的可视化过于抽象，用户难以直观的分析决策边界的变化等。
7.2 未来展望
在上一小节的讨论中可以发现当前系统仍存在改进空间，因此在本节中将探讨
本系统在未来的优化方向。
• 拓展模型和方法： 未来，我们将致力于扩展框架对不同对抗训练方法和深度
学习模型的支持。这包括探索新的对抗训练技术，紧跟对抗训练前沿的方法，为
更新更高鲁棒准确率的方法提供可视化支持；以及支持更广泛的深度学习模型，
如不同结构的卷积神经网络或Transformer模型。
• 跨数据集的通用性： 我们计划将框架扩展到支持多个数据集，而不仅仅局限
于 CIFAR-10。这将使用户能够在不同数据集上进行实验和比较，从而更好地
理解模型的鲁棒性和泛化能力。我们还将探索如何通过跨数据集的可视化分析，
26
发现模型在不同数据分布下的行为模式和性能表现，为深度学习模型的可解释
性研究提供更多见解。
• 提升可解释性： 我们将持续改进框架的可解释性，致力于提供更准确、更直
观的解释结果。这可能涉及优化可视化方法，引入更高级别的特征表示方法，以
及探索更全面的解释性评估指标。我们还将研究如何将深度学习模型的解释结
果与领域专家的知识相结合，以提供更深入、更有洞察力的解释，帮助用户更
好地理解模型的行为和决策过程。
通过持续的研究和创新，我们期望我们的框架能够成为对抗鲁棒性深度学习领
域研究者的实用工具，为研究人员和从业者提供强大的分析和解释能力，推动该领域
的进步和发展。
27
8. 总结
在本文中，我们设计和实现了一个面向对抗鲁棒性的深度学习可视化解释分析
框架，该框架用于帮助专家用户理解对抗训练过程中模型、样本、决策边界的行为变
化。首先项目调研了对抗训练领域相关论文，选取了该领域经典和前沿数种对抗训练
方法，意图在系统中实现 PGD，HAT，DyART 方法的可视化。然后对需求和挑战进
行分析，根据需求设计了由四个视图组成的可视分析框架，并最终实现设计的系统。
系统从模型层面到集群层面再到实例层面，以自顶向下的结构引导用户探索对抗训
练的过程，使用户加深对对抗训练的理解或者发现新的见解。最后，我们通过用户使
用场景和案例分析证明系统的实用性。
28
参考文献
[1] SZEGEDYC,LIUW,JIAY,etal.GoingDeeperwithConvolutions[C].in:IEEEConferenceon
ComputerVisionandPatternRecognition.2015:1-9.
[2] GOODFELLOWI,SHLENSJ,SZEGEDYC.ExplainingandHarnessingAdversarialExamples
[C].in:InternationalConferenceonLearningRepresentations.2015.
[3] MADRY A, MAKELOV A, SCHMIDT L, et al. Towards Deep Learning Models Resistant to
AdversarialAttacks[C].in:InternationalConferenceonLearningRepresentations.2018.
[4] BAIT,LUOJ,ZHAOJ,etal.RecentAdvancesinAdversarialTrainingforAdversarialRobustness
[C].in:ProceedingsoftheThirtiethInternationalJointConferenceonArtificialIntelligence.2021:
4312-4321.
[5] GOWALS,REBUFFISA,WILESO,etal.ImprovingRobustnessusingGeneratedData[C].in:
AdvancesinNeuralInformationProcessingSystems.2021:4218-4233.
[6] WANGZ,PANGT,DUC,etal.BetterDiffusionModelsFurtherImproveAdversarialTraining
[C].in:InternationalConferenceonMachineLearning.2023:36246-36263.
[7] RADER,MOOSAVI-DEZFOOLISM.ReducingExcessiveMargintoAchieveaBetterAccuracy
vs.RobustnessTrade-Off[C].in:InternationalConferenceonLearningRepresentations.2022.
[8] YANJ,YINH,ZHAOZ,etal.EnhanceAdversarialRobustnessviaGeodesicDistance[J].IEEE
TransactionsonArtificialIntelligence,2024,(Toappear).
[9] ZHANG H, YU Y, JIAO J, et al. Theoretically Principled Trade-Off between Robustness and
Accuracy[C].in:InternationalConferenceonMachineLearning.2019:7472-7482.
[10] XU Y, SUN Y, GOLDBLUM M, et al. Exploring and Exploiting Decision Boundary Dynamics
forAdversarialRobustness[C].in:InternationalConferenceonLearningRepresentations.2023.
[11] ZHANGJ,ZHUJ,NIUG,etal.Geometry-AwareInstance-ReweightedAdversarialTraining[C].
in:InternationalConferenceonLearningRepresentations.2021.
[12] DINGGW,SHARMAY,LUIKYC,etal.MMATraining:DirectInputSpaceMarginMaximiza-
tion through Adversarial Training[C]. in: International Conference on Learning Representations.
2020.
29
[13] YUANJ,CHENC,YANGW,etal.ASurveyofVisualAnalyticsTechniquesforMachineLearn-
ing[J].ComputationalVisualMedia,2021,7:3-36.
[14] LIU M, SHI J, LI Z, et al. Towards Better Analysis of Deep Convolutional Neural Networks[J].
IEEETransactionsonVisualizationandComputerGraphics,2017,23(1):91-100.
[15] WONGSUPHASAWATK,SMILKOVD,WEXLERJ,etal.VisualizingDataflowGraphsofDeep
Learning Models in Tensorflow[J]. IEEE Transactions on Visualization and Computer Graphics,
2018,24(1):1-12.
[16] YANGW,WANGX,LUJ,etal.InteractiveSteeringofHierarchicalClustering[J].IEEETrans-
actionsonVisualizationandComputerGraphics,2020,27(10):3953-3967.
[17] LIUL,JIANGH,HEP,etal.OntheVarianceoftheAdaptiveLearningRateandBeyond[C].in:
InternationalConferenceonLearningRepresentations.2019.
[18] WANG J, GOU L, YANG H, et al. GANViz: A Visual Analytics Approach to Understand the
Adversarial Game[J]. IEEE Transactions on Visualization and Computer Graphics, 2018, 24(6):
1905-1917.
[19] WANGX,CHENW,XIAJ,etal.HetVis:AVisualAnalysisApproachforIdentifyingDataHet-
erogeneityinHorizontalFederatedLearning[J].IEEETransactionsonVisualizationandComputer
Graphics,2023,29(1):310-319.
[20] WANGX,CHENW,XIAJ,etal.ConceptExplorer:VisualAnalysisofConceptDriftsInMulti-
Source Time-Series Data[C]. in: IEEE Conference on Visual Analytics Science and Technology.
2020:1-11.
[21] CAOK,LIUM,SUH,etal.AnalyzingtheNoiseRobustnessofDeepNeuralNetworks[J].IEEE
TransactionsonVisualizationandComputerGraphics,2020,27(7):3289-3304.
[22] MAY,XIET,LIJ,etal.ExplainingVulnerabilitiestoAdversarialMachineLearningthroughVi-
sualAnalytics[J].IEEETransactionsonVisualizationandComputerGraphics,2020,26(1):1075-
1085.
[23] VANDENELZENS,VANWIJKJJ.Baobabview:InteractiveConstructionandAnalysisofDeci-
sionTrees[C].in:IEEEConferenceonVisualAnalyticsScienceandTechnology.2011:151-160.
[24] CAVALLOM,DEMIRALPÇ.Clustrophile2:GuidedVisualClusteringAnalysis[J].IEEETrans-
actionsonVisualizationandComputerGraphics,2019,25(1):267-276.
30
[25] DAS S, CASHMAN D, CHANG R, et al. Beames: Interactive Multimodel Steering, Selection,
andInspectionforRegressionTasks[J].IEEEComputerGraphicsandApplications,2019,39(5):
20-32.
[26] STITZ H, GRATZL S, AIGNER W, et al. ThermalPlot: Visualizing Multi-Attribute Time-Series
DataUsingaThermalMetaphor[J].IEEETransactionsonVisualizationandComputerGraphics,
2016,22(12):2594-2607.
[27] LI J, CHEN S, ZHANG K, et al. COPE: Interactive Exploration of Co-Occurrence Patterns in
SpatialTimeSeries[J].IEEETransactionsonVisualizationandComputerGraphics,2019,25(8):
2554-2567.
[28] WALKERJ,BORGOR,JONESMW.TimeNotes:AStudyonEffectiveChartVisualizationand
InteractionTechniquesforTime-SeriesData[J].IEEETransactionsonVisualizationandComputer
Graphics,2016,22(1):549-558.
[29] JÄCKLED,FISCHERF,SCHRECKT,etal.TemporalMDSPlotsforAnalysisofMultivariate
Data[J].IEEETransactionsonVisualizationandComputerGraphics,2016,22(1):141-150.
[30] WUY,ZHU-TIANC,SUNG,etal.StreamExplorer:AMulti-StageSystemforVisuallyExploring
Events in Social Streams[J]. IEEE Transactions on Visualization and Computer Graphics, 2018,
24(10):2758-2772.
[31] LIUD,XUP,RENL.TPFlow:ProgressivePartitionandMultidimensionalPatternExtractionfor
Large-ScaleSpatio-TemporalDataAnalysis[J].IEEETransactionsonVisualizationandComputer
Graphics,2019,25(1):1-11.
[32] TKACHEVG,FREYS,ERTLT.LocalPredictionModelsforSpatiotemporalVolumeVisualiza-
tion[J].IEEETransactionsonVisualizationandComputerGraphics,2021,27(7):3091-3108.
[33] WEBBGI,LEELK,GOETHALSB,etal.AnalyzingConceptDriftandShiftfromSampleData
[J].DataMiningandKnowledgeDiscovery,2018,32(5):1179-1199.
[34] STIGLICG,KOKOLP.InterpretabilityofSuddenConceptDriftinMedicalInformaticsDomain
[C].in:IEEEInternationalConferenceonDataMiningWorkshops.2011:609-613.
[35] PRATT K B, TSCHAPEK G. Visualizing Concept Drift[C]. in: Proceedings of ACM SIGKDD
InternationalConferenceonKnowledgeDiscoveryandDataMining.2003:735-740.
31
[36] DEMŠAR J, BOSNIĆ Z. Detecting Concept Drift in Data Streams Using Model Explanation[J].
ExpertSystemswithApplications,2018,92:546-559.
[37] YAOY,FENGL,CHENF.ConceptDriftVisualization[J].TheJournalofInformationandCom-
putationalScience,2013,10:3021-3029.
[38] HUANG R, XU B, SCHUURMANS D, et al. Learning with a Strong Adversary[A/OL]. 2016.
arXiv:1511.03034.https://arxiv.org/abs/1511.03034.
[39] CROCEF,HEINM.MinimallyDistortedAdversarialExampleswithaFastAdaptiveBoundary
Attack[C].in:ProceedingsofInternationalConferenceonMachineLearning.2020:2196-2205.
32
致谢
做项目写论文时无数次想快点写下意味着论文结束的致谢，但真当文末至此写
致谢时，似乎又成了最难下笔的部分。想说的很多，却没能力用恰当的辞藻修饰出来，
那便想到哪写到哪，让论文的最后自由些。一路走来弯弯绕绕磕磕绊绊谈不上轻松，
幸得贵人相助，不至于迷失自我，仍在正路上。
首先最应该感谢我的父母，养育之恩，无以为报。父母对我的影响更多体现在用
言传身教教会了我如何做一个健全的人，肉体健康，精神富足，三观正常。自开始上
学，迷茫不断，自我怀疑，担忧未来，稍有不慎就可能沉沦无法自拔，父母一次次陪
伴引导鼓励包容让我坚持走到大学毕业，是何其幸运。父母支持我的一切决定，支持
我所有的梦想，支持我去做不随大流的人，待到碰壁受伤时回家疗伤，又是何其幸福。
其次感谢我的老师，我想感谢我的指导老师马昱欣老师，老师在我大学时期最迷
茫不知所措的时候接收了我成为老师的学生。做毕设时，从选题到设计到实现，带着
我一步一步解决项目遇到的困难，每周数次的指导和讨论，让我受益匪浅。我想感谢
沈昀老师，沈老师是我计算机生涯的引路人，在我大二大三时专业课上不厌其烦的为
我解答现在看来很基础的问题，在我自我怀疑的时候告诉我其实我不比其他学生学
的差。
感谢朱昉学长在本项目的后端算法提供莫大的帮助，学长带我进入对抗训练领
域，由浅入深的教我对抗训练方法的实现，同时在可视化设计方面提供建议，培养我
自主科研的思路和习惯，本系统的成功实现离不开朱昉学长的帮助。
感谢我在大学之前结交的挚友们，在四年时间里虽远隔千里却温情仍在，感谢我
的初中同学吴小晔、蔡姝歆等，高中同学张艺瀚、施林婕、田振宇等，每当在深圳感
到清冷孤独时，总有你们能陪我彻夜长谈。
感谢大学时期结交的朋友，作为一个不善社交不主动社交的人，能与你们成为朋
友是我的荣幸。室友们在生活学习上相互帮助，让我有四年和谐的住宿生活；羽毛球
队的队友们一起训练，相互鼓励，特别感谢王霆瀚、李卓昀两位特别优秀的师兄在羽
毛球、生活、学习上的全面帮助，二位是大学时期和我关系最好的朋友，也是将自身
33
经验倾囊相授指导我四年大学生涯的贵人，感激不尽；感谢一位特殊的网友明昭，在
我深陷抑郁的时候给我继续往下走的动力，给我鼓起勇气拨云见日的理由。
最后的感谢送给自己，虽得贵人相助，但这路是一步一步自己走过来的，路上的
美丽风景或是崎岖坎坷，再回首，已成记忆，更是财富。感谢自己在不管多少次动摇
产生放弃念头时，内心那股不服输不信命的火光都能迸发出难以置信的能量；感谢自
己追寻梦想，学业、运动、艺术都未曾丧失兴趣，梦还还在继续，愿梦醒时分我已登
临顶峰；感谢自己当社会风气和普遍认知逐渐下沉时，能坚守一隅，不被外物裹挟，
不随波逐流，出淤泥而不染，归来仍是少年。
感谢一路上涌现的爱和付出。
感谢所有挣扎、遗憾、不甘与曲折。
感谢冰冷现实中永不凋零的彩色的浪漫主义。
34