分类号 编 号
U D C 密 级
本科生毕业设计（论文）
题 目： 交互式视频理解系统性能优化的策略研究
姓 名： 刘向荣
学 号： 12010513
院 系： 计算机科学与工程系
专 业： 计算机科学与技术
指导教师： 宋 轩
2024 年 6 月 7 日
诚信承诺书
1.本人郑重承诺所呈交的毕业设计（论文），是在导师的指导下，独立
进行研究工作所取得的成果，所有数据、图片资料均真实可靠。
2.除文中已经注明引用的内容外，本论文不包含任何其他人或集体已
经发表或撰写过的作品或成果。对本论文的研究作出重要贡献的个人和集
体，均已在文中以明确的方式标明。
3.本人承诺在毕业论文（设计）选题和研究内容过程中没有抄袭他人
研究成果和伪造相关数据等行为。
4.在毕业论文（设计）中对侵犯任何方面知识产权的行为，由本人承
担相应的法律责任。
作者签名：
2024 年 6 月 7 日
交互式视频理解系统性能优化的策略研究
刘向荣
（计算机科学与工程系 指导教师：宋轩）
[摘要] ：随着互联网的飞速发展，视频已成为人们日常生活中不可或
缺的娱乐和信息来源。在这种背景下，随着人工智能技术的飞速发展，交
互式视频理解系统逐渐成为研究和应用的新兴热点。这类系统通过深入理
解视频内容，提供与视频直接相关的互动对话能力，极大地丰富了用户的
互动体验。本文基于一个名为 VideoChat 的初始项目，研究了交互式视频理
解系统性能优化的策略。我们首先通过关键帧提取技术对视频进行预处理，
使得系统能够处理更长的视频序列，从而扩大了系统的应用范围。此外，
我们还加入了语音识别模块，并引入了文字识别技术，通过提高对视频中
语音和文字信息的理解能力，显著提升了视频理解的准确度。我们还通过
优化提示词并使用更高精度的视觉模型，有效减轻了模型在生成响应时出
现的幻觉问题。本研究的核心贡献在于为 VideoChat 系统引入了几项关键技
术优化：一是通过有效的视频预处理提高了系统处理长视频的能力；二是
通过整合语音和文字识别技术增强了对视频内容的理解深度；三是优化提
示词和使用高精度视觉模型，提高了系统的总体性能和输出质量。实验结
果表明，这些优化措施显著提高了系统的性能，为交互式视频理解领域提
供了新的技术路径和应用潜力。未来，我们计划进一步探索多模态数据处
理在视频理解系统中的应用，以实现更加准确和自然的用户交互体验。代
码可在 https://github.com/lxr12010513/video_chat_with_ChatGPT_pro 上获取。
[关键词]：视频理解; 视觉问答; 多模态; 大语言模型
[ABSTRACT]: With the rapid development of the internet, videos have
become an indispensable source of entertainment and information in daily life.
Against this backdrop, as artificial intelligence technology rapidly advances,
interactive video understanding systems have emerged as a hot topic in both
research and application. These systems enrich user interaction experiences by
providing dialogue capabilities directly related to the video content through a
deep understanding of the video. This paper, based on an initial project named
VideoChat, explores strategies for optimizing the performance of interactive
video understanding systems. Initially, we preprocess the video using keyframe
processing techniques, enabling the system to handle longer video sequences
and thus broadening its application scope. Additionally, we have integrated a
voice recognition module and introduced text recognition technology,
significantly enhancing the accuracy of video understanding by improving the
understanding of voice and text information within videos. We have also
optimized prompt words and used a higher precision visual model, effectively
mitigating the hallucination issues in model response generation. The core
contributions of this study include several key technical optimizations for the
VideoChat system: 1) improving the system's ability to handle long videos
through effective video preprocessing; 2) deepening the understanding of video
content by integrating voice and text recognition technologies; 3) enhancing the
overall performance and output quality of the system by optimizing prompt
words and using high-precision visual models. Experimental results demonstrate
that these optimizations significantly enhance the system's performance, offering
new technological paths and potential applications in the field of interactive
video understanding. In the future, we plan to further explore the application of
multimodal data processing in video understanding systems to achieve more
accurate and natural user interactions. All of our code is available at
https://github.com/lxr12010513/video_chat_with_ChatGPT_pro.
[Keywords]: Video Understanding; Visual Question Answering; Multimodal;
Large Language Models
目录
1. 绪论........................................................1
1.1 研究背景与意义..............................................1
1.2 国内外研究现状..............................................1
1.3 研究内容....................................................3
1.4 贡献与创新..................................................3
2. 交互式视频理解系统..........................................5
2.1 交互式视频理解系统的定义...................................5
2.2 VideoChat 系统概述...........................................5
3. 性能优化策略................................................6
3.1 视频时长和上下文限制优化策略................................6
3.2 减轻模型幻觉策略............................................6
4. 关键帧提取..................................................7
4.1 关键帧提取技术原理..........................................7
4.2 关键帧提取实施步骤..........................................8
4.3 关键帧提取优化结果..........................................9
5. 语音识别....................................................9
5.1 语音识别技术概述...........................................9
5.2 OpenAI Whisper 模型简介.....................................10
5.3 Whisper 模型在项目中的应用..................................10
5.4 增加语音识别模块结果......................................11
6. 文本识别...................................................11
6.1 文本识别技术概述...........................................11
6.2 PaddleOCR 技术简介.........................................12
6.3 PaddleOCR 在项目中的应用...................................12
6.4 增加文本识别模块结果.......................................13
7. 视觉模型与提示词优化.......................................14
7.1 视觉模型优化..............................................14
7.2 提示词优化................................................16
7.3 视觉模型与提示词优化结果...................................17
8. 实验评估...................................................18
8.1 实验概述..................................................18
8.2 关键帧处理技术的影响......................................18
8.3 语音识别与文本识别技术的影响...............................19
8.4 视觉模型与提示词优化的影响................................19
8.5 实验结论与讨论.............................................25
9. 总结与展望.................................................25
9.1 研究总结..................................................25
9.2 未来工作...................................................26
参考文献......................................................27
1.绪论
1.1 研究背景与意义
在数字化时代，视频内容已成为互联网上最受欢迎和消耗最快的媒介之一。尤其是
随着社交媒体和视频分享平台的兴起，视频数据的量级呈指数级增长。这些视频内容的
快速增长，不仅带来了信息的丰富性，也带来了信息过载的问题，急需有效的视频理解
和管理技术来解决。交互式视频理解系统，作为一种新兴的技术解决方案，能够理解视
频内容并支持与用户的交互对话，提供个性化的视频内容理解和检索服务，从而极大地
提升了视频内容的可接入性和互动性。
然而，尽管近年来在此领域取得了显著进展，交互式视频理解系统在处理长视频、
实现高准确度视频理解方面仍面临挑战。传统视频理解技术往往依赖于对整个视频的全
面分析，这不仅计算成本高，而且难以实时响应用户的交互需求。因此，如何优化交互
式视频理解系统的性能，使其能够更高效地处理更长的视频，并提高视频理解的准确度，
成为了亟需解决的问题。
1.2 国内外研究现状
近年来，大规模语言模型（LLMs）发展迅速。预训练于庞大数据集上的大规模语言
模型引入了一种新的上下文学习能力。这使得它们能够通过提示处理多种任务，无需微
调。ChatGPT是建立在这一基础上的首个突破性应用，包括生成代码和调用其他模型的
工具或API的能力。许多研究正在探索使用LLMs（如ChatGPT）调用视觉模型API来
解决计算机视觉领域的问题。
LLMs与视频理解能力相结合，提供了更复杂的多模态理解优势，使它们能够处理
和解释视觉和文本数据之间的复杂交互。类似于其在自然语言处理中的影响，这些模型
充当更通用的任务解决器，通过利用其从大量多模态数据中获得的广泛知识库和上下文
理解，处理更广泛的任务[1]。
国内外研究者提出了多种基于大语言模型的视频理解方法，这些方法在视频理解、
生成及多模态融合等方面展现出了巨大的潜力。以下是一些代表性研究及其主要创新点
和实验效果:
1
ChatVideo系统提出了以轨迹为中心的范式，将视频理解为不同轨迹的集合，每个
轨迹代表一个实例，通过多个视频基础模型（ViFMs）对轨迹进行注释，并存储在数据
库中，通过数据库管理器实现多模态视频问答。实验结果表明，ChatVideo在多个场景
中能够有效地回答关于视频外观、运动和音频的问题，展示了其在视频问答和检索任务
中的潜力[2]。
VAST提出了一种全新的多模态预训练范式，通过自动构建大规模多模态数据集
（VAST-27M），实现视觉、语音、字幕和文本的统一表征。VAST在多模态任务中的
表现优异，尤其在视觉-文本、语音-文本和视频-文本任务上取得了新的最优结果，验证
了多模态协同学习的有效性[3]。
mPLUG-video发布了最大的中文视频-语言预训练数据集Youku-mPLUG，涵盖跨模
态检索、视频字幕生成和视频分类等任务。实验结果显示，mPLUG-video在中文视频理
解任务上表现出色，显著提升了模型在视频分类和字幕生成等任务上的准确性[4]。
Macaw-LLM提出了一种新的多模态大语言模型，通过对齐多模态特征与大语言模
型的文本特征，实现图像、音频、视频和文本的无缝集成。实验结果表明，Macaw-LLM
在多模态理解任务上具有显著的性能提升，特别是在图像和视频问答任务中表现优异
[5]。
LanguageBind提出了基于语言的多模态预训练方法，通过对比学习将每个模态与语
言对齐，实现多模态语义对齐。实验结果显示，LanguageBind在视频、音频、深度和红
外数据的零样本分类和检索任务中均取得了优异成绩，展示了其在多模态任务中的广泛
适用性[6]。
Video-LLaVA通过对齐图像和视频表征到统一的视觉特征空间，实现了图像和视频
的联合训练。实验表明，Video-LLaVA在图像和视频理解任务上均表现出色，特别是在
图像问答和视频问答任务中展示了强大的能力[7]。
Chat-UniVi提出了使用动态视觉token统一表示图像和视频的方法，通过多尺度表
示让模型同时感知高层语义概念和低层视觉细节。实验结果显示，Chat-UniVi在图像和
视频理解任务上均取得了优异的性能，展示了其在多模态对话任务中的潜力[8]。
GPT4Video提出了用于视频理解和生成的统一多模态框架，展示了在视频理解和生
成场景中的出色能力。实验结果表明，GPT4Video在视频问答和视频生成任务上具有显
著的性能优势，同时在多模态安全性上表现优异[9]。
2
1.3 研究内容
本文基于github开源项目VideoChat 项目的video_chat_with_ChatGPT版本[10]进
行研究与优化。该项目是一个交互式视频理解系统，通过提取视频中的图像描述并获取
视频文本描述，将其输入大语言模型中，实现与用户就视频内容进行交流。[11]。针对
VideoChat系统在处理长视频以及视频理解准确度上的不足，本文引入了关键帧处理技
术预处理视频，增强了系统处理长视频的能力，并集成了语音识别模块与文字识别模块，
显著提升了视频理解的准确性。此外，本研究还通过优化视觉模型与优化提示词，有效
减轻了模型在生成响应时出现的幻觉问题。
本研究首先介绍交互式视频理解系统的研究背景和意义，并综述了现有技术的局限
性及本研究的创新点。接着，详细阐述了关键帧处理技术、语音识别和文字识别模块的
集成方法以及视觉模型与提示词的优化，包括技术原理、实现步骤和预期效果。通过一
系列实验，验证了这些优化策略的有效性，展示了系统在处理长视频、提高视频理解准
确度以及减轻模型幻觉方面的显著改进。
1.4 贡献与创新
本文的贡献在于对VideoChat 项目的video_chat_with_ChatGPT版本[10]进行了全
面优化，通过加入关键帧提取的预处理提高了系统处理短视频的效率和处理长视频的能
力，通过加入语音识别模块和文本识别模块提高了系统应对用户有关语言和文本信息的
提问时给出符合视频内容和用户预期回答的能力，通过加入更精确的视觉-语言模型以
及优化大语言模型提示词减轻了模型幻觉问题，提高了系统的可用性和用户体验。
优化后的VideoChat代码可https://github.com/lxr12010513/video_chat_with_ChatGPT_pro
上获取。
本文的创新之处在于提出并实证了有效的优化策略，提高了VideoChat 系统的实用
性和用户体验，展示了大语言模型作为通用的任务解决器，搭配其他技术与模型的方式
在处理各类复杂问题上的可行性与潜力，为后续相关领域的研究提供了新的思路和方法。
3
图1：VideoChat系统框架
图2：优化后VideoChat系统框架
4
2.交互式视频理解系统
2.1 交互式视频理解系统的定义
交互式视频理解系统是一种利用人工智能技术，尤其是计算机视觉和自然语言处理
技术，来分析和理解视频内容的系统。该系统能够识别视频中的对象、场景、活动以及
情感等多维度信息，并将这些信息转换为文本描述。这种系统支持用户与之进行交互，
即用户可以通过自然语言查询视频内容，系统则基于对视频的理解提供相应的回答或者
执行相关的动作。这种交互性使得用户能够更直观、更灵活地获取视频内容的深层次信
息，极大地丰富了视频的应用场景和用户体验。
2.2 VideoChat 系统概述
VideoChat系统是一个综合视频相关任务的多轮视频问答系统，它通过文字定义任
务，并在实时推断中不需要或仅需要极少的示例来进行学习。在这种形式中，大型语言
模型（LLM）被视为一个通用的视频任务解码器，将视频相关的描述或嵌入转换成人类
可理解的文本。这一过程对于使用基础模型处理各种视频应用是较为有效的。
系统首先使用视觉模型从视频中提取概念，将视频或图像转换为文本描述或嵌入。
然后，基于用户的问题，系统使用大型语言模型解码任务预测。技术上，理想的端到端
的以聊天为中心的视频理解系统应该使用视频/视觉模型（编码器）将视觉序列转换为
潜在特征，供大型语言模型使用，确保系统的整体可微分性。
VideoChat项目的chat with ChatGPT版本主要使用的方法为通过文本化视频流的
VideoChat。此方法使用多个视觉模型将视频数据转换为文本格式，然后创建专门设计
的提示词来时间结构化预测文本。系统依赖预训练的大型语言模型来解决用户指定的任
务，通过基于视频文本描述回应问题。例如，对于给定的视频，使用 ffmpeg 从视频中
得到视频帧。通过将提取的帧输入到不同模型中，系统获取动作标签、帧摘要、视频标
签、详细描述、对象位置坐标、视频叙述、时间戳和其他片段相关的细节。然后，系统
综合考虑时序，生成带时间戳的视频文本描述[10][11]。
5
VideoChat系统各个模块的输入，输出关系可表达如下：
视频输入:
1. 𝑉𝑉 = {𝐹𝐹1,𝐹𝐹2,…,𝐹𝐹𝑁𝑁}
视觉-语言模型:
2. 𝑇𝑇vision = VisionModel(𝑉𝑉)
大语言模型生成回答:
其中 为视频，3为. 视频帧， 为视觉模𝑂𝑂型=生LL成M的(𝑄𝑄动,𝑃𝑃作,𝑇𝑇标vis签ion、)帧摘要等描述文本，
𝑉𝑉
为视觉 𝐹𝐹-𝑖𝑖语言模型， 𝑇𝑇v为isio大n 语言模型生成的回答， 为大语言模型， 为用
户提问， 为大语言模型的提示词。
VisionModel 𝑂𝑂 LLM 𝑄𝑄
𝑃𝑃
3.性能优化策略
目前的VideoChat 系统在视频理解任务上表现良好，但仍有一些不足之处，本章会
探讨VideoChat 系统目前存在的问题以及对应的优化策略。
3.1 视频时长和上下文限制优化策略
由于计算机内存的限制以及大语言模型上下文长度的限制，处理长视频是交互式视
频理解系统面临的一个重大挑战。为了提高系统的效率和准确性，可以采取以下策略：
关键帧提取： 将关键帧提取模块加入VideoChat 系统，通过该技术，系统仅分析
视频中的关键帧而非每一帧，显著降低了数据处理量。关键帧是视频中信息量最大的帧，
能够代表视频的主要内容和变化。通过算法识别并提取这些帧，既能保证视频内容理解
的准确性，又能提高处理速度。
视频分段处理：作为一项潜在的优化策略，视频分段处理策略将长视频分解为较短
的段落，每个段落独立处理。这种策略可以进一步提高系统的灵活性和响应速度，特别
是在处理实时视频流或非常长的视频文件时。通过确定视频内容的自然分割点，系统可
以有效地并行处理多个视频段，从而实现更快的处理速度和更准确的内容理解。
3.2 减轻模型幻觉策略
目前的VideoChat 系统在某些情况下会出现“模型幻觉”问题。在自然语言处理和
视频理解领域，“模型幻觉”是指模型产生的不准确或虚假信息。为了减轻这一问题，
可以考虑采取以下策略：
6
多模态技术：多模态技术结合了视频的视觉信息、音频信息和文本信息，为视频内
容提供了更全面的理解。通过整合和分析这些不同类型的数据，系统可以更准确地捕捉
到视频的含义，减少误解和幻觉的产生。目前已通过加入语音识别模块与文本识别模块
增强了系统的理解能力。
视觉模型与提示词优化：针对视觉-语言模型可能出现的识别错误以及大语言模型
可能出现的理解错误，可以进行视觉模型与提示词优化。使用更精确的视觉模型可以减
少关键帧图像描述出现的幻觉与错误，通过提示词工程优化提示词，可以提高大语言模
型在整合各个模态的信息以及在处理可能包含错误幻觉信息时仍然可以输出正确内容
的能力。
图3：模型幻觉示例：模型的回答中出现了视频中没有出现的“cell phone”，“skateboard”
4.关键帧提取
为了提高VideoChat 系统处理长视频的效率，我们在VideoChat 系统中加入了关键
帧提取技术。关键帧提取技术的主要目的是从视频中识别并提取出那些具有代表性的帧，
即关键帧，以减少后续处理所需的计算量，同时保持视频内容的核心信息不受损失。我
们采用的是一种基于帧间差分的方法，其优势在于能够有效识别出视频中画面变化显著
的瞬间，从而选取这些瞬间作为关键帧。
4.1 关键帧提取技术原理
关键帧提取技术的核心思想基于一个简单而有效的原理：视频帧之间的变化可以通
过它们的帧间差分来衡量。具体来说，将连续的两帧进行相减，得到的差分图像反映了
7
这两帧之间的变化程度。通过计算差分图像的平均像素强度，可以量化帧间的变化大小。
当某一帧与前一帧之间的变化超过一定阈值时，则认为这一帧为关键帧[12]。
4.2 关键帧提取实施步骤
对实施关键帧提取技术的过程涵盖了从视频读取、计算帧间差分到关键帧的提取多
个步骤。首先，系统读取待处理的视频文件，并将其解码为一系列连续的帧。接着，对
视频中的每一对连续帧进行差分计算，进一步计算这些差分图像的平均像素强度，以量
化帧间的变化大小。关键帧的提取可以通过以下三种方法之一实现：一是按照帧间差分
强度对所有帧进行排序，选择差分强度最高的若干帧作为关键帧；二是设定一个差分强
度阈值，选取超过此阈值的帧作为关键帧；三是选择在平均帧间差分强度上呈现局部最
大值的帧，这种方法可以使得提取的关键帧在视频时间轴上分布更均匀。为了提高关键
帧提取的准确性，对平均帧间差分强度时间序列进行平滑处理也是一种有效的技术，这
通常通过各种滤波器实现，目的是去除噪声并避免相似场景的多个帧被同时选为关键帧
[12][13]。
我们在VideoChat 项目中使用的关键帧提取方法为局部最大值法[13]，提取关键帧
的步骤可表达为：
读取视频帧→计算帧之间的差分值→对帧差分值进行平滑处理→获得局部最大的帧
加入关键帧提取后，VideoChat 系统各个模块的输入，输出关系可表达如下：
视频输入:
1. 关键帧提取𝑉𝑉: = {𝐹𝐹1,𝐹𝐹2,…,𝐹𝐹𝑁𝑁}
2. 𝐾𝐾 =
{𝐹𝐹𝑘𝑘1 ,𝐹𝐹𝑘𝑘2 ,…,𝐹𝐹𝑘𝑘𝐾𝐾
}
视觉-语言模型:
3. 𝑇𝑇vision = VisionModel(𝐾𝐾)
. 大语言模型生成回答:
其中 为关键帧4集合， 为关键帧。 𝑂𝑂 = LLM(𝑄𝑄,𝑃𝑃,𝑇𝑇vision)
𝐾𝐾
𝐹𝐹𝑘𝑘𝑖𝑖
8
4.3 关键帧提取优化结果
加入关键帧提取的预处理后，VideoChat系统处理的视频的效率得到了显著得提高，
能够处理的最大视频长度也得到了明显得提升，从3分钟提升到了30分钟。由于关键
帧是视频中具有代表性的帧，在关键帧上进行的图像理解可以在很大程度上代表其邻近
的帧，因此关键帧提取后VideoChat系统在理解视频的能力方面并没有明显的下降现象。
以下是对比：
图4：VideoChat系统增加关键帧提取前后对比
更多的实验对比将在实验评估部分展示。
5.语音识别
目前的VideoChat 系统中还没有语音识别的能力，当用户的提问涉及视频中的语音
信息时，系统无法做出正确回答，因此我们在 VideoChat 系统中加入了语音识别模块，
提高了系统的视频理解能力。我们使用的语音识别模型为OpenAI 的Whisper模型。
5.1 语音识别技术概述
语音识别技术是人工智能领域的一个重要分支，它使计算机能够理解和转录人类的
语音。这项技术涉及声音信号的捕获、转换以及与预定义语音模式的匹配过程，以识别
和转化为文本数据。近年来，随着深度学习技术的快速发展，语音识别的准确度和应用
范围都有了显著的提升。它不仅被广泛应用于个人助理、客户服务和自动字幕生成等领
域，也在交互式视频理解系统中扮演着越来越重要的角色。
9
5.2 OpenAI Whisper 模型简介
Whisper 是由OpenAI 开发的一种先进的深度学习语音识别模型，旨在提供高准确
度的语音转文字服务。该模型经过大量多语言和多口音的语音数据训练，具有出色的跨
语言识别能力和对复杂环境噪声的鲁棒性。Whisper 模型的一个突出特点是其对于不同
类型的语音内容，无论是公开讲话、对话还是技术性讲解，都能实现高质量的转录效果。
此外，它还支持多种语言的识别，使其成为国际化应用中理想的语音识别解决方案
[14][15]。
5.3 Whisper 模型在项目中的应用
在VideoChat 项目中，我们将Whisper 模型集成为系统的核心组件之一，用以提升
视频理解的准确度。通过 Whisper 模型，视频中的语音信息能够被准确地转录为文本，
这为后续的语义理解和用户交互提供了基础。此外，Whisper 模型的多语言识别能力也
使得我们的系统能够支持多种语言的视频内容理解和交流，从而提高了项目的适用性和
用户体验。
我们在VideoChat 项目中根据处理视频的长度选用不同大小的whisper 模型，包括
tiny,base,small,medium,以确保在视频长度较短时提供更精确的识别结果，在视频长度较
长时提供更快速的响应时间。
加入语言识别模块后，VideoChat系统各个模块的输入，输出关系可表达如下：
视频输入:
1. 关键帧提取𝑉𝑉: = {𝐹𝐹1,𝐹𝐹2,…,𝐹𝐹𝑁𝑁}
2. 𝐾𝐾 =
{𝐹𝐹𝑘𝑘1 ,𝐹𝐹𝑘𝑘2 ,…,𝐹𝐹𝑘𝑘𝐾𝐾
}
. 语音识别:
3 𝑇𝑇voice = VoiceModel(𝐴𝐴)
视觉-语言模型:
4. 𝑇𝑇vision = VisionModel(𝐾𝐾)
. 大语言模型生成回答:
其中 为5语音识别结果， 𝑂𝑂 =为语LL音M(识𝑄𝑄别,𝑃𝑃模,𝑇𝑇v型oic，e,𝑇𝑇v为isi视on频) 的音频部分
𝑇𝑇voice VoiceModel 𝐴𝐴
10
5.4 增加语音识别模块结果
加入语音识别模块后，VideoChat系统的视频理解能力得到提高，能够在回答中加
入语音模态的信息，能够回答用户涉及视频语音方面的提问，以下是对比：
图5：VideoChat系统增加语音识别模块前后对比
更多的实验对比将在实验评估部分展示。
6.文本识别
与语音识别相似，目前的VideoChat 系统中还没有文本识别的能力，当用户的提问
涉及视频中的文本信息时，系统无法做出正确回答，因此我们在VideoChat 系统中加入
了文本识别模块，提高了系统的视频理解能力。我们使用的文本识别模型为飞浆的
PaddleOCR 模型。
6.1 文本识别技术概述
文本识别技术，也被称为光学字符识别（OCR），是一种将图片中的文本转换成机
器可读文本的技术。这一技术通过分析图像中的字符形状和布局来识别和转换文字内容，
使之能够被计算机程序进一步处理和分析。文本识别在文档数字化、自动化表格数据提
取以及实时信息翻译等多种应用中扮演关键角色。随着机器学习和人工智能技术的进步，
文本识别的准确性和效率得到了极大的提高，从而扩展了其在多媒体内容分析和交互式
应用中的实用性。
11
6.2 PaddleOCR 技术简介
PaddleOCR 是由百度开源的一个轻量级、强大的OCR系统，它基于PaddlePaddle 深
度学习框架开发。PaddleOCR支持多种语言的文字识别，并具有高准确率和快速处理能
力。该技术涵盖了文本检测、文本识别和文本校正等全套流程，使其能够从各种类型的
图像中准确提取文本内容。PaddleOCR的一个主要优点是其模块化设计，用户可以根据
需求选择不同的文本检测和识别模型，或者自定义训练模型以适应特定的应用场景。
[16]
6.3 PaddleOCR 在项目中的应用
在VideoChat 项目中，PaddleOCR 用于提升系统对视频中文本信息的理解能力。项
目中的具体应用主要为视频图像中的文本提取。系统通过PaddleOCR识别并提取视频
帧中的文本信息，这些文本信息对于理解视频场景内容及其上下文较为关键。
由于PaddleOCR识别文本的高效性，在处理较短的视频的时候，我们可以将一些
不是关键帧的图像帧也进行文本识别，这样可以确保文本内容识别的完整性。同时，为
了防止视频画面静止不变的时造成的重复冗余识别结果，我们还使用了文本相似度算法
去除了冗余的结果。
加入文本识别模块后，VideoChat系统各个模块的输入，输出关系可表达如下：
视频输入:
1. 关键帧提取:𝑉𝑉 = {𝐹𝐹1,𝐹𝐹2,…,𝐹𝐹𝑁𝑁}
2. 𝐾𝐾 =
{𝐹𝐹𝑘𝑘1 ,𝐹𝐹𝑘𝑘2 ,…,𝐹𝐹𝑘𝑘𝐾𝐾
}
. 语音识别:
3 𝑇𝑇voice = VoiceModel(𝐴𝐴)
视觉-语言模型:
4. 𝑇𝑇vision = VisionModel(𝐾𝐾)
文本识别:
5. 𝑇𝑇ocr = OCRModel(𝐾𝐾)
. 大语言模型生成回答:
其中 为文本6识别结果， 为文𝑂𝑂本=识LL别M模(𝑄𝑄型,𝑃𝑃。,𝑇𝑇 voice,𝑇𝑇vision,𝑇𝑇ocr)
𝑇𝑇ocr OCRModel
12
图6：文本识别模块的识别结果示例
6.4 增加文本识别模块结果
加入语音识别模块后，VideoChat系统的视频理解能力得到提高，能够在回答中加
入文本模态的信息，能够回答用户涉及视频文本方面的提问，以下是对比：
图7：VideoChat系统增加文本识别模块前后对比
更多的实验对比将在实验评估部分展示。
13
7.视觉模型与提示词优化
7.1 视觉模型优化
在交互式视频理解系统中，视觉模型扮演着至关重要的角色。这些模型通过分析视
频中的图像，识别物体、场景以及行为，为后续的文本生成和对话交互提供数据支持。
高效的视觉模型不仅可以提升系统的响应速度，还能增强系统对视频内容的理解深度。
在VideoChat 项目的video_chat_with_ChatGPT版本中，使用的视觉-语言模型包括
Tag2Text和GRiT，其中Tag2Text 是github开源项目recognize-anything的一款由标签
引导的视觉语言模型，它可以同时支持标签和全面的描述生成[17][18]，GRiT 是一种
用于对象理解的生成性区域到文本转换器，可以生成图像的密集描述[19][20]。在一些
复杂的场景中，Tag2Text 生成的图像描述过于简洁，不利于VideoChat 系统理解视频中
细节层面的信息，而GRiT生成的密集描述会有一些错误识别结果，会导致VideoChat
系统对对用户的提问给出错误的回答。因此，在VideoChat 系统中加入更精确的视觉模
型对系统的性能会有明显的优化。
为了优化交互式视频理解系统的视觉识别的精确度，本项目引入了moondream模
型。moondream 是一个参数量仅为1.6亿的计算机视觉模型，专门设计用于回答关于图
像的实际问题。由于其较小的模型大小，moondream 能够在各种设备上运行，包括移动
电话和边缘设备如树莓派。这一特点极大地提高了系统的灵活性和可扩展性。
moondream的处理速度快，能够迅速处理图像，这对于需要实时反馈的交互式系统尤为
重要[21]。
在将moondream 模型集成到VideoChat 项目的过程中，出现了依赖库transformers
版本无法兼容的问题，解决方法为使用subprocess 将环境隔离，在隔离的环境中导入下
载到本地的不同版本的transformers 库：
图8：主文件app.py:正常导入transformers,使用subprocess在单独的环境中运行moondream
图9：视觉模型文件moondream.py:导入本地另一个版本的transformers
14
在牺牲了一部分响应速度的前提下，moondream 模型在理解图像内容的能力上比
Tag2Text和GRiT更优，能够生成大语言模型更容易理解的关键帧图像描述：
图10： moondream视觉模型与Tag2Text, GRiT模型生成图像描述的对比（一）
图11： moondream视觉模型与Tag2Text, GRiT模型生成图像描述的对比（二）
15
7.2 提示词优化
由于加入了语音识别模块与文本模块，输入到大语言模型的信息变多，大语言模型
在一些情况下会出现忽视某些信息的情况。我们考虑使用优化提示词的方式来提醒大语
言模型应该考虑所有模态中包含的信息，并告知大语言模型各个模态信息的识别准确率，
如语音识别与文本识别的结果较为准确，而GRiT识别的结果有可能出现错误。通过优
化提示词的方式，大语言模型能够输出更符合视频内容，更符合用户预期的回答。
图12： 优化前的大语言模型提示词
图13： 优化后的大语言模型提示词
16
进行视觉模型和提示词优化后，VideoChat 系统各个模块的输入，输出关系如下：
视频输入:
1. 关键帧提取𝑉𝑉: = {𝐹𝐹1,𝐹𝐹2,…,𝐹𝐹𝑁𝑁}
2. 𝐾𝐾 =
{𝐹𝐹𝑘𝑘1 ,𝐹𝐹𝑘𝑘2 ,…,𝐹𝐹𝑘𝑘𝐾𝐾
}
. 语音识别:
3 𝑇𝑇voice = VoiceModel(𝐴𝐴)
视觉-语言模型:
4. 𝑇𝑇vision = VisionModel𝑜𝑜𝑜𝑜𝑜𝑜(𝐾𝐾)
文本识别:
5.. 大语言模型𝑇𝑇 生ocr 成= 回O 答CR:Model(𝐾𝐾)
其中 6 为更精确的视觉𝑂𝑂-=语L言LM模(型𝑄𝑄,，𝑃𝑃opt,𝑇𝑇为voi 优ce,化𝑇𝑇v 后isio 的n,大𝑇𝑇o 语cr)言模型提示词。
VisionModel𝑜𝑜𝑜𝑜𝑜𝑜 𝑃𝑃opt
7.3 视觉模型与提示词优化结果
使用了更精确的视觉模型moondream 后， VideoChat 系统在视觉理解方面出现的幻
觉明显减少。进行提示词优化后，VideoChat系统更够更加全面地整合多个模态的信息
并将这些信息关联起来，识别精确度更高的模态将会拥有更高的可信度优先级，从而更
精确地回答用户的提问，以下是对比：
图14：VideoChat系统视觉模型与提示词优化前后对比
更多的实验对比将在实验评估部分展示。
17
8.实验评估
8.1 实验概述
为了评估本研究中提出的性能优化策略的有效性，我们进行了一系列实验。这些实
验旨在比较经过优化的VideoChat 系统与原始VideoChat 系统以及另一个交互式视频理
解系统mPLUG-Owl 的性能差异。以下部分将分别概述这些系统，以及我们采用的各种
优化策略对性能的影响。
VideoChat系统：VideoChat 是一个交互式视频理解系统，通过提取视频中不同的图
像描述来获得视频的文本描述，然后将这些描述输入到大语言模型MiniGPT中。该系
统允许用户与模型就视频内容进行交流对话。实验使用的是VideoChat 项目部署的在线
版本[10][11]。
mPLUG-Owl 系统：mPLUG-Owl 是由阿里达摩院开发的系统，旨在通过模块化学
习赋予大型语言模型（LLMs）多模态能力。该系统集成了基础LLM、视觉知识模块和
视觉抽象模块，支持多种模态并通过模态协作促进不同的单模态和多模态能力。实验使
用的是mPLUG-Owl 项目部署的在线版本[22][23]。
表 1 实验环境配置表
项目 配置信息
操作系统 Microsoft Windows [10.0.22631.3593]
语言 Python3.10.10
框架 Pytorch2.2.1+cuda12.1
CPU 13th Gen Intel(R) Core(TM) i9-13900HX
GPU NVIDIA GeForce RTX 4060(8GB)
内存 16GB
8.2 关键帧处理技术的影响
通过实现关键帧提取技术，我们的系统能够显著提高处理长视频的能力。在实验中，
通过对比原始VideoChat 系统和经过优化的系统，我们发现优化后的系统能够处理的最
大视频时长从3分钟提升至30分钟。视频五（图20），视频六（图21）的视频时长分
18
别为16分钟与38分钟。mPLUG-Owl 以及没有增加关键帧提取的VideoChat 系统无法
处理这类长视频。这一结果表明，关键帧提取技术有效减少了数据处理量，提高了系统
的效率。
除此之外，关键帧提取在减少了处理的图像帧的同时，还保留了大部分的视觉模态
的信息。视频一（图15），视频二（图16），视频三（图17），视频四（图18）都表
明，增加关键帧提取的 VideoChat 系统仍然可以给出符合视频内容与用户预期的回答。
这一结果表明，关键帧提取技术在有效减少数据处理量，提高系统的效率的情况下，保
证了响应结果的质量。
8.3 语音识别与文本识别技术的影响
加入语音识别模块与文本识别模块后，系统的视频理解能力得到了显著提升。通
过语音识别技术与文本识别技术，系统能够理解视频中包含的音频信息与文本信息，增
强了模型对视频内容的全面理解。在对视频四（图19），视频五（图20），视频七（图
22，图23）进行处理时，优化后的VideoChat 系统展现出了优于原系统和mPLUG-Owl
的性能，能够准确理解并响应用户对视频中的关于语音以及文本方面内容的询问。
8.4 视觉模型与提示词优化的影响
为进一步提升视频理解系统的性能，我们加入了更精确的视觉模型 moondream，
并调整了用于生成响应的提示词。视觉模型moondream能更准确地识别视频中的视觉
元素，而精心设计的提示词则有助于模型更好地理解和响应用户的查询。
通过使用高精度的视觉模型，系统能够详细识别和解析视频中的复杂场景和动作，
从而减少误解和提高信息提取的准确性。例如，在处理视频四（图18）和视频八（图
24）时，优化后的视觉模型表现出比原系统中的视觉模型更高的识别准确率。
此外，我们重新设计的提示词策略通过更精确和相关的查询，明显提高了系统对
用户意图的响应质量。在视频四（图18）和视频八（图24）的实验中，通过对比优化
提示词前后的系统，优化提示词后的系统在处理复杂查询时的响应更加准确。
19
图15：视频一实验结果
图16：视频二实验结果
20
图17：视频三实验结果
图18：视频四实验结果（一）
21
图19：视频四实验结果（二）
图20：视频五实验结果
22
图21：视频六实验结果
图22：视频七实验结果（一）
23
图23：视频七实验结果（二）
图24：视频八实验结果
24
8.5 实验结论与讨论
实验结果表明，本研究中提出的性能优化策略显著提高了交互式视频理解系统处
理长视频的能力和视频内容理解的准确度。通过将关键帧处理技术、语音识别技术及文
本识别技术集成到VideoChat 项目中，我们的系统在处理更长视频及理解视频中的音频
和文字信息方面展现出了显著的优势。
此外，使用更精确的视觉模型和精心设计的提示词策略也显著提高了系统对视频
内容的理解深度和响应准确性。这些改进使得系统在处理包含复杂视觉和文本信息的长
视频时，比原始系统和mPLUG-Owl 系统表现出更好的性能，特别是在多模态理解能力
方面。
由于gpt-4模型的api花费较为高昂，本研究的实验评估主要基于与原系统的对
比和定性分析，但结果已充分证明了通过关键帧处理、语音与文本识别以及视觉模型优
化和提示词策略改进所带来的优化策略的有效性。未来，我们计划采用更严格的定量实
验方法，以进一步验证这些优化策略的具体效果和应用潜力，从而为交互式视频理解系
统的发展提供更加深入的技术支持。
9.总结与展望
9.1 研究总结
本研究围绕交互式视频理解系统性能优化的策略展开，以VideoChat项目为基础，
探索了通过关键帧处理技术、语音识别模块、文本识别模块以及使用更精确的视觉模型
和提示词优化来提升系统性能的可行性和有效性。通过这些优化措施，我们旨在解决交
互式视频理解系统在处理长视频和提高视频内容理解准确度方面的挑战。
关键帧处理技术的引入显著提高了系统处理长视频的能力，通过仅分析视频中的
关键帧而非每一帧，有效降低了数据处理量，从而加快了视频处理速度，并保持了视频
内容理解的准确性。此外，加入语音识别模块和文本识别模块增强了系统对视频中语音
和文字信息的理解能力，使系统能够更准确地捕捉视频内容，提高了视频理解的准确度。
通过引入更精确的视觉模型和优化提示词，系统在处理视觉信息和生成响应方面的性能
也得到了提升。
25
本研究的实验评估展示了优化后的系统在处理更长视频及理解视频中的音频信
息和文本内容方面相比原VideoChat 系统以及mPLUG-Owl 系统的显著优势。这证明了
关键帧处理技术、语音识别技术、文本识别技术以及视觉模型优化和提示词优化在交互
式视频理解系统中的应用价值，同时也指出了这些技术在优化系统性能方面的潜力。
综上所述，本研究通过引入关键帧处理技术、语音识别模块、文本识别模块、更
精确的视觉模型和提示词优化，成功地提升了交互式视频理解系统处理长视频的能力和
视频内容理解的准确度。这些成果不仅提高了VideoChat 系统的实用性和用户体验，也
为未来交互式视频理解系统的研究和开发提供了新的思路和方法。
9.2 未来工作
本研究对交互式视频理解系统性能进行了初步优化，通过关键帧处理技术和语音
识别模块的集成，显著提高了系统处理长视频的能力及视频理解的准确度。然而，考虑
到视频内容理解和交互对话的复杂性，未来的工作将集中在以下几个方面以进一步提升
系统性能和用户体验。
优化关键帧提取算法：当前的关键帧提取方法虽然有效提高了处理速度，但仍有
改进空间，特别是在保持视频内容完整性方面。未来，我们将探索更高效的关键帧提取
算法，以减少信息丢失，确保关键帧能更全面地代表视频内容。
视频分段处理：对于长视频，即使通过关键帧处理，也可能因为视频长度导致处
理效率下降或理解不准确。因此，将视频分段处理，可以更灵活地处理大规模视频数据，
同时降低系统的计算压力，提高响应速度。
进一步减少模型幻觉：在某些情况下，模型仍然可能产生与视频内容不符的“幻
觉”回应。为了减少这种情况的发生，未来将研究模型训练和调整策略，通过更精细的
训练数据和模型调整方法，降低幻觉产生的概率，提高回应的准确性和可靠性。
综合性能评估：为了系统地评估优化策略的效果，未来工作将包括综合性能评估，
不仅关注系统处理速度和准确性的提升，也将考虑系统稳定性、用户体验等多维度的评
估指标，在视频理解的不同数据集上进行系统的评价。此外，通过与用户的互动反馈，
进一步调整和优化系统，以满足实际应用需求。
26
参考文献
[1] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An,
Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Feng Zheng, Jianguo Zhang, Ping
Luo, Jiebo Luo, Chenliang Xu. Video Understanding with Large Language Models: A Survey.
(2023). https://doi.org/10.48550/arXiv.2312.17432
[2] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang.
ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System. (2023).
https://doi.org/10.48550/arXiv.2304.14407
[3] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, Jing Liu. VAST: A
Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset. (2023).
https://doi.org/10.48550/arXiv.2305.18500
[4] Haiyang Xu, Qinghao Ye, Xuan Wu, Ming Yan, Yuan Miao, Jiabo Ye, Guohai Xu, Anwen Hu, Yaya
Shi, Guangwei Xu, Chenliang Li, Qi Qian, Maofei Que, Ji Zhang, Xiao Zeng, Fei Huang. Youku-mPLUG:
A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks. (2023).
https://doi.org/10.48550/arXiv.2306.04362
[5] Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi,
Zhaopeng Tu. Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text
Integration. (2023). https://doi.org/10.48550/arXiv.2306.09093
[6] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang,
Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, Li Yuan. LanguageBind: Extending
Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (2023).
https://doi.org/10.48550/arXiv.2310.01852
[7] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, Li Yuan. Video-LLaVA: Learning
United Visual Representation by Alignment Before Projection. (2023).
https://doi.org/10.48550/arXiv.2311.10122
[8] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, Li Yuan. Chat-UniVi: Unified Visual
Representation Empowers Large Language Models with Image and Video Understanding. (2023).
https://doi.org/10.48550/arXiv.2311.08046
[9] Zhanyu Wang, Longyue Wang, Zhen Zhao, Minghao Wu, Chenyang Lyu, Huayang Li, Deng Cai,
Luping Zhou, Shuming Shi, Zhaopeng Tu. GPT4Video: A Unified Multimodal Large Language Model for
lnstruction-Followed Understanding and Safety-Aware Generation. (2023).
https://doi.org/10.48550/arXiv.2311.16511
[10] OpenGVLab. Ask-Anything, https://github.com/OpenGVLab/Ask-Anything
[11] Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., & Qiao, Y. VideoChat:
Chat-Centric Video Understanding. (2023). https://doi.org/10.48550/arXiv.2305.06355
27
[12] Sadiq, Bashir & Muhammad, Bilyamin & Abdullahi, Muhammad & Onuh, Gabriel & Ali,
Abdulhakeem & Babatunde, Adeogun. Keyframe Extraction Techniques: A Review. ELEKTRIKA-
Journal of Electrical Engineering. 19. 54-60.(2020).
https://www.researchgate.net/publication/348169269_Keyframe_Extraction_Techniques_A_Review
[13] monkeyDemon. AI-Toolbox. https://github.com/monkeyDemon/AI-Toolbox
[14] openai. whisper. https://github.com/openai/whisper
[15] Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (n.d.). Robust Speech
Recognition via Large-Scale Weak Supervision. arXiv:2212.04356.
[16] PaddlePaddle. PaddleOCR. https://github.com/PaddlePaddle/PaddleOCR
[17] xinyu1205. recognize-anything, https://github.com/xinyu1205/recognize-anything
[18]Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong
Guo, Lei Zhang. Tag2Text: Guiding Vision-Language Model via Image Tagging. (2023).
https://doi.org/10.48550/arXiv.2303.05657
[19] JialianW. GRiT. https://github.com/JialianW/GRiT
[20] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, Lijuan Wang.
GRiT: A Generative Region-to-text Transformer for Object Understanding. (2022).
https://doi.org/10.48550/arXiv.2212.00280
[21] vikhyat. moondream. https://github.com/vikhyat/moondream
[22] X-PLUG. mPLUG-Owl, https://github.com/X-PLUG/mPLUG-Owl
[23] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,
Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang,
Fei Huang, Jingren Zhou. mPLUG-Owl: Modularization Empowers Large Language Models with
Multimodality.(2023). https://doi.org/10.48550/arXiv.2304.14178
28