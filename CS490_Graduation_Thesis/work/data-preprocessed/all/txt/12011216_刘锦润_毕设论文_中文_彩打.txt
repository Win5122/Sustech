分类号 编号
U D C 密级
本科生毕业设计（论文）
题 目： 基于生成式模型的图像数据增广方法
姓 名： 刘锦润
学 号： 12011216
系 别： 计算机科学与工程系
专 业： 计算机科学与技术
指导教师： 史玉回 讲席教授
2024 年 6 月 7 日
诚信承诺书
1. 本人郑重承诺所呈交的毕业设计（论文），是在导师的指导下，
独立进行研究工作所取得的成果，所有数据、图片资料均真实可靠。
2. 除文中已经注明引用的内容外，本论文不包含任何其他人或
集体已经发表或撰写过的作品或成果。对本论文的研究作出重要贡
献的个人和集体，均已在文中以明确的方式标明。
3. 本人承诺在毕业论文（设计）选题和研究内容过程中没有抄袭
他人研究成果和伪造相关数据等行为。
4. 在毕业论文（设计）中对侵犯任何方面知识产权的行为，由本
人承担相应的法律责任。
作者签名:
2024 年 6 月 7 日
基于生成式模型的图像数据增广方法
刘锦润
（计算机科学与工程系 指导教师：史玉回）
[摘要]：图像数据增广方法的意义在于通过扩展数据集、提高泛化能力和
增加模型鲁棒性，以及降低标注成本，改善机器学习模型的性能和应用效
果。传统的图像增广方法通常依赖于简单的操作，如旋转、翻转和缩放，但
这些方法可能无法生成具有多样性和逼真度的图像。相比之下，本文结合
了基于 Stable Diffusion 的 InstructPix2Pix 模型，实现了在垃圾分类任务中
的图像数据集的数据增广，允许用户通过输入指令来控制生成图像的特
征，从而实现更加精细和个性化的图像增广。实验将使用 InstructPix2Pix
模型得到的图像与传统的数据增广方法得到的图像进行对比，结果表明，
该模型能够有效地生成高质量、具有多样性和逼真度的图像，有利于提
高下游分类任务的准确度。
[关键词]：深度学习；图像数据增广
I
[ABSTRACT]: The significance of image data augmentation method is to
improve the performance and application effect of machine learning model by
expanding data set, improving generalization ability, increasing model robust-
ness,andreducingannotationcost. Traditionalmethodsofimageaugmentation
often rely on simple operations such as rotation, flipping, and scaling, but these
methods may not produce images with variety and fidelity. In contrast, this
paper combines the Stable Diffusion InstructionsPix2Pix model to realize data
augmentation of image data set in garbage classification tasks, allowing users
to control the features of generated images by inputting instructions, so as to
achieve more detailed and personalized image augmentation. Experiments are
conducted to compare images obtained by using InstructPix2Pix model with
thoseobtainedbytraditionaldataaugmentationmethods. Theresultsshowthat
InstructPix2Pix could effectively generate high quality, diverse and realistic
images. These images are conducive to improving the accuracy of downstream
classification tasks.
[Key words]: Deep Learning; Image Data Augmentation
II
目录
1. 绪论 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 研究背景与意义 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2 国内外研究现状 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2.1 基于传统图像处理技术的方法 . . . . . . . . . . . . . . . . . . . . . 1
1.2.2 基于机器学习的数据增广方法 . . . . . . . . . . . . . . . . . . . . . 4
1.3 主要研究内容 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.4 论文结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2. 相关理论和技术 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.1 机器学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.1.1 定义与概念 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.1.2 马尔科夫链 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2 扩散模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2.1 基本原理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2.2 Stable Diffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3. 基于 InstructPix2Pix 模型的图像数据增广方法 . . . . . . . . . . . . 11
3.1 研究目的 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.2 InstructPix2Pix 的基本原理 . . . . . . . . . . . . . . . . . . . . . . . . . 11
4. 实验设计和结果 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.1 数据集准备：trashnet . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.2 使用 InstructPix2Pix 增强图像 . . . . . . . . . . . . . . . . . . . . . . . 14
III
4.3 实验结果和分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
5. 结论与展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
参考文献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
致谢 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
IV
1. 绪论
1.1 研究背景与意义
随着深度学习的广泛运用，机器学习的影响力持续扩大[1]。特别是卷积神经网络
（ConvolutionalNerualNetwork，CNN[2]）等多种深度学习模型，在计算机视觉领域展
现出卓越的成效，涵盖任务包括对象的分类[3]、检测[4]及语义分割[5]等。深度学习算
法因深层神经网络结构和大规模标注数据集的支撑而迅速崛起。为了达到理想性能，
现代机器学习模型需求大量的高质量标注数据。然而，数据的收集和标注常需人工操
作，既耗时又耗资源，在多个实际应用中，获取足够的训练数据成为挑战[6]。以医学
领域为例，由放射学专家进行的高质量标注往往成本高昂且难以实现规模化，因此在
实践中难以应用[7]。当前，数据增广被视为解决这一问题的有效手段[6]。
数据增广不仅是增加样本数量，更关键的是丰富数据集中样本的特征，从而提升
深度学习模型训练后的性能，强化模型的泛化能力，并预防过拟合现象[8]。过拟合常
发生在神经网络试图完美拟合训练数据时，尤其是当数据集的特征分布复杂或样本
量不足时。深度卷积神经网络在计算机视觉任务中表现卓越，但这些网络需要大量数
据来防止过拟合。数据增广技术能从有限数据中衍生出更多数据，不仅增加了数据集
的大小，还提升了其质量，从而在一定程度上解决了数据集过于简单或数量不足的问
题[9]。
数据增广在各个机器学习领域都得到了广泛的应用，涵盖了自然语言处理[10]、计
算机视觉[3][4][5]等多个领域。不同类型的数据集都需要适合的数据增广方法。目前，针
对图像数据集的数据增广方法可以划分为基于传统图像处理技术的方法和基于机器
学习的方法两大类。传统方法包括对图像进行几何变换、色彩变换和像素变换；基于
机器学习的方法主要指用机器学习相关理论实现的图像生成、图像转换模型。[11]
1.2 国内外研究现状
1.2.1 基于传统图像处理技术的方法
传统的图像数据增强方法，通常使用图像处理技术来完成数据集的扩充和图像
质量优化，大致分为几何变换、色彩变换、像素变换三大类。
几何变换：在数据处理过程中，几何变换是一种常见的技术手段。然而，对这种
1
图1 图像数据增广方法
技术的应用需要谨慎，因为它可能会引发一些问题，如改变图像原始的标签信息或增
加一些不相关的数据，这种情况通常被称为不安全的转换。以文字识别任务为例，对
图像进行翻转操作在很多情况下并无实际意义，甚至可能干扰识别过程。不过，对于
存在位置偏差的数据集，几何变换技术可以有效地对数据进行校正。但在实际操作
中，需要认识到训练集与测试集的数据差异往往十分复杂，除了简单的移位、旋转等
操作外，还可能包括其他更为复杂的变换。因此，几何变换的应用范围实际上是有所
限制的。
在对图像进行几何变换时，通常会考虑一些基本操作，如翻转和旋转。图像翻转
包括水平翻转和垂直翻转两种类型，其中垂直翻转可以通过先进行水平翻转再对图
像进行 180 度旋转来实现。这种技术的优点是易于实现，且在 CIFAR-10[12]等数据集
上具有较好的应用效果。然而，在如 MNIST[13]等文本识别数据集中，使用图像翻转
可能会更改其标签信息[9]，因此需要谨慎使用。图像旋转是另一种常见的几何变换方
法。旋转后图像的维数是否变化取决于旋转的角度以及原始图像的形状。在特定情况
下，如长方形图像旋转180度或正方形图像旋转90度、180度、270度时，旋转后的
2
图像可以与原始图像保持一致的维数。与图像翻转操作类似，旋转操作的安全性也取
决于具体的数据集和旋转角度。随着旋转角度的增加，转换后的图像可能会丢失原始
标签信息。
除了翻转和旋转外，图像的随机剪裁也是一种有效的几何变换方法。这种方法可
以视为从原始图像中进行随机抽样，然后再将抽样获得的图像数据样本恢复到原始
图像的大小。通过这种方式，我们可以增加数据的多样性并提高模型的泛化能力。
图像移位是另一种常见的几何变换技术，它是指不改变图像的尺寸而将图像在
坐标轴上进行横轴和纵轴的移动，并针对边缘部分进行填充处理。这种操作可以有效
地提高模型的鲁棒性，因为移位后的图像数据中对于图像任务有用的部分将位于图
像的边缘部分，从而迫使深度学习模型在进行计算机视觉任务训练时将关注焦点转
移到任意位置，而不是仅仅针对图像中心区域的学习。
在进行了几何变换之后，需要将变换后的图像恢复到与原始图像尺寸一致的大
小。这个恢复过程通常通过对图像的边缘部分进行填充操作来实现。常用的图像填充
方法包括常数填充和边界值填充等。这些方法可以有效地处理图像变换后可能出现
的边缘效应问题。
色彩变换：色彩空间转换是一种非常有效的图像增强方法。不同的色彩空间虽然
各有特性，但由于它们之间的同构性，可以进行相互转换。例如，图像通常位于三维
RGB颜色空间中，但RGB颜色空间在感知上不均匀，颜色的接近度并不直接表示颜
色的相似性。因此，通过将图像在不同的颜色空间（如 RGB、HSV、LAB 等）之间
进行转换，我们可以以不同的方式对每个分量进行加权处理，从而适应不同的数据集
并提高模型的性能。然而，色彩空间转换也存在一些缺点。首先，这种转换可能会消
耗大量的内存空间和时间资源。其次，不恰当的色彩空间转换可能会产生不合理的效
果。例如，在人脸识别任务中，如果转换结果引入了过多的红绿等颜色信息，可能会
干扰模型的识别效果。此外，虽然色彩空间转换相比几何变换具有更多的多样性，但
不恰当的使用也可能导致模型发生欠拟合现象。
像素变换：在像素变换方面，增加噪声是一种常见的策略。通过在原始图像上随
机叠加一些孤立的、能够引起较强视觉效果的像素点或像素块来扰乱图像的可观测
3
信息，可以增加数据的多样性并提高卷积神经网络模型的泛化能力。这些噪声可以以
不同的方式生成并以不同数值填充到不同大小的像素遮掩点中，再与原图混合以扰
乱原始图像的一些特征。
另一种像素变换方法是使图像变得更模糊。模糊处理在本质上可以看作是对原
始图像进行卷积操作的过程。常用的方法是高斯模糊，它采用服从二维正态分布的卷
积核矩阵对图像进行卷积处理以减少各像素点值的差异，从而降低细节层次，使图像
数据的像素变得平滑化达到模糊图片的效果。模糊半径越大处理后的图像就越模糊。
这种方法可以有效地消除图像中的噪声和细节信息同时保留图像的主要结构特征。
此外，图像融合技术是通过计算两张图像的像素均值来实现图像混合，或者通过
随机裁剪和拼接来生成新图像。当融合来自整个训练集的多样化图像时，其效果更
佳，而非仅限于同一类别的图像。尽管从人类视觉角度看，这种融合可能显得无意义，
但实验结果表明，它确实有助于提高模型的准确性。举例来说，样本类别不平衡是数
据收集中常见的问题，这种不平衡会对分类器的性能产生不利影响。为解决此问题，
SMOTE[14]方法被提出，该方法通过合成小样本类别的新样本来实现样本均衡。具体
而言，它首先将图像特征映射到特征空间，设定采样倍率后，选择最相邻的几个样本，
并在它们之间的连线上随机选择一个点作为新样本，直到实现样本均衡。
在图像像素变换方面，图像信息删除技术也得到了广泛应用。这类技术包括随机
擦除[15]和 CUTOUT[16]等方法。随机擦除与添加噪声的方法类似，它随机选择一个图
像中的矩形区域，并用随机像素值进行遮盖。这种方法可以轻松地集成到大多数卷积
神经网络模型中。其优势在于，它可以促使模型学习更多关于图像的描述性特征，从
而避免对特定视觉特征的过拟合，确保网络能够关注整个图像。然而，其缺点是可能
会改变图像的原始标签。与随机擦除类似，CUTOUT 是在图像的随机位置使用固定
大小的正方形进行0-mask剪裁。
1.2.2 基于机器学习的数据增广方法
在机器学习模型训练过程中，优化模型的核心目标是降低损失函数值。为实现这
一目标，通常需要大量的训练数据。传统的数据增强方法主要依赖于对现有数据集进
行微调，如翻转、旋转和平移等，以生成略有差异的新数据集。这种方法允许我们将
4
经过调整的数据视为与原始数据集不同的新数据，并将其用于模型训练。数据增强的
好处不仅在于增加训练样本数量和提高模型泛化能力，还能通过引入噪声数据来增
强模型的稳健性。
近年来，随着机器学习的迅猛发展和广泛应用，研究人员开始将机器学习技术引
入数据增广领域，并取得了一定的研究成果。
特征空间增广：神经网络在将复杂输入转化为简洁表示方面具有显著能力。这类
网络能将图像映射为二元类别或 n×1 向量。神经网络的处理流程可以被调整，以便
从整个网络中分离出中间表示。在全连接层中，可以提取和隔离出图像数据的低维表
示。
特征空间增广特别适用于使用自编码器（AutoEncoders）处理数据。自编码器的
工作原理是通过网络的一半（编码器）将图像映射为低维向量表示，以便网络的另一
半（解码器）能将这些向量重构回原始图像。这种编码表示被用于特征空间增广。
基于 GAN 的数据增广方法：目前，利用生成对抗网络（Generative Adversarial
Network, GAN）进行生成建模是数据增广的常用手段。GAN 在数据增广任务中的应
用主要是通过生成新的训练数据来扩充模型的训练样本，从而提升图像分类任务的
效果。研究人员在原始GAN框架的基础上提出了多种改进方案，通过设计不同的神
经网络结构和损失函数等手段来不断提升GAN 变体的性能，典型方案如下：
DCGAN（DeepConvolutionalGANs[17]）尝试将CNN与GAN相结合，在图像分
类任务上表现优于其他无监督算法。该算法对CNN架构进行了三项主要修改：首先，
使用卷积层替代了池化层，使生成器能学习自身的空间下采样方式；其次，消除了卷
积特征上的全连接层，尝试将最高卷积特征直接连接到生成器和判别器的输入和输
出；最后，引入了批量归一化（BatchNormalization，BN）[18]来稳定学习过程，并有效
解决深度生成器的样本坍塌问题。DCGAN实现了CNN和GAN的有效结合，是一种
强大的图像生成模型，广泛应用于数据集样本的生成。然而，当训练时间较长时，部
分模型中仍存在不稳定的问题。
CycleGAN（CycleGenerativeAdversarialNetworks[19]）是图像转换领域的重要模
型，能实现无需配对的样本数据转换，如将名人图像转换为卡通人物，或者件将卡通
5
人物的形式转换成名人的图像。这种图像转换能极大地扩充样本数据，同时保留原始
图像的轮廓。CycleGAN作为一种无需对齐数据的图像转换方法，现在被广泛应用于
模态转换。CycleGAN 实际上是由两个对称的 GAN 组成的环形网络。与 DCGAN 相
比，CycleGAN 能更精确地控制图像生成。
尽管GAN在图像生成领域得到了广泛应用，但由于其训练的不稳定性和对大量
训练数据的需求，其不同的变体方法在某些情况下可能无法有效地实现数据增广的
任务。
其他图像生成模型：扩散模型[20]（DiffusionModel）作为一种图像生成模型，近
年来在图像处理问题中展现出了优越的性能。随着深度学习技术的发展，越来越多的
研究表明，扩散模型在生成高质量、逼真的图像方面具有潜力。相比传统的生成对抗
网络（GAN）等模型，扩散模型采用了不同的生成策略，通过逐步增加噪声水平来生
成图像，从而能够更好地控制生成过程，提高生成图像的质量和稳定性。
在图像数据增广问题中，扩散模型可以应用于生成新的训练样本，以扩充训练数
据集并提高模型的泛化能力。通过在扩散过程中逐步引入噪声，扩散模型能够生成
具有丰富多样性的图像样本，从而有助于训练模型更好地捕捉数据分布的特征，提高
模型的鲁棒性和泛化性能。此外，扩散模型还能够生成具有细节丰富、逼真度高的图
像，有助于提升模型在真实场景中的表现。除了在数据增广问题中的应用，扩散模型
还可以用于图像生成、图像修复、超分辨率重建等任务。其优越的生成能力和稳定性
使其成为了当前图像生成领域的研究热点之一。
1.3 主要研究内容
本文尝试使用基于StableDiffusion[21]图像生成算法的图像编辑模型InstructPix2Pix[22]
对垃圾分类数据集trashnet[23]进行增广。该数据集包含各种类型的垃圾图像，如纸张、
金属、塑料等。通过使用InstructPix2Pix模型，我们可以根据给定的编辑指令，将原
始垃圾图像转换为具有不同特征的图像，从而扩充数据集并增加样本的多样性。这有
助于提高垃圾分类模型的性能和泛化能力，使其能够更准确地识别和分类各种类型
的垃圾。
6
1.4 论文结构
本文一共分为五章，分别为：
第一章绪论：介绍数据增广的研究背景与意义，以及国内外图像数据增广方法的
研究现状，引出本文的研究内容与意义。
第二章相关理论和技术：介绍机器学习的相关概念，以及扩散模型的基本原理。
第三章基于 InstructPix2Pix 模型的图像数据增广方法：介绍本文引入 Instruct-
Pix2Pix模型进行数据增广的研究目的，以及InstructPix2Pix模型的基本原理。
第四章实验结果和分析：介绍具体实验设置，对结果进行分析。
第五章结论与展望：对目前已经完成的工作进行总结，得出结论，分析实验中存
在的不足之处，对之后的工作进行展望。
7
2. 相关理论和技术
本章主要介绍了机器学习的相关概念，以及扩散模型的基本原理。
2.1 机器学习
2.1.1 定义与概念
机器学习（Machine Learning，ML）是人工智能（Artificial Intelligence，AI）领
域的一个分支，目前并没有公认的权威的定义。一般来说，机器学习理论主要是设计
和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分
析获得规律，并利用规律对未知数据进行预测的算法。
监督学习（Supervised Learning）、无监督学习（Unsupervised Learning）和强化
学习（ReinforcementLearning）是机器学习领域中三种主要的学习范式。在监督学习
中，模型通过使用带有标签的数据进行训练，以学习输入与输出之间的映射关系，从
而对新的未标记数据进行预测。监督学习的训练集要求是包括输入和输出，也可以说
是特征和标签。训练集中的标签一般是由人标注的。常见的监督学习算法包括回归分
析和统计分类。与之相反，无监督学习则在没有标签的情况下对数据进行建模，其目
标是发现数据中的潜在结构或模式。监督学习和非监督学习的差别就是训练集目标
是否有人为标注。它们都有训练集，且都有输入和输出。常见的无监督学习算法有生
成对抗网络（GAN）、聚类。而强化学习则是一种基于奖励和惩罚的学习方法，在与
环境的交互过程中，Agent通过尝试不同的行动来最大化累积奖励。这三种方法在解
决不同类型的问题时发挥着重要作用，例如监督学习常用于分类和回归问题，无监督
学习则常用于聚类等任务，而强化学习则广泛应用于控制问题和决策制定。
无论属于哪种机器学习范式，机器学习算法的所有组合都包含以下内容：
• 表示：对问题进行建模
• 评估：目标/得分函数
• 优化：存在有效的优化方法提高算法性能
8
2.1.2 马尔科夫链
马尔科夫链[24]（MarkovChain）在机器学习中有着广泛的应用，也是扩散模型中
的重要概念。马尔科夫链为状态空间中经过从一个状态到另一个状态的转换的随机
过程，该过程要求具备“无记忆性”，即下一状态的概率分布只能由当前状态决定，在
时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作马尔可夫性
质。
马尔可夫链的数学定义如下：假设序列状态是 ...X ,X ,X ,X ...，那么
t−2 t−1 t t+1
在X 时刻的状态的条件概率仅依赖于前一刻的状态X ，即：
t+1 t
P(X |...X ,X ,X ) = P(X |X ) (1)
t+1 t−2 t−1 t t+1 t
由于某一时刻状态转移的概率只依赖于它的前一个状态，那么只需求出系统中
任意两个状态之间的转换概率，即可确定该马尔科夫链。
2.2 扩散模型
2.2.1 基本原理
在2020年，DDPM（DenoisingDiffusionProbabilisticModel[25]）被提出，也被称
为扩散模型（DiffusionModel）。该模型涵盖了前向过程（ForwardProcess）和反向过
程（ReverseProcess）两个核心部分，前向过程又被称为扩散过程（DiffusionProcess）。
无论是前向过程还是反向过程都可以被视作是一个参数化的马尔可夫链。其中，反向
过程可用于生成数据样本，类似于 GAN 中的生成器，但值得注意的是，与 GAN 的
生成器不同，DDPM 的反向过程不会导致维度的变化。
扩散模型是设计用于学习数据分布 p(x) 的概率模型, 通过逐步去噪一个正态分
布的变量来实现,这对应于学习长度为T 的固定马尔可夫链的逆过程。对于图像合成，
最成功的模型 DDPM 依赖于数据分布 p(x) 的变分下界的一个重新加权变体。DDPM
模型可被解释为一系列同等权重的去噪自编码器 ϵ (x ,t);t = 1...T 的序列，它被训
θ t
练以预测其输入 x 的一个去噪变体, 其中 x 是输入 x 的一个带噪版本。相应的目标
t t
可以简化为:
9
[ ]
L = Ex,ε ∼ N(0,1),t ||ϵ−ϵ (x ,t)||2 , (2)
DM θ t 2
其中t 是从1,...,T 中均匀采样的。
2.2.2 StableDiffusion
本文采用的InstructPix2Pix模型依靠StableDiffusion[21]生成样本图片来获取数据
集并训练，StableDiffusion正是在扩散模型上进一步发展和完善的。
Stable Diffusion 模型中的 Generative Modeling of Latent Representations[21]概念是
其与普通的扩散模型（如 DDPM）存在区别的重要原因，其原理是通过训练的感知
压缩模型 ε 和 D 访问一个高效的低维潜在空间，在这个空间中，高频不可感知的细
节被抽象掉了。与高维像素空间相比，这个空间更适合基于似然的生成模型，因为它
们可以 (i) 专注于数据的重要语义位，以及 (ii) 在更低维、计算效率更高的空间中进
行训练，使用重新加权的边界进一步将目标集中在感知上最相关的位上：
[ ]
L = Eε(x),ε ∼ N(0,1),t ||ϵ−ϵθ(z ,t)||2 . (3)
LDM t 2
模型的神经网络骨干ϵ (o,t)被实现为一个时间条件的UNet。由于前向过程是固
θ
定的, z 可以在训练期间从 ε 有效获得，并且来自 p(z) 的样本可以通过 D 单次解码
t
到图像空间。
10
3. 基于 InstructPix2Pix 模型的图像数据增广方法
3.1 研究目的
由于目前现有的图像数据增广方法受到了一定的限制，无法满足对图像进行定
向且多样化的增广需求。因此，本文引入了图像编辑模型InstructPix2Pix对数据集进
行增广。相较于基于 GAN 的数据增广方法，InstructPix2Pix 具有更高的灵活性和可
定制性，能够根据用户提供的指令或条件，针对性地对图像进行编辑，从而实现更加
精准和多样化的数据增广。通过引入InstructPix2Pix模型，对图像数据进行增广时能
够充分利用其强大的图像编辑能力，为数据集增添新的样本，丰富数据的多样性，进
而提升模型的性能和泛化能力。
3.2 InstructPix2Pix 的基本原理
InstructPix2Pix 是一种教导生成模型遵循人类编写的指令对图像进行编辑的方
法：给定自然语言形式的图像编辑命令和一张图像，要求模型理解并根据指令对图像
进行编辑。由于该类任务的训练数据难以大规模获取，作者使用了一种生成配对数据
集的方法，该图像编辑实例数据集结合了预训练于不同模态的多个大型模型：一个大
型语言模型（GPT-3[26]）和一个文本到图像模型（StableDiffusion[21]）。这两个模型捕
捉到关于语言和图像的互补知识，可以结合起来创建跨越两种模态的任务的配对训
练数据。
InstructPix2Pix 利用大型语言模型接收图像标题，并生成编辑指令和编辑后的文
本标题。通过微调 GPT-3 模型可以实现这一目标，该模型在一个小型的编辑三元组
数据集上进行了训练，包括输入字幕、编辑指令和输出字幕。接着，使用Prompt-to-
Prompt 方法，借助交叉注意力权重的去噪步骤，以促进文本到图像模型的多代相似。
虽然这有助于生成更一致的图像，但不同的编辑可能需要不同程度的图像变化。每
对标题通过StableDiffusion模型生成100对样本图像，并使用基于CLIP的指标进行
过滤，以确保图像对的质量和多样性。这一过程使生成的数据生成更加稳健，即使在
Prompt-to-Prompt和稳定扩散失效时也能保持有效。
因此，训练于该数据集的InstructPix2Pix模型本质上是一个条件扩散模型，根据
相应的指令在扩散过程中生成编辑后的图像。Diffusion 模型[20]通过一系列去噪自编
11
图2 跨模态配对训练数据
码器学习生成数据样本，这些去噪自编码器估计数据分布的分数 (指向高密度数据的
方向)。潜在扩散[21]通过在预训练的变分自编码器的潜在空间中运行，提高了扩散模
型的效率和质量，自编码器包含编码器 E 和解码器 D。对于图像 x，扩散过程会向
编码的潜在变量 z = E(x) 添加噪声，产生噪声潜在变量 z ，噪声水平会随着时间步
t
t ∈ T 的增加而增加。学习一个网络 e ，给定图像条件 c 和文本指令条件 c ，预测
g I T
添加到噪声潜在变量z 中的噪声。以下潜在扩散目标需要被最小化:
t
[ ]
L = E ∥ϵ−ϵ (z ,t,ε(c ),c )∥2 (4)
ε(x),ε(cT),cT,ε∼N(0,1),t g t T T 2
InstructPix2Pix 使用预训练的 Stable Diffusion 检查点初始化模型的权重，利用其
强大的文本到图像生成能力。第一个卷积层额外添加了输入通道，将 z 和 E(c ) 级
t I
连。StableDIffusion模型的所有可用权重都从预训练的检查点初始化，而在新添加的
输入通道上运行的权重则初始化为零。原本用于文字注释的文本条件机制被改为接
收文本编辑指令c ，这样就允许了文本编辑指令作为输入。
T
12
4. 实验设计和结果
本文的实验部分旨在对垃圾分类数据集 trashnet[23]进行多种方式的数据增广，并
在使用不同数据增广方法后的数据集上进行垃圾分类神经网络的训练，衡量并比较
不同数据增广方法对为下游分类任务所训练的模型性能的提升效果。
实验过程为，分别使用传统方法和InstructPix2Pix对trashnet数据集进行增广，增
广后的数据集将被输入到具有相同结构的残差网络中进行垃圾分类任务。记录分类
结果的准确性以及分类过程中损失函数的变化情况，并通过可视化方式展现结果。具
体来说，本文将比较不同数据增广方法对分类准确性的影响，以及增广后数据对模型
的训练效果和泛化能力的影响。通过对实验结果的分析，本文将评估不同数据增广方
法在垃圾分类任务中的有效性和适用性，并得出最终的结论。
4.1 数据集准备：trashnet
trashnet数据集是由来自斯坦福大学的MindyYang和GaryThung手工收集的[23]，
这一数据集填补了此前缺乏的垃圾分类数据集的空白。在创建这一数据集之前，很少
有公开可用的数据集能够准确地代表垃圾分类任务的现状。通过对回收工厂和回收
物品现状的深入研究，Yang和Thung发现，通过其他渠道（如Google搜索）获得的
图片大多并不能准确地反映回收物品的实际情况。例如，FlickrMaterialDatabase中的
图像以原始和未损坏的状态呈现材料，这在作为废物处理的回收材料中是不可能的。
为了解决这一问题，Yang 和 Thung 决定手工收集图像数据，以确保数据集中的
图像能够准确地代表回收物品的真实状态。他们在回收工厂和相关场所进行实地拍
摄，并精心选择和标注了各种类型的回收物品，包括玻璃、纸张、金属、硬纸板和塑
料等。trashnet 数据集成为垃圾分类任务的重要资源，为该领域的研究提供了数据支
持。
数据集包含六个类的回收对象图像，每个类大约有400-500张图像(除了“trash”
类，它只有大约100张图像)，总计2532张图像。表1为该数据集中每种类别图像的
数量和大小。图3显示了来自这六个类的样例图像。
13
表1 trashnet数据集各类别图像分布
类别 图片数量（张） 大小（MB）
cardboard 404 630
glass 502 576
metal 411 557
paper 595 934
plastic 483 623
trash 137 158
总计 2532 3478
a) cardboard b) glass c) metal
d) paper e) plastic f) trash
图3 trashnet数据集样例
4.2 使用 InstructPix2Pix 增强图像
本次实验借鉴了TimothyBrooks等人对InstructPix2Pix模型的实现[27]，在他们的
基础上做了一些修改以适应实验需求。使用 InstructPix2Pix 模型对 trshnet 进行数据
增广已经取得了成效。以下是一些例子：在 metal 中的图像（ID2）中，图像主体是
残缺的易拉罐的一部分，即易拉罐的底部。使用指令“make it a complete can”通过
InstructPix2Pix 对该图像进行编辑，成功生成了一个带有其他部分的底部打开的完整
易拉罐，如图4所示。这个编辑后的图像不仅填补了原图中的缺失部分，而且与原图
的风格和细节保持一致，使得结果自然且真实。
对于metal类别中的图像（ID343），原图像显示了一个常见类型的易拉罐，具有
一个典型的拉环。通过使用InstructPix2Pix模型并给出指令“changethepullringofthe
14
a) input b) output
图4 指令为“makeitacompletecan”
can”，成功地改变了易拉罐的外形特征，去除了原图像中易拉罐拉环的孔洞，如图5
所示。
a) input b) output
图5 指令为“changethepullringofthecan”
InstructPix2Pix 模型展示了其在数据集中实现跨类别图像转换的能力。例如，在
trash类别中的图像（ID2）中，原图像是一个一次性纸杯。通过指令“makeitlooklike
it is made of iron”，成功地将图像转换成了铁质杯架，如图 6 所示。这种跨类别转换
能力为数据增广任务提供了更大的灵活性和创造性。
这些例子展示了 InstructPix2Pix 模型在对 trashnet 数据集进行增广时的有效性和
可行性。
15
a) input b) output
图6 指令为“makeitlooklikeitismadeofiron”
4.3 实验结果和分析
通过实验来评估使用传统数据增广方法和 InstructPix2Pix 分别对 trashnet 数据集
增广的性能。
表2 传统方法组数据集各类别图像分布
类别 图片数量（张）
cardboard 806
glass 1002
metal 820
paper 1188
plastic 964
trash 274
总计 5054
表3 IP2P组数据集各类别图像分布
类别 图片数量（张）
cardboard 1189
glass 1492
metal 1213
paper 1782
plastic 1446
trash 411
总计 7533
在本次实验中，使用 ResNet18 作为分类器的模型进行训练，该网络的基本架构
是残差网络，网络的深度（权重层数，包括池化层、激活层和线性层）是18层。分类
器的 loss function 为 Cross Entropy Loss，初始学习率为 2×10−6，每进行 40 次迭代，
16
学习率衰减为原来的十分之一，每次训练迭代100次。分类器的实现参考了Sangmin
Woo的代码[28]，在其基础上进行修改以适应实验需求。
图7 三组准确率变化情况对比
为了保证学习效果的可对比性，在实验中每张图片都被提前处理成分辨率大小
相同的图片，每张图片的分辨率均为512*384像素。在此基础上再进行后续不同方式
的数据增广和分类器的训练。
本次实验分为三组，对象数据集分别为trashnet原版数据集（简称为对照组，下
同），使用传统方法进行数据增广的trashnet数据集（简称为传统方法组，下同）以及
使用 InstructPix2Pix 进行数据增广的 trashnet 数据集（简称为 IP2P 组，下同）。传统
方法组的对象数据集的大小和分布如表 2 所示，IP2P 组的对象数据集的大小和分布
如表3所示。
实验对三组数据集进行分类器训练，评测指标为Top-1Accuracy。对于每组实验，
训练轮次均为100轮，每轮的平均准确率如图7所示，每轮的损失函数的平均值如图
17
8所示。
根据图7可知，三组实验中分类器的准确率曲线均呈现上升趋势后逐渐平缓，具
有稳定性。相比于对照组，使用传统方法对数据集进行增广后，分类器的识别准确率
有微小的提升，在 60 轮后逐渐收敛于 75% 左右；而 IP2P 实验组的准确率相比于另
外两组有着显著的提升，达到了90%以上，并且在40轮就已逐渐收敛，准确率曲线
收敛的速度也明显强于另外两组。这说明 IP2P 实验组在本次实验中发挥良好，使用
InstructPix2Pix增广后的数据集可以大幅提升分类器的性能，证明了该方法的有效性。
图8 三组Loss函数收敛情况对比
如图8所示，相比于对照组，传统方法实验组的loss函数的收敛速度与对照组一
致，loss函数的值在前20轮几乎与对照组相同，之后略低于对照组，在0.7左右趋于
稳定，说明传统方法可以一定程度上增强数据集，提高数据特征和标签的关联性，但
效果很有限；IP2P 实验组的收敛速度与对照组也几乎一致，但是 loss 函数的值要明
显低于其他两组，最终收敛于 0.2 以下。这说明模型在 IP2P 组的数据集上表现优秀，
18
能更好地拟合训练数据，能更快地学习数据的有效特征，即对于分类器来说，使用该
方法增强的数据有更明显的特征，更适用于分类器的训练，证明了该方法的适用性。
19
5. 结论与展望
本文将基于 Stable Diffusion 的 InstructPix2Pix 图像编辑模型应用于图像数据增
广，以探索新型的图像数据增广技术，为任意类型的图像数据集提供通用的数据增广
方法，并且用实验验证了该方法在处理trashnet数据集时的有效性和适用性。试验结
果表明，通过使用该方法增强的数据集进行对以神经网络为框架的分类器的训练，能
够使分类器的准确率明显提升，同时获得更快的收敛速度。
然而，值得注意的是，目前通过该模型生成的图像仍存在一些不足之处。例如，
生成图像可能在细节或真实性方面存在一定的缺陷。这表明模型在某些情况下可能
存在一定的泛化能力不足，需要进一步的调试和试验才能得到更令人满意的结果。同
时，本文只针对了特定的数据集进行研究，之后可以对多个不同类型、不同体量的数
据集进行更多探索。
20
参考文献
[1] BRIGATO L, IOCCHI L. A Close Look at Deep Learning with Small Data[C]. in: 2020 25th
InternationalConferenceonPatternRecognition(ICPR).2021:2490-2497.DOI:10.1109/ICPR4
8806.2021.9412492.
[2] LIZ,LIUF,YANGW,etal.ASurveyofConvolutionalNeuralNetworks:Analysis,Applications,
and Prospects[J]. IEEE Transactions on Neural Networks and Learning Systems, 2022, 33(12):
6999-7019.DOI:10.1109/TNNLS.2021.3084827.
[3] HEK,ZHANGX,RENS,etal.DelvingDeepintoRectifiers:SurpassingHuman-LevelPerfor-
mance on ImageNet Classification[C]. in: Proceedings of the IEEE International Conference on
ComputerVision(ICCV).2015.
[4] REDMONJ,DIVVALAS,GIRSHICKR,etal.YouOnlyLookOnce:Unified,Real-TimeObject
Detection[C].in:ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
(CVPR).2016.
[5] LONGJ,SHELHAMERE,DARRELLT.FullyConvolutionalNetworksforSemanticSegmen-
tation[C]. in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR).2015.
[6] MUMUNIA,MUMUNIF.DataAugmentation:AComprehensiveSurveyofModernApproaches
[J/OL].Array,2022,16:100258.https://www.sciencedirect.com/science/article/pii/S259000562
2000911.DOI:https://doi.org/10.1016/j.array.2022.100258.
[7] LITJENS G, KOOI T, BEJNORDI B E, et al. A Survey on Deep Learning in Medical Image
Analysis[J/OL].MedicalImageAnalysis,2017,42:60-88.https://www.sciencedirect.com/scien
ce/article/pii/S1361841517301135.DOI:https://doi.org/10.1016/j.media.2017.07.005.
[8] YING X. An Overview of Overfitting and its Solutions[J/OL]. Journal of Physics: Conference
Series, 2019, 1168(2):022022. https://dx.doi.org/10.1088/1742-6596/1168/2/022022. DOI:
10.1088/1742-6596/1168/2/022022.
[9] SHORTENC,KHOSHGOFTAARTM.AsurveyonImageDataAugmentationforDeepLearning
[J/OL].JournalofBigData, 2019,6(1):60.https://doi.org/10.1186/s40537-019-0197-0.DOI:
10.1186/s40537-019-0197-0.
[10] SHORTENC,KHOSHGOFTAARTM,FURHTB.TextDataAugmentationforDeepLearning
[J/OL].JournalofBigData,2021,8(1):101.https://doi.org/10.1186/s40537-021-00492-0.DOI:
10.1186/s40537-021-00492-0.
[11] 冯晓硕,沈樾,王冬琦.基于图像的数据增强方法发展现状综述[J].ComputerScienceandAp-
plication,2021,11:370.
[12] KRIZHEVSKYA.LearningMultipleLayersofFeaturesfromTinyImages[C/OL].in:2009.htt
ps://api.semanticscholar.org/CorpusID:18268744.
[13] DENG L. The Mnist Database of Handwritten Digit Images for Machine Learning Research[J].
IEEESignalProcessingMagazine,2012,29(6):141-142.
21
[14] FERNÁNDEZA,GARCIAS,HERRERAF,etal.SMOTEforLearningfromImbalancedData:
Progress and Challenges, Marking the 15-year Anniversary[J]. Journal of Artificial Intelligence
Research,2018,61:863-905.
[15] ZHONGZ,ZHENGL,KANGG,etal.RandomErasingDataAugmentation[J/OL].Proceedings
oftheAAAIConferenceonArtificialIntelligence,2020,34(07):13001-13008.https://ojs.aaai.or
g/index.php/AAAI/article/view/7000.DOI:10.1609/aaai.v34i07.7000.
[16] DEVRIES T, TAYLOR G W. Improved Regularization of Convolutional Neural Networks with
Cutout[Z].2017.arXiv:1708.04552[cs.CV].
[17] RADFORDA,METZL,CHINTALAS.UnsupervisedRepresentationLearningwithDeepCon-
volutionalGenerativeAdversarialNetworks[J].arXivpreprintarXiv:1511.06434,2015.
[18] IOFFES,SZEGEDYC.BatchNormalization:AcceleratingDeepNetworkTrainingbyReducing
InternalCovariateShift[C].in:Internationalconferenceonmachinelearning.2015:448-456.
[19] ZHUJY,PARKT,ISOLAP,etal.UnpairedImage-to-imageTranslationUsingCycle-consistent
Adversarial Networks[C]. in: Proceedings of the IEEE international conference on computer vi-
sion.2017:2223-2232.
[20] SOHL-DICKSTEINJ,WEISSE,MAHESWARANATHANN,etal.DeepUnsupervisedLearn-
ingUsingNonequilibriumThermodynamics[C].in:Internationalconferenceonmachinelearning.
2015:2256-2265.
[21] ROMBACHR,BLATTMANNA,LORENZD,etal.High-ResolutionImageSynthesisWithLa-
tentDiffusionModels[C].in:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR).2022:10684-10695.
[22] BROOKS T, HOLYNSKI A, EFROS A A. InstructPix2Pix: Learning To Follow Image Editing
Instructions[C]. in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition(CVPR).2023:18392-18402.
[23] trashnet[EB/OL].https://www.github.com/garythung/trashnet.
[24] NORRISJR.Markovchains[M].Cambridgeuniversitypress,1998.
[25] HOJ,JAINA,ABBEELP.DenoisingDiffusionProbabilisticModels[Z].2020.arXiv:2006.11239
[cs.LG].
[26] BROWNT,MANNB,RYDERN,etal.LanguageModelsAreFew-shotLearners[J].Advances
inneuralinformationprocessingsystems,2020,33:1877-1901.
[27] BROOKS T, HOLYNSKI A, EFROS A A. InstructPix2Pix: Learning to Follow Image Editing
Instructions[J].arXivpreprintarXiv:2211.09800,2022.
[28] WOOS.RecycleNet[EB/OL].2020.https://github.com/sangminwoo/RecycleNet.
22
致谢
时光荏苒，转眼间就到了毕业季。回想本科期间的点滴，在来到南方科技大学的
这四年里，我收获颇丰，也感慨很多。这几年里，我经历了不少事，有成长、胜利和
失败；有不少人的身影在我脑海回荡，有良师、益友、家人；很多情绪蔓延开来，我
感到无比幸运、些许遗憾、十分不舍，还有难以言表的感激。
感谢在我的学习、生活和科研路上给予指导和帮助的老师们。十分感谢史玉回教
授和赵琪老师对于我毕业论文的悉心指导。特别感谢本科期间亲爱的导师刘烨庞教
授对我的关怀和引领，是您带我初窥科研的世界，感激您毫无保留的帮助和真诚的建
议。感谢我的生活导师曾振中教授，在我刚步入大学时给了我很多宝贵的意见和建
议。
最感谢我的父母。在我迷茫的时候，他们是我最强而有力的依靠。感谢爸爸妈妈
为我做的一切，感谢你们对我毫无保留的关爱和无条件的支持。
感谢我的朋友们。感谢你们的帮助、激励，感谢那些同甘共苦的日子，感谢这段
终将化作珍贵回忆的美好时光。希望所有人的未来都无比光明。
尤其感谢我的女朋友。你像灿烂的阳光，照进我的生活，让它熠熠生辉。感谢你
带给我的无与伦比的温暖。感谢你的关心、爱护和包容，让我更加勇敢地面对困难。
感谢图书馆和工学院，感谢每一个不知死活的夜晚，感谢晨曦和雾霭，感谢闷热
夏日里每一丝凉爽的风。
就借用东坡的词来结尾吧：人生如逆旅，我亦是行人。愿我能坦然面对一切，愿
万事胜意。
23