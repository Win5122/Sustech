本科生毕业论文
题 目： 基于临床特征融合注意力网络的
白内障分级模型研究
姓 名： 赵宇航
学 号： 11910711
系 别： 计算机科学与工程系
专 业： 计算机科学与技术
指导教师： 刘 江
年 月 日
诚信承诺书
1.本人郑重承诺所呈交的毕业设计（论文），是在导师的指导下，
独立进行研究工作所取得的成果，所有数据、图片资料均真实可靠。
2.除文中已经注明引用的内容外，本论文不包含任何其他人或集
体已经发表或撰写过的作品或成果。对本论文的研究作出重要贡献的
个人和集体，均已在文中以明确的方式标明。
3.本人承诺在毕业论文（设计）选题和研究内容过程中没有抄袭
他人研究成果和伪造相关数据等行为。
4.在毕业论文（设计）中对侵犯任何方面知识产权的行为，由本
人承担相应的法律责任。
作者签名：
年 月 日
基于临床特征融合注意力网络的
白内障分级模型研究
赵宇航
（计算机科学与工程系 指导教师：刘江）
[摘要]：白内障是导致视觉损伤与失明的主要疾病。目前已有一些
研究将神经网络模型应用于白内障自动分级工作，但大多研究关注如
何提高白内障的分类性能，对于网络的可解释性（可理解性）方面关
注较少。因此，本文提出一种临床特征融合注意力网络的白内障分级
模型 CFINet 来解决核性白内障分类问题。在提出的网络模型中，本
文设计了临床特征融合注意力模块，其通过动态调整特征图不同区域
的权重从而使网络模型关注重要的病理区域。本文在临床 AS-OCT
影像数据集与公开 LAG 数据集上分别进行了实验，实验结果表明，
本文模型的分类性能优于其他先进注意力机制网络模型。例如，在临
床 AS-OCT 影像数据集上，本文模型的准确率达到了 92.161%，相较
于其他先进注意力机制能够提高 1-2%。此外，本文还使用了可视化
方法来展示神经网络对于重要的病理区域的关注，提高了本文模型在
决策过程中的可理解性。
[关键词]：核性白内障分类；眼前节光学相干断层成像；临床特征
融合注意力模块；可解释性
[ABSTRACT]: Cataract is a major disease that causes visual
impairment and blindness. Some studies have applied neural network
models to automatic cataract grading work, but most of them focus on
how to improve the classification performance of cataract, with less
attention to the interpretability(intelligibility) aspect of the network.
Therefore, this paper proposes a clinical features integrated attention
network(CFINet) to solve the nuclear cataract classification problem. In
the proposed network model, we designed a clinical feature integrated
attention module, which makes the network model focus on important
pathological regions by dynamically adjusting the weight of different
regions in the feature map. In this paper, experiments are conducted on
clinical AS-OCT image dataset and public LAG dataset respectively, and
the experimental results show that the model in this paper has better
classification performance compared with other advanced attention
network models. The accuracy of this model reaches 92.161% on the
clinicalAS-OCT image dataset, which is 1-2% better than other advanced
attention mechanisms. In addition, this paper also uses a visualization
method to demonstrate the focus of neural networks on important
pathological areas, which improves the comprehensibility of the model in
the decision-making process.
[Keywords]: nuclear cataract classification; AS-OCT; clinical features
integrated attention model; interpretability
目录
1.引言...................................................1
2.临床特征融合注意力网络模型.............................3
2.1 临床特征融合注意力模块................................3
2.2 网络模型..............................................4
3.数据集介绍.............................................4
3.1 临床 AS-OCT 影像数据集................................4
3.2 LAG 数据集...........................................5
4.实验设置与结果分析.....................................5
4.1 评价标准..............................................5
4.2 实验设置..............................................6
4.3 基准模型..............................................6
4.4 结果分析..............................................6
4.5 可视化结果分析........................................7
4.5.1 分组权重可视化结果..................................7
4.5.2 基于类激活映射方法的可视化结果......................8
5.结束语.................................................9
参考文献................................................11
致谢....................................................13
1.引言
《2021-2022 年度全球眼健康调研报告》显示，世界上大约有2.85亿人视力受损，
其中有3900 万人失明，而51%的失明病例和33%的视力损害病例是由白内障引起，
白内障已成为全球第一的致盲性眼科疾病。根据中华医学会眼科学分会统计，我国
60 至 89 岁人群的白内障发病率大约为 80%，90 岁以上人群的白内障发病率甚至达
90％以上。50岁以上人群视力障碍比例高达30％，主要原因就是白内障。
白内障在临床上表现为眼睛的晶状体出现混浊，因此导致视力受损乃至失明。根
据晶状体的混浊部位，白内障从内而外可分为三种类型：核性白内障，皮质性白内障，
以及后囊性白内障[1]。其中，核性白内障是一种常见的白内障类型，其在临床上的表
现为晶状体的核性部分产生混浊，本文所研究的白内障分级模型即针对核性白内障。
根据LOCSⅢ（Lens Opacities Classification System Ⅲ，晶状体混浊分类系统）[2]以及
临床诊断需要，核性白内障可分为三种级别：正常，轻度以及重度[3]。正常指晶状体
核性区域未出现混浊；轻度指晶状体核性区域出现不明显混浊，对应 LOCS III 中的
一级和二级；重度指晶状体核性区域出现明显混浊，对应 LOCS III 中的三级及以上
级别。
受我国老龄化影响，白内障患者人数逐年增加，导致了如今医患数量的不均衡。
同时，在诊断时，医生通常基于自身的临床经验来判断白内障级别，这种诊断方法主
观性较强且与医生本身诊断水平有关，不能保证其准确性。所以，开发计算机辅助诊
断白内障技术来提高诊断的效率与准确性是有必要的。
近年来，已有一些研究将神经网络模型应用于白内障自动分级工作中。Cao 等人
采用深度分割网络对 AS-OCT 图像进行分割，得到了图像的核性、皮质和囊性区域[4]，
为三种类型白内障的分类工作打下了基础。刘振宇等人使用卷积神经网络来提取裂隙
灯图像的特征，在核性白内障分级中获得了不错的效果[5]；李建强等人使用深度学习
方法来提取眼底图像的特征进行白内障分类[6]；Xu 等人设计了一种在眼底图像上的
全局-局部特征表示模型来提高白内障的分级性能[7]。对于白内障的分类工作，如今
大多研究关注如何提高白内障的分类性能，但对于神经网络模型的可解释性方面关注
较少。
相较于上文提到的裂隙灯图像和眼底图像，AS-OCT（Anterior Segment Optical
CoherenceTomography，眼前节光学相干断层成像）对于晶状体结构的获取更加清晰
完整，更适合不同类型白内障的分类。文献[8，9]在 AS-OCT 影像下使用斯皮尔曼相关
1
系数方法分析了像素均值与核性白内障严重程度之间相关性，结果显示两者存在较强
相关性。因此，在网络中可以选择平均池化的方式来凸显像素均值这一关键特征。有
临床研究指出核性白内障患者晶状体核性区域的中间与下半部分混浊程度比上半部
分更明显[10]。因此，注意力网络需对空间维度，尤其是对纵向的病理分布给予更多关
注，使网络能够将更多的注意力给到重要的病理区域。
注意力机制(attention mechanism)在加入神经网络后，能够提高网络模型在处理
医学影像任务中的效果[11]，同时这也是一种提高网络可解释性的方式。挤压激励注意
力机制（squeeze-and-excitationattention mechanism）是一种被广泛运用的注意力机制，
在解决计算机视觉问题中有很好的效果[12]。但它是一种通道注意力机制，根据上文提
到的核性白内障患者晶状体核性区域特点，我们更倾向于关注空间维度。坐标注意力
（coordinateattention）是一种通道注意力与空间注意力结合的注意力机制，它能充分
的利用空间信息[13]。但它将纵向维度与横向维度等价处理，对空域的处理没有针对性，
而我们更想对纵向维度给予更多的关注，所以其对于解决我们的问题也有一定的不足。
空间分组增强（Spatial Group-wise Enhance）模块是一种通过对通道分组，达到凸显
其特征语义的空间注意力机制[14]。虽然它是针对空间的注意力机制，但我们的图片并
没有特别明显的语义特征，所以并不适用于我们的问题。但它的分组思想可以借鉴使
用来达到我们想要更加关注纵向维度的目的。
图1 临床特征融合注意力模块（CFI）
根据白内障临床特征与注意力网络特点，本文提出是否可以将像素均值转化为特
征图的特征表示，并将表示图像下半部分的特征放大，以此来提升核性白内障的分类
效果并提高网络的可解释性。为此，本文提出了一个临床特征融合注意力网络
（Clinical Features IntegratedAttentionNetwork，CFINet）来对核性白内障进行自动分
类。在提出的 CFINet 中，插入一个临床特征融合注意力模型（Clinical Features
IntegratedAttention Model，CFI）。如图 1所示，该模型对输入特征的纵向维度进行
2
分组并在横向维度进行平均池化操作，以获得其在每一纵向维度上的像素均值特征表
达，之后为每一个特征生成重要性系数，并进行归一化与激活函数处理，从而实现对
特征图重要信息的放大。
本文在两个数据集上进行实验，以证明本文模型的性能高于已有注意力网络。为
了更直观的了解模型的决策过程，本文使用类激活映射可视化方法来展示特征的权重
分布。
2.临床特征融合注意力网络框架
2.1 临床特征融合注意力模块
图 2 所示为该临床特征融合注意力网络结构图。对于任一卷积特征图
(其中，N 表示批大小，C 表示通道数，H 和 W 表示特  征 =
×  ×  × 
[图  1的 ,  高2,和 ...宽 ,  )  。 ] ∈我  们考虑将其沿H 维度划分为G组，目的是凸显出那些表示图像下半
部分的组的特征。在上文白内障临床特征部分提到过可选择平均池化的方式来凸显像
素均值这一关键特征，所以首先要对W维度进行平均池化操作，以获得其在每一 H
维度上的特征表达：
（1）
1
同时要利用该特征表达为每  一=个  特 征0≤生  ≤成   相  (应ℎ,的  )重要性系数，这一步可以使用简
单的点乘来实现：
（2）
为了防止不同组之间系数大小有偏   差 =，  我 ∙  们  在W维度上对 c 进行归一化处理：
， ， （3）
−   1  2 1  2
其中 是为了   数  =值  稳  +定  性  而  =添  加 的  常   数。   简=单  地 归  (一   化−可   能) 会改变图层可以表
示的内容，ϵ为了确保插入网络中的转换可以表示恒等转换，引入一对参数 、 来对
归一化值进行缩放和移位：
β γ
（4）
为了获得增强的特征向量 ，  原  =始      +被  生成的重要性系数 通过 sigmoid
函数作为门控单元在空间上缩放   ：     
（5）
最后，所有增强的特征组成了特     征 =组   ∙  (   ) 。
×  ×  × 
3    = [   1,   2...,     ] ∈ 
图2 临床特征融合注意力模块（CFI）结构图
2.2 网络模型
本文网络模型以 ResNet[15]（residual convolutional neural network，残差卷积神经
网络）作为网络骨架，原因在于许多其他的注意力网络都使用它作为主干网络，且它
在分类任务中可以取得很好的结果，可以更好的验证本文网络模型的性能。本文将提
出的临床特征融合注意力模块插入到残差模块中构成残差临床特征融合注意力模块，
本文的网络框架 CFINet 是由多个残差临床特征融合注意力模块组合而成的。
3.数据集介绍
3.1 临床 AS-OCT 影像数据集
临床 AS-OCT 影像数据集来源于某一三甲医院，由日本 Tomey 公司生产的
CASIA2 设备进行采集。数据集包括530名受试者以及839只眼睛，其中左眼428只，
右眼411只，经专业医生筛选后得到15884张可用的AS-OCT 影像。目前还没有基于
4
AS-OCT图像的白内障分级标准，由于受试者同时拍摄了AS-OCT图像与裂隙灯图像，
所以由专业医生基于 LOC Ⅲ分级标准给出裂隙灯图像下的白内障级别并进行映射，
以得到每张 AS-OCT 图像的白内障严重级别标签。根据临床经验以及数据规律，同一
个受试者的两只眼睛白内障严重级别相近，所以本文以受试者为基本单位对数据进行
随机拆分，以6：2：2的比例分为训练集，验证集以及测试集。表1为训练集、验证
集和测试集的三种严重程度的核性白内障的 AS-OCT 影像数量分布。
表1 三种核性白内障严重级别数量分布表
数据集 正常 轻度 重度
训练集 1004 2872 5518
验证集 229 569 2302
测试集 345 1305 1740
总计 1578 4746 9560
3.2 LAG 数据集
LAG 数据集是参考文献[16]Li 等人为检测青光眼建立的眼底图像数据集。选择该
数据集的原因在于，青光眼患者的一大特征就是视杯面积与视盘面积之比一般较大[17]，
也就是说在“高”维度上的像素分布对于在眼底图像上诊断青光眼也是重要的一点。
实验将数据随机分为训练集，验证集和测试集，其中训练集有图片 2911张，验证集
有图片972 张，测试集有图片971张。表2为训练集、验证集和测试集的两种眼底图
像数量分布。
表2 两种眼底图像数量分布表
数据集 正常 患病
训练集 1885 1026
验证集 629 342
测试集 629 343
总计 3143 1711
4.实验设置与结果分析
4.1 评价标准
5
根据参考文献[18，19，20],本文采用ACC（Accuracy，准确率）、PR（Precision Rate，
精确率）、Sen（Sensitivity，召回率）、F1 指标以及 Kappa 系数五种评价标准来衡
量本文方法的性能。其中，准确率表示方法预测出的正确数目占总数的比例，精确率
表示实际正例数目占被分为正例的比例；召回率表示分类器中判定为真的正例占总正
例的比率；F1 指标是精确率和召回率的调和平均值。准确率、精准率、召回率和 F1
评价指标可以通过以下公式表示：
（6）
+  
=   +   +   +   （7）

=   +   （8）

=   +   （9）
2×   ×  
其中，TP表示真阳性，TN表示  真1阴=性   ，+  F  P 表示假阳性，FN表示假阴性。
4.2 实验设置
实验使用PyTorch、scikit-learn等软件包来实现模型搭建。深度学习模型采用SGD
（StochasticGradient Descent，随机梯度下降优化器）作为优化器。epochs（训练周期）
设置为150，batch size（批量大小）设置为32、LR（Learning Rate，初始化学习率）
设置为 0.0025，当 epochs 小于 10 时，每个 epochs 学习率增加十分之一；当 epochs
大于 100 时，每个 epochs 学习率缩小 10 倍。本实验将所有影像的高和宽都缩放为
224*224，作为神经网络模型的输入。实验硬件环境配置为1张 NvidiaTitanGPU ，
Ubuntu 操作系统，DDR 12GB 内存。
4.3 基准模型
为了验证本文模型的有效性，实验选择以下先进注意力模型作为基准注意力模型
进行对比：SE（挤压激励注意力）、CA（坐标注意力）、SGE（空间分组增强）、
SRM[21]（风格校准模块）和BAM[22](瓶颈注意力组件)。其中既有通道注意力模型、
空间注意力模型，也有通道与空间相结合的注意力模型，可以更全面的验证本文模型
的性能。
4.4 结果分析
6
实验采用 Resnet18 作为网络骨架，将各种注意力模型嵌入其中进行分类任务。
表3展示了本文的注意力模块CFI与其他注意力模块在临床 AS-OCT 影像数据集上
的分类效果对比。
表3 不同注意力机制在临床AS-OCT影像数据集上的分类结果对比表
方法 ACC PR Sen F1 Kappa
Resnet18 88.557 86.882 91.356 88.612 74.823
Resnet18+SE 89.516 86.315 93.883 89.075 77.100
Resnet18+CA 91.484 88.562 94.060 90.697 80.672
Resnet18+SGE 89.874 87.563 93.321 89.781 77.958
Resnet18+SRM 90.968 88.469 94.093 90.639 79.724
Resnet18+BAM 90.097 87.872 94.630 90.138 78.395
Resnet18+CFI 92.161 87.981 94.232 90.712 82.092
注：加粗数字表示最优结果。
可以看到，本文模块在ACC、F1、Kappa 三个评价标准上的效果都可以达到最好，
分别为92.161%，90.712%，82.092%，且在ACC 评价标准中，相比其他模块能够提
高 1-2%；在 PR 和 Sen 标准中，SRM 与 BAM 取得了最好的效果，但 CFI 模块与其
相差很小。
表4列出了CFI模块与其他模块在LAG数据集上的实验结果对比，可以看到CFI
模块在五个评价标准中都取得了最高的成绩，分别为95.057%，94.933%，94.183%，
94.539%，89.079%，证明该模型具有很好的泛化能力。
表4 不同注意力机制在LAG数据集上的分类结果对比表
方法 ACC PR Sen F1 Kappa
Resnet18 94.342 93.991 93.573 93.776 87.553
Resnet18+SE 93.827 93.195 93.308 93.251 86.502
Resnet18+CA 94.753 94.505 93.958 94.221 88.443
Resnet18+SGE 93.827 93.063 93.507 93.277 86.555
Resnet18+SRM 94.342 93.559 94.170 93.849 87.700
Resnet18+BAM 93.210 92.614 92.500 92.556 85.113
Resnet18+CFI 95.057 94.933 94.183 94.539 89.079
注：加粗数字表示最优结果。
7
4.5 可视化结果分析
4.5.1分组权重可视化
本文统计了在不同深度的 CFI模块中，不同组权重的分布。由于在实验中发现当
组数为 7 时，本文模型可以得到最好的结果，所以在分组权重分析中使用组数为 7
的模型。如图3所示，分别展示出了layer1.1、layer2.1、layer3.1 和layer4.1 这4层中
对于不同组的权重折线图。因分组按照图像纵向维度进行，可以认为0-2 组代表图像
上半部分，3组代表图像中间部分，而 4-6 组代表图像下半部分。之前提到，白内障
的病理区域大多表现在图像的中间以及下半部分。由图可以看出，CFI模块在每一层
都倾向于赋予图像下半部分相对更大的权重值，尤其对于第4组，基本所有层中都将
最高的权重赋给了它。而随着网络的加深，网络模型对于图像的病理区域更为关注，
比如在 layer4.1，给予了 3-6 组更大的权重值。以上结果说明了本文提出的 CFI 模块
能够提高对白内障病理区域的关注，以此来提高分类效果以及提升模型决策过程的可
理解性。
(a) layer1.1的权重 (b)layer2.1的权重
(c)layer3.1的权重 (d)layer4.1的权重
图3 CFINet分组权重分布折线图
8
4.5.2基于类激活映射方法的可视化
本文使用 CAM（class activation mapping，类激活映射）[23，24]可视化方法来展示
神经网络在推理过程中对图片的重点关注区域。如图 4所示，为CFINet 和其他注意
力网络模型具有代表性的三种级别核性白内障图片的可视化类激活图。其中第一行是
一张正常的 AS-OCT 图像以及其对应的各种注意力网络模型的类激活图；第二行是患
有轻度核性白内障患者的 AS-OCT 图像以及其对应的各种注意力网络模型的类激活
图；而第三行则代表患有重度核性白内障患者的AS-OCT 图像以及其对应的各种注意
力网络模型的类激活图。第一列是三种级别核性白内障的AS-OCT 图片，而第二列到
第七列分别代表 SE、CA、SGE、SRM、BAM、CFI六种注意力网络模型所对应的类
激活图。可以看出，对于正常的AS-OCT 图像，由于像素分布比较均匀，六种注意力
网络模型都关注整个核性区域。而对于患病的AS-OCT 图像，本文的CFINet 相比于
其他的注意力网络模型能够更多的关注到图像的病理区域，符合白内障的临床特征，
也解释了本文模型性能高于其他模型的原因。
图4 CFINet与其他注意力网络模型的可视化类激活图
5.结束语
本文将白内障临床特征融入模型设计，根据影像像素特征以及浑浊分布设计出一
种临床特征融合注意力网络的白内障分类模型（CFINet），实现了在AS-OCT 影像下
核性白内障的自动分级。本文模型在 AS-OCT 影像数据集上的测试准确率达到了
92.161%，可以高出对比方法1-2%。可视化结果显示出，本文模型能够更加关注核性
9
白内障关键病理部位，解释了取得好结果的原因。在 LAG数据集上的实验证明，本
文模型具有良好的泛化能力，对于解决其他眼病的分类问题有一定的参考价值。
然而，如何将更多的临床特征融入到注意力网络设计中来提升模型分类效果，是
接下来需要解决的问题。对于可解释性方面的理论证明，也需要进一步进行研究。同
时，如何设计更轻量级的网络，以能够将模型部署到眼科医疗系统中，也是需要继续
努力探索的问题。
10
参考文献
[1] GALI HELENAE, SELLA, RUTI,AFSHARI, NATALIEA. Cataractgrading systems: areview of
pastandpresent[J].Currentopinioninophthalmology,2019,30(1):13-18.
[2] LT CHYLACK, J K WOLFE, D M SINGER, M C LESKE, MA BULLIMORE, I L BAILEY, J
FRIEND, D MCCARTHY, SYWU. The Lens Opacities Classification System III[J].Archives of
Ophthalmology,1993Jun;111(6):831-6.
[3] OZGOKCE M, BATUR M, ALPASLAN M, et al. A comparative evaluation of cataract
classifications based on shear-wave elastography and B-mode ultrasound findings[J]. Journal of
Ultrasound,2019,22(4):447-45.
[4] Cao Guiping,Zhao Wei,Higashita Risa,Liu Jiang,Chen Wan,Yuan Jin,ZhangYubing,Yang Ming.An
Efficient Lens Structures Segmentation Method on AS-OCT Images[J]. 42ND ANNUAL
INTERNATIONAL CONFERENCES OF THE IEEE ENGINEERING IN MEDICINE AND
BIOLOGY SOCIETY: ENABLING INNOVATIVE TECHNOLOGIES FOR GLOBAL
HEALTHCAREEMBC'20,2020.
[5] 刘 振 宇 , 宋 建 聪 . 基 于 深 度 学 习 的 白 内 障 自 动 诊 断 方 法 研 究 [J]. 微 处 理
机,2019,40(03):48-52.
[6] 李建强,张苓琳,张莉,杨吉江,王青.基于深度学习的白内障识别与分级 [J]. 第二军医大学
学报,2018,39(08):878-885.DOI:10.16781/j.0258-879x.2018.08.0878.
[7] Xu X, Zhang L, Li J, et al. A Hybrid Global-Local Representation CNN Model for Automatic
CataractGrading[J].IEEEJournalofBiomedicalandHealthInformatics,2020,24(2):556-567.
[8] WongAL, Leung K S, Weinreb R N, et al. Quantitative assessment of lens opacities with anterior
segmentopticalcoherencetomography[J].BritishJournalofOphthalmology,2009,93(1):61-65.
[9] Makhotkina, Natalia, Y, et al. Comparability of subjective and objective measurements of nuclear
densityincataractpatients[J].ActaOphthalmologica,2018.
[10] Thylefors B, Chylack L T, Konyama K, et al. A simplified cataract grading system The WHO
CataractGradingGroup[J].OphthalmicEpidemiol,2002,9(2):83-95.
[11] 陈朝一,许波,吴英,吴凯文.医学图像处理中的注意力机制研究综述[J].计算机工程与应
用,2022,58(05):23-33.
[12] Hu J, Shen L, Sun G, et al. Squeeze-and-Excitation Networks[J]. IEEE Transactions on Pattern
AnalysisandMachineIntelligence,2017,PP(99).
[13] Hou Q, Zhou D, Feng J. Coordinate attention for efficient mobile network design[C] //
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021:
13713-13722.
[14] Xiang Li, Xiaolin Hu, Jian Yang. Spatial Group-wise Enhance: Improving Semantic Feature
LearninginConvolutionalNetworks.[J].CoRR,2019,abs/1905.09646.
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun 0001. Deep Residual Learning for Image
11
Recognition.[J].CoRR,2015,abs/1512.03385.
[16] Li Liu et al. A Large-Scale Database and a CNN Model for Attention-Based Glaucoma
Detection.[J].IEEEtransactionsonmedicalimaging,2020,39(2):413-424.
[17] 徐志京,汪毅.青光眼眼底图像的迁移学习分类方法[J].计算机工程与应
用,2021,57(03):144-149.
[18] 章晓庆,方建生,肖尊杰,陈浜,Risa HIGASHITA,陈婉,袁进,刘江.基于眼前节相干光断层扫
描成像的核性白内障分类算法[J].计算机科学,2022,49(03):204-210.
[19] Zhang X ,Hu Y ,Xiao Z , et al. Machine Learning for Cataract Classification/Grading on
Ophthalmic Imaging Modalities: A Survey[J]. Machine Intelligence Research, 2022,
19(03):184-208.
[20] Cao L, Li H, Zhang Y, et al. Hierarchical method for cataract grading based on retinal images
usingimprovedHaarwavelet[J].InformationFusion,2019
[21] Lee H J, Kim H E, Nam H. SRM: A style-based recalibration module for convolutional neural
networks[C]// Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019:
1854-1862.
[22] Park J,WooS, Lee JY,et al.Asimple and light-weight attention module for convolutional neural
networks[J].InternationalJournalofComputerVision,2020,128(4):783-798.
[23] Zhou B, KhoslaA, LapedrizaA, et al. Learning Deep Features forDiscriminative Localization[J].
CVPR,2016.
[24] 司念文. 面向图像识别的深度学习可视化解释技术研究[D].战略支援部队信息工程大
学,2021.DOI:10.27188/d.cnki.gzjxu.2021.000012.
12
致谢
时光荏苒，四年转瞬即逝。我与南科会于2019年初秋，别于2023年盛夏，回忆
四年，皆为美好。
感谢在生活与学习中给予我帮助的导师刘江教授。我们一起庆祝节日，一起爬山，
一起玩耍，即使远在他乡，刘老师也给了我家一般的温暖。在遇到学习问题，学术困
难时，刘老师也总会耐心的给予最全面的帮助。也要感谢实验室的章晓庆师兄，师兄
重视培养我的科研思维，对于论文的选题和写作都给予了非常大的帮助，同时师兄对
于科研的认真与坚持也是我在学术生涯中需要一直学习的。
感谢我的父母，一直默默的在我身后支持着我的求学生涯。你们说：“我俩好好
工作，你好好学习，一家三口一起努力！”你们说：“我们对你现在做的事情了解不
多，但一定会尊重和支持你的决定！”你们是我成长路上最温暖的港湾！我为有这样
一个美满的家庭而感到十分的幸运与骄傲。
感谢朋友们，感谢易哥，涛哥，轶群哥，小龙，阿祖，阿乐，阿豪……四年中无
数的悲伤失意时刻，都有兄弟们的陪伴和支持。欲买桂花同载酒，终不似，少年游？
时光流逝难挡，但祝我们永远拥有少年之心，每当相会时都能找回如这四年般的意气
风发。
感谢遇到的所有人，一次次的相遇，成就了如今更好的自己。也要感谢自己，克
服这一路上的困难，走在自己认为正确的道路上，希望自己永远乐观，永远自信，永
远抬头挺胸。
终有一别，但来日方长。
13