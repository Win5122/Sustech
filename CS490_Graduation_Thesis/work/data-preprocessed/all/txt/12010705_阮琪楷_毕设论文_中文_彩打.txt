分类号 编号
U D C 密级
本科生毕业设计（论文）
题 目： 面向 Vision Transformer 的
交互式可视化教学系统
姓 名： 阮琪楷
学 号： 12010705
系 别： 计算机科学与工程系
专 业： 计算机科学与技术
指导教师： 马昱欣 副教授
2024 年 6 月 7 日
诚信承诺书
1. 本人郑重承诺所呈交的毕业设计（论文），是在导师的指导下，
独立进行研究工作所取得的成果，所有数据、图片资料均真实可靠。
2. 除文中已经注明引用的内容外，本论文不包含任何其他人或
集体已经发表或撰写过的作品或成果。对本论文的研究作出重要贡
献的个人和集体，均已在文中以明确的方式标明。
3. 本人承诺在毕业论文（设计）选题和研究内容过程中没有抄袭
他人研究成果和伪造相关数据等行为。
4. 在毕业论文（设计）中对侵犯任何方面知识产权的行为，由本
人承担相应的法律责任。
作者签名:
2024 年 6 月 7 日
面向 Vision Transformer 的
交互式可视化教学系统
阮琪楷
（计算机科学与工程系 指导教师：马昱欣）
[摘要]：机器学习领域在近年取得较大突破，投入该领域学习的人数水
涨船高，但是由于其技术较为复杂抽象，对于很多初学者而言有一定的
学习难度，而神经网络作为机器学习的核心技术之一，理解其内部的结
构和运作原理对初学者而言是一个不小的挑战。使用合适的可视化设计
能够将复杂的神经网络结构和运算过程以直观、生动的方式展现出来，
方便初学者理解神经网络，从而更容易投入机器学习方面技术的学习。
Vision Transformer 模型因其注意力机制在计算机视觉领域表现突出，但
由于其结构复杂，学习门槛较高。本项目设计并实现了一个面向 Vision
Transformer 模型的可视化教学系统，以可视化交互的形式展现模型处理
图片分类任务的全过程，并将重点放在注意力机制的展示上，用户通过
操作该系统可以了解到注意力机制的实现以及模型的整体架构，有助于
理解模型的运行机制。此外，本项目通过案例分析和用户调研的方式，验
证了系统帮助用户理解学习的有效性。
[关键词]：可视分析，数据可视化，Vision Transformer
I
[ABSTRACT]: The field of machine learning has made significant break-
throughs in recent years, attracting a growing number of learners. However,
due to its complex and abstract nature, it presents a certain level of difficulty
for many beginners. As one of the core technologies in machine learning, un-
derstandingtheinternalstructureandoperationprinciplesofneuralnetworksis
a considerable challenge for novices. Employing suitable visualization designs
can present the complex structure and computational processes of neural net-
works in an intuitive and vivid manner, facilitating beginners’ comprehension
of neural networks and making it easier for them to engage in learning machine
learning technologies. The Vision Transformer model has demonstrated out-
standing performance in the field of computer vision due to its attention mech-
anism. However, its complex structure poses a high learning threshold. This
project designs and implements a visualization teaching system for the Vision
Transformermodel,showcasingtheentireprocessofthemodelhandlingimage
classificationtasksthroughinteractivevisualizations. Thefocusisplacedonil-
lustrating the attention mechanism. By interacting with this system, users can
understand the implementation of the attention mechanism and the overall ar-
chitectureofthemodel, aidinginthecomprehensionofthemodel’soperational
mechanisms. Additionally, the effectiveness of the system in helping users un-
derstand and learn has been validated through case studies and user surveys.
[Key words]: Visual Analytics, Data Visualization, Vision Transformer
II
目录
1. 导言 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
2. 相关工作和理论基础 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.1 Transformer 模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.1.1 自注意力机制 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.1.2 多头机制 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Vision Transformer 模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2.1 图片分割编码机制 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2.2 cls token 分类机制 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.3 可视化系统调研 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3. 系统需求分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.1 图片分块编码展示 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2 模型整体层次划分 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.3 多头自注意力机制实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.4 ViT 逐层输出比较 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.5 图片分类效果展示 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4. 系统设计与实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
4.1 系统总体设计布局 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
4.2 图片选择和编码模块 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.3 模型逐层输出模块 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4.4 多头自注意力模块 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
III
4.5 模型输出分类模块 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
4.6 相关实现技术 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
5. 案例分析 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.1 对比分析注意力权重矩阵 . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.2 对比分析编码层输出 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
6. 用户调研 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
6.1 总体框架 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
6.2 可视化视图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
6.3 改进建议 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
7. 总结与未来展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
7.1 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
7.2 未来展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
参考文献 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
致谢 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
IV
1. 导言
机器学习领域在近年取得了许多令人瞩目的突破，深度学习模型的广泛应用是
其中的重要因素之一。这些模型在图像识别、语音识别、自然语言处理等领域取得了
显著进展，准确性和效率上都有巨大的提升，使得计算机能够以接近甚至超越人类水
平的准确率来处理各种复杂的任务。与之相应，投入机器学习领域研究的人数水涨船
高，但是由于其技术较为复杂抽象，对于很多初学者有一定的学习难度，神经网络作
为机器学习的核心技术之一，理解其内部的结构和运作原理对初学者而言是一个不
小的挑战。视觉和交互方法已被证明可以有效地为经验丰富的模型研究者解释复杂
模型的工作机制和概念，借助交互式可视化来理解、诊断和完善机器学习模型的过
程，有助于用户解决现实世界的人工智能问题[1][2]，将这种方式推广到帮助初学者理
解模型上值得一试。使用合适的可视化设计能够将复杂的神经网络结构和运算过程
以直观、生动的方式展现出来，方便初学者理解神经网络，从而更容易投入机器学习
方面技术的学习。
深度学习模型在计算机视觉领域的广泛应用推动了该领域的快速发展，卷积神
经网络（CNN）、残差神经网络（ResNet）等经典的神经网络架构在许多任务，如
图像分类、目标检测、图像分割等任务上表现优异[3][4][5]。2017 年 Vaswani et al. 提
出的 Transformer 模型[6]在自然语言处理领域效果不俗，许多研究者也开始探索该模
型迁移到计算机视觉领域的可行性。于是 2020 年，Dosovitskiy 等人提出了 Vision
Transformer模型[7]，在图像分类任务中达到同期最优水平，证明了直接应用于图像的
纯Transformer架构可以在图像分类任务上表现良好。
1
图1 交互式可视化系统示意图
本项目以开发一个可视化系统为目标（如图1所示），将VisionTransformer处理
图片分类任务的过程以可视化交互的形式展现，从而便于在机器学习课程的教学中
加深初学者对VisionTransformer的理解。具体而言，系统分为图片分块和编码模块、
模型分层输出展示模块、多头自注意力模块、分类视图展示模块。不同的模块之间联
系紧密且存在交互联动，在相互呼应之下降低用户学习VisionTransformer的门槛。
2
2. 相关工作和理论基础
2.1 Transformer 模型
Transformer是一种在自然语言处理（NLP）领域广泛使用的深度学习模型架构[8]，
这个模型在机器翻译领域取得了不俗的效果，且相较于ResNet和CNN，Transformer
不需要递归和卷积，具有更好的并行性。同时该模型的拓展性同样优异，不止在机器
翻译领域成果显著，在自然语言处理领域的其他任务，比如语言建模、文本生成等，
表现同样良好。特别是在大语言模型领域，OpenAI 于 2018 年推出了 GPT[9]，利用
Transformer 的解码器进行语言建模和生成；同一时期，Google 推出了 BERT 这一完
全依赖于 Transformer编码器进行预训练和微调的模型[10]。此外，各种基于这两个大
模型的拓展和增强版本，如XLNet[11]、GPT-2/3[9][12]和InstructGPT[13]，进一步丰富了
大语言模型这一领域。
图2 Transformer结构示意图
Transformer 的结构如图 2 所示，其核心思想是通过自注意力机制，允许模型在
处理序列数据时能够直接聚焦于序列中的任意部分，从而更有效地捕捉序列内部的
3
依赖关系。其主要由编码器和解码器两部分组成，两个部分都由 N=6 层的编码器层
和解码器层串联。编码器将输入的序列进行编码，解码器接收编码器的结果并根据其
进行解码，并生成新的序列作为目标输出。在Transformer的实现中有两个核心部分，
分别是自注意力机制和多头机制。
2.1.1 自注意力机制
自注意力机制是 Transformer 提出的核心概念，也是该模型有别于其他模型的核
心之处。自 Transformer 发表以来，自注意力机制被广泛应用于之后的众多深度学习
模型之中，由此可见其性能之优异、在该模型中地位之重要。注意力指对一定对象
的指向和集中，有选择的关注该对象并忽略其余对象。在 Transformer 模型中，注意
力机制指模型选择性地确定在输入序列中哪些元素更重要，并根据重要性动态调整
不同元素对输出序列的影响。该机制的具体实现通常涉及到计算查询（Query）、键
（Key）和值（Value）三者之间的交互，以确定信息的权重分布，通过权重分布来确
定不同元素的重要性。由此，模型能够在处理复杂的输入序列时，有效捕捉长距离依
赖关系，从而提高处理信息的能力和准确性。
( )
QKT
Attention(Q,K,V) = softmax √ V
d
k
注意力机制先根据Q和K 的相似程度来获得一个权重函数，这一过程通过计算
√
Q 和 K 之间的点积来实现，然后再将这个点积结果除以 d ，其中 d 是 K 向量的
k k
维度。这样做的目的是为了调整点积的幅度，避免在较高维度下因点积过大而导致梯
度消失的问题。通过这种缩放可以得到一个更加稳定的相似度度量，进而可以通过应
用 softmax 函数将这些相似度转换成取值范围在 [0, 1] 且和为 1 的权重。得到的权重
直接反映了K 对应的V 对于给定Q的重要性。具体来说，如果一个K 与V 更相似，
那么这个键对应的值就会被赋予更高的权重。最后，这些权重将应用于值矩阵，通过
加权求和的方式，生成了对每个 Q 的响应，这个响应就是注意力机制的输出。这个
过程可以理解为：对于每一个查询向量 Q，注意力机制都会在所有的键向量 K 中寻
找最匹配（即最相似）的部分，并将对应的值加以聚焦和突出作为输出。因此，通过
4
这种机制，模型在处理序列数据时能够动态地聚焦于与当前任务最相关的信息，提高
处理效率和准确性。
2.1.2 多头机制
当输入的序列过长或者过于复杂时，单一的注意力机制很可能无法兼顾数据中
的所有信息，产生的遗漏可能影响最终输出的准确性。同时，在处理复杂数据时，使
用单注意力机制的模型可能存在对特定数据过拟合的风险，降低模型对不同输入的
泛化能力。为了解决以上问题，Transformer模型引入多头注意力机制，即并行运行多
个注意力机制，这样既可以让模型同时关注到不同表示子空间中的信息，使得模型对
输入序列中的信息捕捉更为全面，还能通过学习到输入数据的不同表示来增强对不
同输入的适应能力，降低在使用复杂数据进行训练的过程中可能存在的过拟合风险。
图3 多头机制示意图
MultiHead(Q,K,V) = Concat(head ,head ,...,head )WO
1 2 h
head = Attention(QWQ,KWK,VWV)
i i i i
5
如图 3 所示，在多头注意力机制中，输入的查询向量 Q、键向量 K 和值向量 V
被同时投影到多个子空间中，这些子空间也被称为“头”。每个头都拥有自己独立的
权重矩阵来进行线性变换，从而使得每个头可以关注输入数据的不同方面或特征。每
个头中输入的 Q、K 和 V 分别与对应的权重矩阵相乘进行线性变换。具体来说，Q、
K 和V 分别与权重矩阵WQ、WK、WV 相乘，其中i表示头的索引。这些模型通过
i i i
训练学习得到权重矩阵使得每个头可以专注于不同的特征。通过线性变换，每个头能
够将输入数据映射到自己独特的表示空间中，以便更好地捕获输入序列的相关信息。
所有头的输出在计算完成后将被拼接在一起形成一个更大的输出矩阵，这个操作使
得每个头捕捉到的不同特征能够被综合考量，模型可以获得更丰富和全面的信息表
示。最后，通过另一个线性变换WO，将拼接后的输出进一步处理，以生成最终的多
头注意力机制的输出。总的来说，多头注意力机制的运行过程充分利用各个头学习到
的信息并调整其表示，以便更加适应后续操作。
2.2 Vision Transformer 模型
Transformer 在自然语言处理领域成绩斐然，令不少其他领域的研究者开始思考
能否将其迁移到本领域中。但在计算机视觉领域试图使用 Transformer 架构来完成图
像识别任务时，面临着一些难题：高维的图像数据无法直接作为 Transformer 这一序
列模型的输入，即便是分辨率较低的图像（如 224×224），若将每个像素点作为序列
中的一个输入词元，序列的长度也将达到50，000；与卷积神经网络这一已在计算机
视觉领域证明自己的模型相比，Transformer缺乏局部性、平移不变性等归纳偏置，这
可能对 Transformer 处理图像数据时带来困难，可能需要在训练模型时使用大量的数
据来弥补这方面的不足。
2020年，Dosovitskiy等人提出VisionTransformer（后文称ViT）模型，以相对简
单的方法四两拨千斤地解决了上述的难题。如图 4 所示，ViT 只使用了 Transformer
的编码器部分，通过将图像分割为一系列的小块（称为“patch”），编码后将这些小
块序列化以满足传统Transformer对于输入序列的要求。相比于标准的Transformer模
型，ViT最显著的区别在于其图片分割编码机制和引入classtoken 的分类机制。
6
图4 VisionTransformer结构示意图
2.2.1 图片分割编码机制
将图片的每个像素点作为序列中的一个词元进行编码会导致序列长度过长，为
了解决这一问题，ViT将图片分割成一系列小块，再对这些小块进行编码和序列化操
作：对一张大小为H×W 的图片，将其不重叠地分割为M ×M 张大小固定为P ×P
的图像块（其中M = HW/P2），由于图像的输入通道数为C，每个图像块将被展平
为 D = P ×P ×C 的向量。由于块状分割操作破坏了原来图片中 2D 平面的位置信
息，ViT 通过一个可学习的 1D 位置编码来补全。因此，，最终经过分割编码，可以
得到一个长度为 M ×M 的序列，且序列中每个元素都是一个 D = P ×P ×C 的向
量，从而适应传统 Transformer 对输入序列的要求。以分辨率为 224 × 224 的图片为
例，ViT将其分割为196个大小为16×16的小块，同时由于输入通道数为3，每个图
像块将被编码为D = 16×16×3 = 768的向量。最终这张224×224的图片将被编码
为一个196×768 的序列，能够被Transformer识别并作为输入。
7
2.2.2 clstoken分类机制
为了能更好的学习到图片的特征，以方便完成最终的图片识别和分类任务，ViT
借鉴了BERT模型[10]中的classtoken机制，将可学习的嵌入词元嵌入到图像块编码中，
从而在模型的运行过程中持续学习整张图片的特征。具体做法是，在图片经过分割和
编码形成 M2 ×D 的序列矩阵输入到 ViT 模型前，在矩阵的开头添加一行作为 class
token，然后将形状为 (M2 + 1) × D 的矩阵作为模型的输入。以分辨率为 224 × 224
的图片为例，在图片被编码为一个 196 × 768 的序列后，在其首位添加一行，形成
197×768 的矩阵输入到模型中。经过模型内部多层编码器的处理，最终输出的 class
token将能够预测图片的类别。
2.3 可视化系统调研
深度学习模型的可解释性和透明度仍然是一个长期存在的问题[14]，也正是这些
问题使得不少初学者望而却步。越来越多的研究人员正积极探索理解、比较和增强深
度学习模型的方法[15][16]，近年来，可视化对深度学习可解释性的贡献赢得了广泛赞
誉[17]。目前已有一些机器学习的可视化工具完成了特定神经网络的交互性展示，如
CNN Explainer[18]在其系统中解释了卷积神经网络的模型结构和其中的底层数学运算
过程，将各层的输出和连接可视化，同时允许用户自由上传图片并编辑网络的整体
展示。GAN Lab[19]允许用户交互式地训练生成模型，并可视化动态训练过程的中间
结果，通过集成概述图，总结生成对抗网络的结构以及分层分布视图，帮助用户解
释子模型之间的相互作用。TransforLearn[20]设计了结构驱动探索和任务驱动探索两
种不同的探索模式，对 Transformer 模型不同层级的细节及其工作流程进行了细致分
析，并提供每层操作和数学公式的交互式视图，帮助用户理解长文本序列的数据流。
LSTMVis[21]分析 LSTM 网络的隐藏状态的动态变化，允许用户选择输入范围以关注
局部状态变化并过滤隐藏状态，将视觉信号与噪声分开。DQNViz[22]演示了 Deep Q
网络在四个级别中训练过程的细节，并在代理模型的经验空间中进行全面分析。
此外，还有一些可视化工具将重点放在Transformer的注意力机制上，如AttViz[23]探
索自注意力得分与真实数据之间的关联性，从输入序列和整体这两个角度来解释注
意力权重矩阵；AttentionFlows[24]支持用户在基于Transformer实现的模型中查询、跟
8
踪和比较层内、跨层以及注意力头之间的注意力，并将设计重点放在描述不同层间的
注意力如何在输入中的词元中流动；Dodrio[25]重点关注注意力权重对输入句子的结
构和语义信息的影响。这些可视化系统出于帮助初学者更好理解深度学习模型相关
的概念这一目的而设计，但是在 Vision Transformer 这一神经网络上，面向初学者的
交互式可视化教程还存在一定的空白。
9
3. 系统需求分析
3.1 图片分块编码展示
Vision Transformer 的一个重点就是如何将一张二维的图片编码为一串能够被
Transformer 模型识别的序列，因此当用户使用该系统来帮助其学习 ViT 模型时，了
解图片的分块及编码是学习过程的重要一环。因此要求系统提供简洁的界面以清晰
展示图片的分块形式和编码信息，帮助用户直观了解图像块的位置、对应的嵌入向量
等相关信息，全方位理解ViT模型的patchembedding过程。
3.2 模型整体层次划分
针对深度学习的初学者这一系统的主要用户群体，他们一般对Transformer、ViT
了解较少，对模型的运行过程、内部架构通常都不熟悉甚至完全不了解。如果缺少对
模型整体架构的展示，可能会给用户带来学习上的困难，甚至导致用户对模型的理解
出现偏差。因此在系统中提供完整、清晰的模型架构和层次介绍，帮助用户建立对模
型架构的整体认知非常重要。
3.3 多头自注意力机制实现
多头自注意力机制是Transformer和VisionTransformer的核心，因此这也应该是
初学者学习这两个模型时最关注的内容。如何将多头注意力机制的内在逻辑清晰的
展示出来，充分挖掘权重矩阵中包含的信息，帮助用户更好的理解透彻多头自注意力
机制的实现，是系统设计的重要工作。
3.4 ViT 逐层输出比较
在VisionTransformer中，模型只使用了Transformer的编码器部分，并且采取了
多层叠加的操作。用户在使用该系统时，希望能够看到输入的图片在每一个编码层的
输出结果，并将其相互对比，从而直观的了解到不同层的编码器更关注图片的哪些特
征，随着层次由浅入深，模型更加倾向于关注图片的哪些部分，有助于用户深入分析
模型在不同层次上的表征能力和学习特征的过程。
10
3.5 图片分类效果展示
在处理图像分类任务时，最终的分类效果能最直观的反映模型的性能，因此将
Vision Transformer 的分类结果采用可视化的方式清晰易懂地展示出来是系统的重要
任务。用户希望能够跟随系统动画，了解ViT提取classtoken这一包含整张图片信息
地特殊标记，并将其与最终的分类建立联系的整个过程，同时还希望模型预测分类的
概率分布也能以清晰直观的方式展现。
11
4. 系统设计与实现
4.1 系统总体设计布局
为了达到帮助初学者理解 Vision Transformer 模型的核心设计与主题架构这一目
标，本设计并实现了一个可视化动画系统。如图5所示，系统主要包含4个模块，分
别是图片选择和编码模块（a）、多头注意力模块（b）、模型逐层输出模块（c）以及
模型输出分类模块（d）。
图5 ViT-VIS系统总视图，其中a为图片选择和编码模块，b为多头注意力模块，c为模型逐层
输出展示模块，d为模型输出分类模块
整个系统动线清晰，从左到右依次为 Vision Transformer 的输入、主要神经网络
层、分类输出，符合模型主题架构和用户直觉，清晰的展示了系统的具体使用步骤，
降低了用户的使用门槛，从而更加专注到使用系统帮助对 Vision Transformer 的学习
理解之中。用户首先在图片选择视图（a1）中选择想要作为模型输入的图片，选中的
图片会在图片编码视图（a2）中展示，并在此视图展示选中图像的分割、嵌入编码操
作的可视化图表。以选中图片编码后的序列作为输入，后续模块会将该图片在模型
中各区域的特征信息直观的展现给用户：多头注意力模块（b）以热力图的方式展示
权重矩阵；模型逐层输出展示模块（c）清晰划分模型的整体层次，并反映不同层次
关注的模型信息特征；模型输出分类模块（d）将抽象的输出结果以连线的形式展示，
与输入图片的呼应让用户掌握图像分类任务的完成情况。丰富且相互呼应的交互符
12
合用户直觉，通过简单的点选操作就能让用户自由探索各视图中的详细信息，更能激
发用户深入使用该系统来辅助模型学习的兴趣。
由于有多个视图都涉及多维数据，为了让用户更直观的感受到数据的差异和变
化，同时让可视化效果更加简洁清晰，系统中添加了多个颜色图例。通过以图例与视
图数据建立映射的方式，多维数据中所蕴含的复杂信息将以叫直接明了的方式传递
给用户。
4.2 图片选择和编码模块
图片选择和编码模块分为两个视图：图片选择视图和图像块分割嵌入视图。如图
6所示，图片选择模块让用户通过鼠标点击的方式将关注的图片作为模型的输入，当
选中图片时目标图片上浮高亮并出现在图像块分割嵌入视图中，其余图片下沉以突
出选中图片；如图7所示，图像块分割嵌入视图支持用户通过移动鼠标的方式唤起当
前鼠标所在的图像块，并在下方出现该图像块对应的编码。通过鼠标的点击、移动，
用户可以自由探索到不同图片和图像块经过分割编码后生成的序列，并通过一侧的
颜色图例了解编码数值的大致变化，将其与图像块的信息特征建立对应。
与应用于机器翻译任务的传统Transformer不同，由于VisionTransformer的输入
序列具有长度和维度较长的特征，不能像一些针对前者的可视化系统一样将所有词
元编码同时展示：机器翻译中，“Why are you so happy?”的文本输入被编码成一个
长度为8的序列，而一张图片经过分割、嵌入、合并classtoken后形成的序列长度为
197。因此本系统以鼠标选中的方式，在同一时间只展示一个图像块对应的嵌入词元，
避免过多的同类信息同时展示给用户带来视觉疲劳。
图6 图片选择视图
4.3 模型逐层输出模块
模型逐层输出展示模块与 Vision Transformer 多层编码器串联的结构相对应，将
每层编码器的输出结果按顺序排列。需要注意的是，在实际的模型中，每层编码器最
13
图7 图像块分割嵌入视图
终的输出是一个 197×768 的矩阵，表示 class token 和 196 个图像块的编码结果。考
虑到描述一个图像块信息的长度为 768 的序列信息难以被用户直观感知，系统中采
用将矩阵转置的方式：在不考虑第一列（class token）的情况下，将得到一个形状为
768×196的矩阵。通过热力图的方式将矩阵按768×14×14的形式排列，并使用颜
色深浅将矩阵中的值表示出来，将得到768张由196个图像块拼接出来的图片（如图
8所示，由于系统篇幅限制，只展示了768张图片中的前5个和后5个）。
这 768 张图片相当于输入图片在某些方面特征的抽象化，而热力图的形式也通
过颜色的深浅在14×14的矩阵中勾勒出输入图片的大致轮廓，将图像的信息以更加
14
图8 模型逐层输出模块图示
直观易感知的方式展现给用户。同时，为了激发用户探索编码层的兴趣，本系统给每
个编码层都添加了直观的交互和流畅的动画效果。通过点击代表当前编码层的按钮，
整个模块会相应移动，展示编码层内部的核心结构：多头注意力模块。
4.4 多头自注意力模块
多头自注意力模块展示了 Transformer 模型的核心结构：多头机制和自注意力机
制，同时还添加了与其他模块的联动效果，帮助用户理解数据在模型中的变化过程。
如图9所示，在点击编码层按钮后，视图中会根据当前层的位置展开多头自注意力模
块，表示当前编码层的多头机制和自注意力机制。正如 2.1.1 节所述，自注意力机制
通过一个权重矩阵给输入序列中不同的词元赋予不同的关注度，因此在本模块中，系
15
统以热力图的形式展示注意力权重矩阵，辅以颜色图例，用户能通过颜色深浅变化了
解权重矩阵的变化。除此之外，模型以多头机制关注不同表示子空间的信息，增强模
型的泛化能力，因此同一编码层中，不同头的注意力权重矩阵也不一样。用户能够以
鼠标点击的方式选择不同的头，热力图中会随之切换展示当前头的注意力权重矩阵，
使得系统的信息表示更加全面。
图9 多头自注意力模块图示
此外，在注意力权重矩阵中还存在与图片选择和编码模块的联动：如图10所示，
当用户使用鼠标选中一个图像块时（图 10-a），权重矩阵中相关的行和列也将被同时
高亮选中（图 10-b）。该设计的目的是让用户了解到图像块在当前编码层和头中所关
注的内容。
16
a) 选中图像块图示 b) 权重矩阵中高亮对应行/列图示
图10 图像块与权重矩阵联动效果图
4.5 模型输出分类模块
本系统体现 Vision Transformer 模型完成图像分类任务的全过程，因此设计并实
现了模型输出分类模块，在视图中展示模型最终输出的编码，并将其与分类结果联
系起来。模型最后的输出经过归一化处理，放缩到[-1,1]的尺度。由于classtoken在
模型运行中学习到了所有图像块的特征信息，因此可以只根据 class token 来进行分
类。在系统中将classtoken抽取出来展示为一个长768的序列，其颜色深浅表示class
token 学习到的信息。然后在视图中使用连线将序列中的每一个信息与预测分类连接
起来（只展示概率最高的10个类别），直观地将分类结果与模型输出建立联系。
b) 视图——展开
a) 视图——折叠
图11 模型输出分类模块示意图
如图 11 所示，为了在单页面中即可展示系统的全部内容，系统采用折叠的方式
17
将输出分类视图的详细信息隐藏（图 11-a），当用户点击输出层按钮时，被折叠隐藏
的详细信息会随之展开（图 11-b）。通过这样的方式能够合理的排布空间，过渡动画
也能捕捉用户的注意力，让用户更加关注当前展示的视图。除此之外，系统中还添加
了高亮显示功能来更好抓取用户注意力。如图 12 所示，用户可以通过鼠标悬停的方
式选中分类，视图中将高亮从输出层到该分类结果的连线，辅以不同的颜色将输出序
列中每个值对分类的映射信息让用户直观感知（图 12-a）；用户还可以通过鼠标选中
单一连线，了解序列中的某个值与分类的相关程度，由于颜色深浅只能大致反映值的
大小，视图中添加了文字提示展示当前选中连线的具体数值（图12-b）。
a) 选中分类图示 b) 选中连线图示
图12 模型输出分类模块展开示意图
4.6 相关实现技术
在该系统的设计与实现过程中，主要采用Vue.js作为前端的基础框架。基于Vue.js
灵活高效的组件化开发优势，能够轻松地将整个系统划分为多个组件专注于特定功
能和视图的组件，使得整个项目的代码逻辑清晰、易于维护和拓展。同时，Vue提供
了丰富的生态系统和社区支持，开发者不仅能够根据需求选择到合适的第三方库，还
能轻松的将其集成到 Vue 项目中快速上手使用。为了满足系统对于可视化设计和动
画的需求，本系统使用D3.js[26]制作系统中的可视化视图和动画效果。D3.js作为可视
化领域广泛使用的库，提供了丰富的 SVG 元素支持和灵活的数据绑定操作，使得开
发者能够根据需求制作细腻流畅的动画效果、简洁清晰的可视化图表以及生动的交
18
互设计。此外，本文使用已训练完成的模型[27]并基于其获取运行过程中的数据。该
模型是VisionTransformer模型基于PyTorch的重新实现，基于ImageNet数据集训练，
用于完成计算机视觉领域的图像分类任务。
19
5. 案例分析
在本节中将以一张图片作为输入案例（图 13），观察其在 Vision Transformer 模
型各部分中的状态，展示本系统如何帮助用户更好理解VisionTransformer模型。
图13 图片输入案例
5.1 对比分析注意力权重矩阵
注意力权重矩阵根据其形状的不同，可以分为 5 类[28]，如图 14 所示：垂直型
（Vertical）、对角线型（Diagonal）、垂直+对角线型（Vertical+Diagonal）、块型（Block）、
混合型（Heterogeneous）。垂直型主要对应于第一列，表示在整个输入序列中，所有
词元对于classtoken这一特殊标记的关注程度远高于其他输入的图像块；对角线型体
现其对于词元本身/前一词元/后一词元的关注程度高；垂直+对角线型是前二者的混
合，说明前两种情况都有出现；块型体现输入中可能分为多个区别比较大的块，并表
现块内注意力；混合型说明注意力的情况较复杂，无法用具体哪一类来表征。
图14 注意力权重矩阵分类图示[28]
图 15 展示了在第一个编码层中 12 个头的注意力权重矩阵。对比输入图片在第
一个编码层的权重矩阵，可以看到大部分头的权重矩阵都存在明显的注意力倾向：头
1、5 为对角线型，说明在这些头的探索中，图像块的注意力更多放在自己本身，因
此在权重矩阵对角线上的数更大；头2、3、6、7、9为垂直+对角线型，说明在这些
头所在的表示子空间中，图像块不止关注到自身，还将很大一部分注意力放在排第一
20
个的 class token 这一特殊标记之中；头 11、12 为明显的垂直型，说明在这些头中每
个图像块，包括classtoken都将几乎最大的注意力放在classtoken上；头4、8、10也
为垂直型但不止一条竖线且相对分散，说明图像块普遍关注某几个图像块。因此可以
初步得出结论：多头机制形成的注意力权重矩阵有各不相同的关注重点，因此让模型
可以关注到图像中不同的特征信息，从而增强了模型的泛化能力。
图15 编码层1中各个头的注意力权重矩阵，从左上到右下依次为head1-head12
然而随着层数的加深，权重矩阵反映出来的注意力倾向越发不明显，逐渐形成关
注度分散的趋势。图 16 选取编码层 1、4、7、10 中头 1 的注意力权重矩阵并从左到
右依次展示，可以看到layer1的权重矩阵还呈现清晰的对角线型，说明其中的图像块
更关注自身；layer4中出现了几条不重合的对角线，且颜色更浅，说明其中的图像块
不只关注于自身，而是做了一些拓展，对位置离自身较近的邻近图像块也施加了一些
关注，随之注意力较浅层更分散；layer7中更加明显，不仅颜色变淡，还出现了垂直
型的特征，说明图像块注意力的分散程度加深了；layer10 中已经几乎观察不到对角
线的特征。由此得出结论，随着模型编码层次的加深，模型的注意力将不再局限于自
身邻近位置或特定的标记（classtoken），而是对整个输入序列都施加了关注，因此最
后的输出能学习到整张图片的特征信息，并据此做出图像的分类。
图16 权重矩阵图示，从左到右依次为layer1、4、7、10
21
5.2 对比分析编码层输出
通过与输入图像、前一编码层的对比，用户能轻松关注到当前序列捕捉到了图片
中哪些独特的特征，如图17所示。
a) 输入图片图示 b) 编码层输出热c) 编码层输出热d) 编码层输出热e) 编码层输出热
力图例1 力图例2 力图例3 力图例4
图17 编码层输出与原图对比
图17-a是图片输入，图17-b、17-c、17-d、17-e都是经过模型编码层处理后输出
的热力图。与原图相比，图17-b捕捉到了更多鸟的轮廓信息，因此在热力图中勾勒出
鸟的形状；图 17-c 关注到鸟的尾巴与翅膀末端的羽毛部分，通过与原图比较可能是
因为这一部分的颜色较图片中的其余部分更深；图 17-d 更关注背景部分的信息，放
在鸟身上的注意力相对来说很少；图 17-e 与图 17-d 正相反，将注意力完全集中到了
鸟身上。由此可见，编码层中的不同部分可以捕捉到图片不同方面的信息，汇总之后
即可以学习到整张图片的全部特征并据此做出分类。
此外，对比不同编码层的输出热力图，可以发现，随着层数加深，模型越来越能
捕捉到图片中的特征，反映在图片上即一张热力图的颜色深浅对比越来越大。
图18 不同编码层的输出热力图对比图示
从图18中可以观察到，当模型处于较浅的编码层，如layer1-6，14×14的热力图
中的元素之间颜色差别相对不大，说明此时矩阵中的数据差值不大，模型在当前的层
次中并没有显著将注意力集中到哪个部分；而随着模型的层次加深，热力图中的颜色
反差越来越大，通过显著的颜色差别大致能勾勒出图形的轮廓。比如在 layer10、11、
22
12 的热力图中，鸟的形状越来越明显，且代表鸟身的色块与背景色块形成强烈的颜
色对比。说明当处于较深的层次时，模型逐渐学习到了输入图片的特征信息，为后续
的分类工作提供了依据。
23
6. 用户调研
本系统旨在帮助初学者更好的理解 Vision Transformer 模型，为进一步评估系统
对用户的协助效果，本文邀请了6位计算机专业本科生进行系统体验，并完成调查问
卷。这 6 位本科生中，有三位了解过其他神经网络，但是对 Vision Transformer 完全
不了解（记为 S1-1、S1-2、S1-3）；另外三位本科生使用过 Vision Transformer，但对
其模型内部具体的运行细节了解不深（记为 S2-1、S2-2、S2-3）。在使用系统前先向
他们介绍了设计本系统的背景和目标、系统各模块的主要功能和使用方法，在介绍完
毕后邀请用户依次自由体验系统，但不能看到之前体验者的操作和评价。在用户使用
系统的过程中，只在旁观察记录用户的行为，不提供帮助也不做出干扰。在6位体验
者都体验完毕后，请他们填写调查问卷。问卷中提出以下几个问题，并尝试通过评分
的方式定量分析用户对系统的满意程度：
1） 通过使用本系统，能否对您理解VisionTransformer有所帮助？
2） 对于系统各个模块和视图，是否有不理解之处？
3） 系统的整体设计、交互效果是否符合您的使用习惯？
4） 如果您曾经使用过其他神经网络的可视化系统，能否分享一下其与本系统有什
么不同？
5） 您认为系统还有哪些方面的不足，有哪些改进建议？
6） 请您按以下几个指标给本系统评分（1-5 分）：易于使用；便于理解；美观程度。
6位用户对系统的评分如下：
表1 用户评分情况
指标 S1-1 S1-2 S1-3 S2-1 S2-2 S2-3
易于使用 5 4 4 5 5 4
便于理解 3 3 4 5 4 5
美观程度 5 5 5 5 5 4
收集6位体验者的调查问卷后，本文整理总结体验者的总体使用体验，并将其分
为3部分：总体框架、可视化视图、改进建议。
24
6.1 总体框架
所有体验者都对系统的整体设计表示满意。S1-2 表示页面设计较为精致，布局
合理，符合他的个人审美和使用习惯；S1-3 表示系统能够突出重点，色彩搭配恰当，
能突出可视化视图引起用户关注；S2-1 认为折叠展开的设计很巧妙，能自然地捕捉
到用户的注意，动画过渡也很流畅；S2-2提出整个系统连接比较紧密，且将模型的核
心结构较好的展示出来，输入图片、编码层热力图、分类结果相互呼应，帮助他更好
的理解了模型的运行全过程。
6.2 可视化视图
总体而言，可视化视图得到了体验者的积极反馈。S2 组体验者普遍认为，通过
系统的可视化视图加深了他们对模型的理解：S2-1 表示通过切换对比不同头的注意
力权重矩阵，他发现不同头的权重矩阵不一致，说明其在各个表示子空间中学习的信
息不同，明白了多头机制的具体功能和目的；S2-3表示注意力权重矩阵与他对注意力
机制的了解相互印证，让他更好的理解了自注意力机制的实现原理。存在 S1 组体验
者对一些视图产生困惑，但总体反馈偏积极：S1-1 表示他不了解 Vision Transformer
这一模型，但是通过观察编码层热力图的变化趋势，他发现模型学习到的信息越来越
符合输入图片的特征，一定程度上说明了模型的学习能力和有效性；S1-2 提出虽然
他在使用系统的过程中发现了注意力权重矩阵主要的几种分布特征，但是他不理解
这些特征所代表的含义；S1-3 关注到输出图片与权重矩阵的交互联动效果，认为这
种高亮的方式能帮助他了解到该图像块在自注意力机制中的关注分布以及其他其他
图像块对它的关注程度。
6.3 改进建议
体验者在使用本系统后指出了当前的一些不足之处。S1-2 表示可以在注意力权
重矩阵中多增加一些说明帮助用户理解自注意力机制，比如增加分布特征的对比图
及其含义、公式等，降低完全不了解VisionTransformer模型的用户的理解门槛；S2-3
提出可以在系统中多增加一些提示，比如当鼠标悬浮在某个视图上时，可以出现一些
详细信息像具体数值、文字说明等，将系统变得更丰富。根据反馈，当前系统确实存
在一定瑕疵，特别是对完全未接触过 Vision Transformer 的初学者还不够友好，需要
25
接下来进一步完善以满足不同类型用户的需求。
26
7. 总结与未来展望
7.1 总结
本文设计并实现了一个面向 Vision Transformer 模型的交互式可视化教学系统，
帮助神经网络领域的初学者理解该模型，使原本陡峭的学习曲线变得平滑。该系统聚
焦于使用 Vision Transformer 完成图像分类任务这一场景，主要分为 4 个模块，分别
是图片选择和编码模块、多头注意力模块、模型逐层输出模块和模型输出分类模块。
图片选择和编码模块支持用户选择想探索的图片作为输入，并查看其分割嵌入情况；
多头注意力模块以权重矩阵的方式将模型的多头机制和注意力机制可视化；模型逐
层输出模块以热力图形式展示每层编码器的输出结果；模型输出分类模块提供将模
型输出与分类结果建立联系的视图。系统的不同模块间存在联动交互效果，彼此紧密
连接，构成合理完善的可视化分析过程。在系统实现后，本项目还通过案例分析和用
户调研证明了该教学系统的有效性，对用户理解VisionTransformer模型有切实帮助。
7.2 未来展望
当前本系统主要以图表形式展示 Vision Transformer 模型运行过程中的可视化，
对有一定基础的用户可以帮助他们加深理解，但是对于完全不了解模型的初学者使
用门槛过高，容易出现发现图表中存在某些特征但不理解其含义的情况。在本文 6.3
节体验者提出了这方面的改进建议，因此增加解释性的文字说明和示例，使用简洁明
了的方式传递更多信息，降低系统的使用门槛，从而更好帮助不同类型的用户学习
VisionTransformer模型，是未来系统优化工作中重要的改进方向。
27
参考文献
[1] LIUS,WANGX,LIUM,etal.TowardsBetterAnalysisofMachineLearningModels:AVisual
AnalyticsPerspective[J].VisualInformatics,2017,1(1):48-56.
[2] SMILKOV D, CARTER S, SCULLEY D, et al. Direct-Manipulation Visualization of Deep Net-
works[C].in:ICMLWorkshoponVisualizationforDeepLearning.2017.
[3] KRIZHEVSKYA,SUTSKEVERI,HINTONGE.ImageNetClassificationwithDeepConvolu-
tionalNeuralNetworks[C].in:AdvancesinNeuralInformationProcessingSystems.2012.
[4] GIRSHICKR, DONAHUE J, DARRELL T, et al. Rich Feature Hierarchies for AccurateObject
Detection and Semantic Segmentation[C]. in: Proceedings of the IEEE conference on computer
visionandpatternrecognition.2014:580-587.
[5] REN S, HE K, GIRSHICK R, et al. Faster R-CNN: Towards Real-Time Object Detection with
Region Proposal Networks[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence,
2017,39(6):1137-1149.
[6] VASWANI A, SHAZEER N, PARMAR N, et al. Attention is All you Need[C]. in: Advances in
NeuralInformationProcessingSystems.2017.
[7] DOSOVITSKIYA,BEYERL,KOLESNIKOVA,etal.AnImageisWorth16x16Words:Trans-
formersforImageRecognitionatScale[J].ArXiv,2020,abs/2010.11929.
[8] YOUNGT,HAZARIKAD,PORIAS,etal.RecentTrendsinDeepLearningBasedNaturalLan-
guageProcessing[J].IEEEComputationalintelligenCemagazine,2018,13(3):55-75.
[9] RADFORDA,NARASIMHANK,SALIMANST,etal.ImprovingLanguageUnderstandingby
GenerativePre-Training[J].2018.
[10] DEVLINJ,CHANGMW,LEEK,etal.BERT:Pre-trainingofDeepBidirectionalTransformers
forLanguageUnderstanding[C].in:NorthAmericanChapteroftheAssociationforComputational
Linguistics.2019.
[11] YANG Z, DAI Z, YANG Y, et al. XLNet: Generalized Autoregressive Pretraining for Language
Understanding[C].in:AdvancesinNeuralInformationProcessingSystems.2019.
[12] BROWNT,MANNB,RYDERN,etal.LanguageModelsareFew-ShotLearners[C].in:Advances
inNeuralInformationProcessingSystems.2020:1877-1901.
[13] OUYANGL,WUJ,JIANGX,etal.TrainingLanguageModelstoFollowInstructionswithHuman
Feedback[C].in:AdvancesinNeuralInformationProcessingSystems.2022:27730-27744.
[14] ALICIOGLU G, SUN B. A Survey of Visual Analytics for Explainable Artificial Intelligence
Methods[J].Computers&Graphics,2022,102:502-520.
[15] MONTAVONG,SAMEKW,MÜLLERKR.MethodsforInterpretingandUnderstandingDeep
NeuralNetworks[J].Digitalsignalprocessing,2018,73:1-15.
[16] WANGJ,LIY,ZHOUZ,etal.When,WhereandHowDoesitFail?ASpatial-TemporalVisual
AnalyticsApproachforInterpretableObjectDetectioninAutonomousDriving[J].IEEETransac-
tionsonVisualizationandComputerGraphics,2022,29(12):5033-5049.
28
[17] SEIFERT C, AAMIR A, BALAGOPALAN A, et al. Visualizations of Deep Neural Networks in
ComputerVision:ASurvey[J].2017:123-144.
[18] WANGZJ,TURKOR,SHAIKHO,etal.CNNExplainer:LearningConvolutionalNeuralNet-
workswithInteractiveVisualization[J].IEEETransactionsonVisualizationandComputerGraph-
ics,2020,27(2):1396-1406.
[19] KAHNG M, THORAT N, CHAU D H, et al. Gan Lab: Understanding Complex Deep Genera-
tive Models Using Interactive VisualExperimentation[J]. IEEE transactions on visualization and
computergraphics,2018,25(1):310-320.
[20] GAO L, SHAO Z, LUO Z, et al. TransforLearn: Interactive Visual Tutorial for the Transformer
Model[J].IEEETransactionsonVisualizationandComputerGraphics,2024,30(1):891-901.
[21] STROBELT H, GEHRMANN S, PFISTER H, et al. LSTMVis: A Tool for Visual Analysis of
HiddenStateDynamicsinRecurrentNeuralNetworks[J].IEEEtransactionsonvisualizationand
computergraphics,2017,24(1):667-676.
[22] WANGJ,GOUL,SHENHW,etal.DQNViz:AVisualAnalyticsApproachtoUnderstandDeep
Q-Networks[J].IEEEtransactionsonvisualizationandcomputergraphics,2018,25(1):288-298.
[23] ŠKRLJ B, ERŽEN N, SHEEHAN S, et al. Attviz: Online exploration of self-attention for trans-
parentneurallanguagemodeling[J].arXivpreprintarXiv:2005.05716,2020,abs/2005.05716.
[24] DEROSEJF,WANGJ,BERGERM.AttentionFlows:AnalyzingandComparingAttentionMech-
anismsinLanguageModels[J].IEEETransactionsonVisualizationandComputerGraphics,2020,
27(2):1160-1170.
[25] WANG Z J, TURKO R, CHAU D H. Dodrio: Exploring Transformer Models with Interactive
Visualization[C].in:Proceedingsofthe59thAnnualMeetingoftheAssociationforComputational
Linguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing:System
Demonstrations.2021:132-141.
[26] BOSTOCK M, OGIEVETSKY V, HEER J. D3 data-driven documents[J]. IEEE transactions on
visualizationandcomputergraphics,2011,17(12):2301-2309.
[27] https://github.com/lukemelas/PyTorch-Pretrained-ViT/tree/master.
[28] KOVALEVAO,ROMANOVA,ROGERSA,etal.RevealingtheDarkSecretsofBERT[J].2019.
29
致谢
流年似水，岁月蹉跎，四年大学时光即将落幕，感谢一直以来帮助我、陪伴我的
所有人。
感谢马昱欣老师和王焕辰学长对我的指导，从项目选题到系统实现再到最后的
论文撰写和修改，给予我无微不至的帮助。
感谢超哥四年以来的关照，在生活的方方面面给我莫大的帮助，给我提供了许多
次宝贵的机会。
感谢父母一直以来的坚定支持和陪伴，这四年我有许多灰心的时刻和不成熟的
选择，家人一直作为我坚强的后盾，给我信心和力量。
感谢朋友们在四年中的陪伴，感谢书，帆，布朗尼·夏，崔宝，龙宝，赵哥，球
队的队友们，412的室友……认识你们是我的幸运。
感谢齐琪，在我面临人生中几乎是最重大挑战的时候给我莫大的支持和鼓励，小
太阳的温暖跨越2900公里。
祝所有人都有个好天气。
30