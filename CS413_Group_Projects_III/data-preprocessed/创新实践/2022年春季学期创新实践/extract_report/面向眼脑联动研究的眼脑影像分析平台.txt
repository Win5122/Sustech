Image Analysis Platform for Eye-Brain Interaction Research: Fundus Vascular
Segmentation and Multi-Parameter Extraction Function
Haowei Kuang, Shiwei Luo, Huaitang Wang
Southern University of Science and Technology
f11910710, 21990005, 11913005 g@mail.sustech.edu.cn
Abstract
In order to beneÔ¨Åt the early diagnosis of Alzheimer‚Äôs
disease and considering the high correlation between the
brain and the eye, we want to develop an image analysis
platform for eye-brain linkage studies. Under the guidance
of doctors in the group, we plan to implement a retinal
vessel segmentation and parameter extraction function with
accurate results, easy to use and fast computation under the
framework of the eye-brain linkage platform, using OCTA
images as the analysis object during this semester.
1. Introduction
In this report, we develop an image analysis platformfor
the study of eye-brain interaction, perform accurate segmen-
tation and multi-parameter analysis of fundus blood Ô¨Çow,
and deeply understand the correlation between fundus blood
vessels and brain structure in AD, so as to assist in the early
diagnosis of AD.
We try to choose histogram equalization method as our
preprocessing method, using OCTA-NET algorithm as the
vascular segmentation algorithm. After testing, our vascular
segmentation algorithm plays a very good role. We also
extracted multiple parameters from the segmented image,
including vascular centerline, total length of vascular, vas-
cular area, explants area, percentage of vascular area, fractal
dimension and so on.. [1]
2. Related Work
2.1. IMED-ROSE dataset
iMED-ROSE is an OCTA dataset from iMED team[2].
It consists of two subsets: Rose- 1 and Rose-2. The subset
ROSE-1 consisted of 117 OCTA images from 39 subjects
(26 patients with disease and the rest healthy controls),
divided into 90 images for training and 27 images for
testing, including superÔ¨Åcial (SVC), deep (DVC), and in-
traretinal vascular plexus. All OCTA scans were captured
by the RTVue XR Avanti SD-OCT system (Optovue, USA)
equipped with AngioVue software at a resolution of 304 √ó
304 pixels. The manual labeling of these vascular networksis scored by imaging experts and clinicians, and their con-
sensus is taken as the basic fact. There are two different
types of container annotations: centerline level annotations
and pixel level annotations. The ROSE-2 subset contains 112
OCT-A images from 112 eyes. Ninety images were trained
and 22 images tested. Each image is adjusted to a grayscale
image of 840 √ó 840 pixels. All visible retinal blood vessels
were manually tracked by experienced clinical ophthalmol-
ogists using software (Mathworks R2018,Natwick).
3. Data Preprocessing
After trying, we chose the histogram mean method as
the pretreatment method of our experiment.The basic idea
of equalization is to try to make the number of pixels in
each gray level equal [1]. That is, there are no large swaths
of similar pixels to improve the contrast of the image. The
results are as follows: Figure 1 shows the result of applying
histogram equalization to the OCTA image processing com-
pared to the original image. Figure 2 shows the distribution
of grayscale values of the rightmost image before and after
processing.
The effect of histogram equalization: to make the image
gray levels span a wider range of gray levels, thus improving
the image contrast.
Another advantage of histogram equalization: no ad-
ditional parameters are needed and the whole process is
automatic.
The disadvantage of histogram equalization: some gray
levels may not be mapped after stretching, resulting in a
grainy image perception.
4. OCTA-Net
We use a split-based coarse-to-Ô¨Åne network, named as
OCTA-Net, for retinal vessel segmentation in OCTA images.
The pipeline of OCTA-Net has two indispensable stages -
coarse stage and Ô¨Åne stage, as illustrated in Figure 3. In
the coarse stage, a split-based coarse segmentation (SCS)
module is used to produce preliminary conÔ¨Ådence maps.
In the Ô¨Åne stage, a split-based reÔ¨Åned segmentation (SRS)
module is used to fuse these vessel conÔ¨Ådence maps to
produce the Ô¨Ånal optimized results.Figure 1. Histogram equalization(The top is the original image, and the
bottom is the pre-processed image)
Figure 2. grey-scale distribution(Grayscale distribution before and after
Histogram equalization)
4.1. Coarse Stage
Since the dataset has both pixel-level and centerline-
level vessel annotations for each en face OCTA image, the
split-based coarse segmentation (SCS) module has a partial
shared encoder and two decoder branches (for pixel-level
and centerline-level vessel segmentation, respectively), to
balance the importance of both pixel-level and centerline-
level vessel information, as illustrated in the coarse stage
of Figure 3.
1) Pixel-Level Vessel Segmentation: Pixel-level vessel
segmentation is a U-shape network including Ô¨Åve encoder
layers and the symmetric decoder layers. A ResNet-style
structure with split attention module, ResNeSt block [4], is
used as the backbone of each encoder-decoder layer.
2) Centerline-Level Vessel Segmentation: Compared with
pixel-level annotation, vessel annotation at centerline level
aims to grade the vessels in regions with poor contrast, more
complex topological structures, and relatively smaller diam-
eters. So the network structure is adjusted, several ResNeSt
blocks followed by an up-sampling layer are appended in
the third encoder layer of the backbone, as the decoder of
the centerline-level vessel segmentation network.4.2. Fine Stage
In order to further recover continuous details of small
vessels, the Ô¨Åne stage is used to adaptively reÔ¨Åne the
vessel prediction results from the coarse stage. A split-
based reÔ¨Åned segmentation (SRS) module is used as the
Ô¨Åne stage. The structure of SRS module is illustrated in
the Ô¨Åne stage of Figure 3. In order to fully integrate pixel-
level and centerline-level vessel information from the SCS
module, the predicted pixel-level and centerline-level vessel
maps and the original (single channel) OCTA image are
Ô¨Årst concatenated as input (total 3 channels) to the SRS
module. In addition, the SRS module will produce adap-
tive propagation coefÔ¨Åcients to reÔ¨Åne the pixel-level and
centerline-level maps respectively. Finally, the reÔ¨Åned pixel-
and centerline-level maps are then merged into a complete
vessel segmentation map, by choosing the larger value from
the two maps at each pixel.
5. Parametric Extraction
Vessels Area, Explant Area, Vessels Percentage Area,
Total Vessels Length, Vessel Center Line, Fractal Dimen-
sion, Vessel Curvature.
5.1. Vessels Area
White part in Figure 4.
5.2. Explant Area
All valid areas in Figure 4.
5.3. Vessels Percentage Area
The ratio of vessels area and explant area.
5.4. Vessel Center Line
Use Rosenfeld reÔ¨Ånement algorithm. Scan all pixels.
If the pixel is a boundary point, but not an outliers and
endpoints, and deletion of the pixel does not change the
eight-connectivity of the surrounding eight pixels, delete the
pixel. Repeat the scan until there are no pixels in the image
that can be deleted. Then get the centerline image, as Figure
5.
5.5. Total Vessels Length
Calculated from the vessel center line.
5.6. Fractal Dimension
Fractal dimension expresses the internal regularity repre-
sentation of the seemingly irregular shape of blood vessels.
Since the box dimension is equal to the fractal dimension,
we calculate the box dimension to get the fractal dimension.Figure 3. Architecture of the proposed OCTA-Net network (with an example of en face of the angioram of SVC+DVC in the ROSE-1 set). The SCS
module (coarse stage) is designed to produce two preliminary conÔ¨Ådence maps that segment pixel-level and centerline-level vessels, respectively. The SRS
module (Ô¨Åne stage) is then introduced as a fusion network to obtain the Ô¨Ånal reÔ¨Åned segmentation results.
Figure 4. Vessels Segmentation Vessels Area(white area)
Figure 5. Vessel Center LineWe place the blood vessels on a evenly divided grid and
calculate the minimum number of cells needed to cover
the blood vessels. By progressively reÔ¨Åning the grid, the
change in the number of coverage required is viewed to
calculate the box dimension. The calculation formula is as
dimbox(S) = lim!0logN()
log (1=)Figure 6.
Figure 6. Fractal Dimension ( represents the side length of the lattice and
N represents the number of cells required to cover the vessels).
5.7. Vessel Curvature
Curvature convolution kernel is used to extract the cur-
vature of segmented vascular images. Firstly, the algorithm
performs open operation on the segmented image, that is
Ô¨Årst corrodes and then expands. Secondly, perform closed
operation, that is Ô¨Årst expands and then corrodes. Finally,
the curvature of the image is calculated according to the
formula asH=2
666664 1
165
16 1
165
16 15
16
 1
165
16 1
163
777775
UFigure 7.Figure 7. Vessel Curvature Formula 6.12 of [7]
6. Experiments
6.1. Experimental Setting
The proposed method was implemented with PyTorch.
Both the coarse and the Ô¨Åne stage were trained with 300
epochs and with the following settings: Adam optimization
with the initial learning rate of 0.0005, batch size of 2
and weight decay of 0.0001. For more stable training, we
adopted poly learning rate policy with a poly power of
0.9. For training and inference of the proposed method, the
ROSE [5] subset was split into 30 images for training and
9 images for testing. Data augmentation was conducted by
randomly rotation of an angle from -10¬∞ to 10¬∞during all
training stages.
6.2. Evaluation Metrics
To achieve comprehensive and objective assessment of
the segmentation performance of the proposed method, the
following metrics are calculated and compared:
Area Under the ROC Curve (AUC)
Sensitivity (SEN)=TP/(TP+FN)
SpeciÔ¨Åcity(SpeciÔ¨Åcity)=TN/(TN+FP)
Accuracy(ACC)=(TP+TN)/(TP+TN+FP+FN)
Kappa score= (Accuracy pe)=(1 pe)
False Discovery Rate(FDR) = FP/(FP + TP)
G-mean score [6]=pSensitivitySpecificity
Dice coefÔ¨Åcient(Dice)=2√óTP/(FP+FN+2√óTP)
where TP is true positive, FP is false positive, TN is true
negative, and FN is false negative. pe in Kappa score repre-
sents opportunity consistency between the ground truth andprediction, and it is denoted as:
pe= ((TP+FN)(TP+FP) + (TN+FP)(TN+
FN))=(TP+TN+FP+FN)2
6.3. Performance Comparison and Analysis
We compared the effects of different pretreatment meth-
ods on the segmentation results on ROSE dataset and eval-
uated the segmentation accuracy of the proposed algorithm.
Then, we also test the portability of the segmentation algo-
rithm on iMED-Huaxi dataset to prove that the algorithm
has good portability and can be used in our platform devel-
opment.
6.4. Segmentation Accuracy Comparisons
In order to compare and obtain the most suitable
preprocessing methods, we carried out four different
preprocessing methods for images, including histogram
equalization method, top hat transformation method,
CLAHE method and gamma transformation method. The
model was trained on the training set and evaluated on the
test set respectively, and the results were as follows.
From Table 1, We can see that through the improvement
of the training method, the results of the model trained by
us are signiÔ¨Åcantly improved compared with the results in
the paper. After comparing and analyzing the results of
four kinds of pretreatment, we Ô¨Ånally choose histogram
averaging method as our pretreatment method.
6.5. Portability Test
Then, we verify the portability of the model on iMED-
Huaxi dataset. We used two types of images of superÔ¨Åcial
vessels and deep vessels to verify the mobility of the model.
Since this dataset was not manually annotated, we evaluated
the segmentation results under the guidance of clinicians.
The following Figure 8. shows the segmentation results of
the original images of superÔ¨Åcial vascular data and deep
vascular data and our model.
After staining observation, we found that our model
performed well on both types of images on the dataset both
in overall and in detail. It is proved that our model has good
transferability.
So we can come to the conclusion. The vascular segmen-
tation algorithm we implemented has been tested and has
good segmentation accuracy and portability, which can be
used in the development of vascular extraction and analysis
platform functions.
7. Platform
We Ô¨Ånally succeeded in building a medical image pro-
cessing platform with easy operation. The platform can
be used for processing OCTA images, allowing for vesselTABLE 1. SEGMENTATION RESULTS OF ORIGINAL IMAGES AND IMAGES OBTAINED BY DIFFERENT PRETREATMENT
METHODS ON ROSE
Method AUC ACC FDR Kappa G-mean Dice
Thesis results 0.945 0.918 0.178 0.720 0.836 0.770
original image 0.949 0.921 0.146 0.728 0.832 0.775
Histogram equalization 0.950 0.922 0.142 0.725 0.828 0.771
Top-Hat 0.933 0.917 0.121 0.697 0.797 0.745
CLAHE 0.950 0.922 0.104 0.716 0.807 0.761
Gamma transform 0.950 0.923 0.120 0.725 0.820 0.770
Figure 8. OCTA image and segmentation results. From left to right: images of shallow vessels, segmentation results of shallow vessels, images of deep
vessels, segmentation results of deep vessels
segmentation, browsing and management of datasets, as well
as manual annotation, multi-parameter extraction. The name
of the platform is ELIA, Figure 9.
Figure 9. ELIA
8. Conclusion
In this semester‚Äôs work, we developed an image analysis
platform for the study of eye-brain linkage. We conducted
repeated model training for the OCTA-NET method of
vessel segmentation and OCTA vessel image extraction,and improved the quality of the model by adjusting the
training method and adding the pretreatment method, thus
realizing the function of vessel segmentation. In addition,
we also tested the segmentation method on multiple data
sets to verify the portability of the method. We have also
developed multi-parameter extraction capabilities for blood
vessels and integrated them into a platform that clinicians
can use easily. The professionalism and accuracy of our
functions and the convenience of our platform have been
recognized by professional doctors.
9. Acknowledgement
In the end, we would like to express our special thanks
to Professor Jin Zhang and Professor Jiang Liu for their
guidance and help throughout our project. In addition, we
would like to thank Senior sister Hanpei Miao, Senior
brother Zhongxi Qiu and Senior Brother Richu Jin for their
great help to our project. Without them, our project would
not have been completed.