图1.1.1
图1.1.2基于超分辨的眼科病灶提取算法  ⼀、前期⼯作回顾 1.1 项⽬重点及模型结构  本项⽬的⽬标是提出⼀套基于深度学习的⾼效轻便的眼科病灶提取算法，研究重点为如何引⼊超分辨技术来缓解由于Resize导致的眼底图像分割精度下降的问题。简略流程如图1.1.1所示
 在前期的⼯作中，我们搭建了超分辨分⽀与分割分⽀的融合模型，并在⾎管分割上初步达到预期效果，后续将继续在渗出分割上进⾏调整，提⾼⽹络在眼科病灶分割上的通⽤性。图1.1.2展示了模型的结构。
图1.2.2
图1.2.31.2 中期⼯作总结  在上⼀次的答辩中，我们主要针对渗出分割的难点，即类别不均衡和病灶位置不固定来调整⽹络。其中针对类别不均衡问题我们进⾏了两种类型的尝试，⼀种是图像预处理，尝试了对图像进⾏Patch，即裁剪成⼩图像后删除没有病灶的⼩图像来减少负样本数量。另⼀种是调整模型损失，尝试将模型中的分割和融合损失替换成CBCE Loss或Focal Loss。在进⾏⼀系列尝试之后，我们发现CBCE Loss能够在⼀定程度上缓解渗出分割类别不均衡问题，因此将模型中的分割和融合损失替换成CBCE Loss。
 图1.2.2和图1.2.3分别展示了改进后的模型的消融实验结果和IDRid数据集上的可视化结果。
图1.3.1
图1.3.21.3 中期问题及解决⽅向  虽然模型改进后类别不均衡问题有所缓解，但我们可以看到，改进后的模型分割结果边界定位不准确，出现外扩问题（如图1.3.1所示）。我们猜测是由于病灶的边界的定位与周围区域存在⼀定的联系，⽽现有的分割损失基于逐像素进⾏计算，忽略了像素间的相关性，⽽未能准确地定位边界。因此中期答辩时我们提出的改进⽅向为引⼊基于区域的损失以最⼤化超分分⽀和分割分⽀的互信息。
 同时，我们在中期答辩时将现有实验数据与SOTA进⾏⽐较（如图1.3.2所示），认为我们的实验结果虽然与SOTA还存在⼀定的差距，但我们的融合模型能够以更⼩的图像作为输⼊，同时达到较⾼分割精度。但是我们之前并没有做验证我们的模型使⽤更⾼分辨率图像作为输⼊能够达到更⾼分割精度的实验，在这次的⼯作中我们尝试设计实验进⾏验证，发现现在的模型backbone在⽬前有限计算资源下⽆法⽀持⾼分辨率图像输⼊进⾏训练，因此我们计划更换为轻量级的backbone进⾏实验。
   ⼆、实验结果图2.1.1
图2.1.2
图2.1.3⼆、实验结果 2.1 基于区域的损失RMI Loss实验  基于上次答辩总结的⽅向，经过调研，我们调研到RMI Loss[1]。不同于⼤多数分割⽹络的损失，如CE Loss，是基于基于单个像素进⾏计算⽽忽略了像素之间的关系，RMI Loss ⽤⼀个像素及其周围的⼋个像素组成的区域来表示该像素，通过最⼤化预测值和ground truth的区域互信息来减⼩两者的差异。其区域表示及损失计算公式如图2.1.1和图2.1.2所示。
 在之前⽹络的基础上（如图2.1.3），我们将融合模块损失调整为RMI Loss进⾏实验，如图2.1.4所示。
图2.1.4
图2.1.5
图2.1.6
 实验结果如图2.1.5所示。我们发现，分割效果在IDRiD数据集上有显著提升，在DDR数据集上不如先前模型。
 我们⾸先就IDRiD数据集上的可视化结果进⾏分析，如图2.1.6所示，我们可以看到，将融合模块的损失换成RMI Loss之后，CBCE Loss过度惩罚负样本的问题在⼀定程度上得到了缓解（即将⿊⾊背景部分预测为病灶，也就是图中的红⾊区域）。
图2.1.7
图2.1.8
图2.1.9
图2.1.10 接着我们尝试分析加⼊RMI Loss后在DDR数据集上效果不如先前模型的原因，我们⾸先对DDR数据集进⾏分析，发现DDR数据集中⽆病灶图像较多，如图2.1.7所示，DDR数据集训练集中总图像有383张，其中138张是没有病灶的。同时我们也发现DDR数据集图像存在⿊⾊边框，如图2.1.8所示。这两个特点使DDR数据集相⽐于IDRiD数据集来说拥有更多负样本，⽽RMI Loss基于区域进⾏计算，在⿊⾊背景上预测值与ground truth易迅速趋于⼀致，导致RMI Loss在训练中迅速降低，影响⽹络对病灶区域的判断。
 因此，我们对DDR数据集中⽆病灶的样本进⾏删除，并裁剪图像⿊边部分进⾏实验，得到的实验结果如图2.1.9所示。我们可以看到，虽然相⽐与没有处理的数据集上进⾏的实验来说，分割效果是有所提升的，但相⽐于先前模型来说，IoU仍然是下降的（图2.1.10）。
 我们进⼀步对处理后的数据集上的分割可视化结果进⾏分析，如图2.1.11所示，我们可以看到CBCE Loss过度惩罚负样本的问题在很⼤程度上得到缓解，但边缘定位仍然不是很准确。
图2.1.11
图2.1.12 我们基于DDR数据集和IDRiD数据集的对⽐进⾏了进⼀步的分析（如图2.1.12所示），发现相⽐于IDRiD数据集来说，DDR数据集图像中病灶区域较⼩，因此在RMI Loss计算以区域为单位的损失时受到周围⿊⾊像素影响更⼤。
 2.2 backbone 调整实验  
调整原因
原始的 Unet 模型加上超分辨分支之后，模型参数量比较大，限制了训练时输入图像的分辨率，项目中输
入图像的分辨率较低，而低分辨率的图像由于图像下采样倍数较高，损失的细节信息较多，在此基础
上，引入超分辨来补充细节信息为分割所带来的提升是比较有限的。因此本项目希望可以进一步提高分
辨率来增加超分辨信息引入所带来的提升。然而，现有模型无法支持更高分辨率的输入图像进行训练，
因此项目对 backbone 的模型大小进行调整，降低模型参数量，从而提高输入图像的分辨率，以验证提
高图像分辨率，能够提高分割的准确性。
调整方式
项目对于 Unet 的调整是将其每一层的特征图的输出通道数减半。
实验结果
图2.2.1 展示了在调整后的轻量 Unet 上，不同输入大小下模型的测试结果，在 IDRID 数据集上，项目将输
入图像的分辨率提升到 720x480 ，可以看出增大图像分辨率后，模型的 IoU 提升比较明显，将近 3 个点。
                      图2.2.1
实验进一步将当前的最高测试结果与 sota 进行比较，如图 2.2.2 所示，当前模型的测试结果与 sota 仍然存
在一定的差距，但是也可以发现，当前模型的输入图像仅为 sota 的一半，由上述实验结果可知，提高输
入图像的分辨率能够提高分割的准确性，因此项目希望在此 720x480 这一分辨率上进一步提高分辨率，
但是在轻量的 Unet 上可以达到的最高输入分辨率依然比较小，因此项目希望将现有的 Unet backbone
更换为更加轻量的 backbone ，从而进一步提高输入图像的分辨率，达到接近 sota 的水平。
                      图2.2.2
因此项目对于轻量的分割 backbone 进行了调研，调研到一篇比较适合引入超分辨的分割网络
STDCNet[2]，图 2.2.3 为该模型的结构图，这个模型轻量且高效，有突出的两个特点，一个是在分割网
络中引入了双分支结构，使用 Context info 以及 Spatial info 两个分支分别提取图像的语义以及空间信
息，并使用一个 FFM 模块来融合这两方面的信息。第二个特点是在 Spatial info 分支上引入了 detail  
head 用来生成一个 Binary_Ground-truth 来学习细节信息。                      图2.2.3
该网络于项目模型有两处类似的地方。一是模型框架相似，都使用了双支路的结构，用来学习不同的图
像信息。二是网络的 Spatial info 分支与项目模型的超分辨分支作用十分类似，都是用来为分割分支补充
细节信息，从而得到精度更高的分割结果。因此，项目认为，在该网络中引入超分辨技术是比较合适
的。项目已经在该网络中进行了 baseline 实验，如图 2.2.4 所示，后续将在如何将超分辨技术引入该网络
方面进行探索与尝试。
                         图 2.2.4
 
三、学期总结  
本学期的工作主要围绕三个重要问题展开，并逐一针对三个问题对模型的结构进行调整。
重要问题
1. 对超分辨与分割的交互类型进行总结，发现在医学影像中两个分支交互区域的特点，分割只关注病
灶区域，而超分辨关注全图。
2. 对渗出数据集的分割难点进行调研，发现图像存在病灶区域只占极小一部分的问题，存在类别不均
衡的问题
3. 发现当前模型的分割准确性受限于训练时输入图像的分辨率，需要验证模型在增大训练时输入分辨
率会，准确性能够提升。
模型调整
1. 针对医学影像中两个分支交互区域的特点，对模型的融合模块进行改进，引入空间注意力机制，提
取两个分支共同关注的区域进行交互。
2. 对损失函数进行调整，引入两种类型的损失。一是使用 CBCE Loss 缓解类别不均衡的问题，二是
使用 RMI Loss 来最大化两个分支的区域互信息，对于边界进行限制，提升边界定位的准确性。3. 减小 backbone 大小，从而增大输入图像的分辨率。进行了两种尝试，一是在原始 Unet 上进行调
整，将每一层特征图的输出通道数减半，提高了输入图像的分辨率，得到更高的分割精度，以 sota
二分之一的输入图像分辨率，达到接近 sota 水平。二是为了进一步提高输入图像分辨率，对于轻量
模型进行调研，最终确定在 STDCNet 上进行研究，已经完成 baseline 实验，后续则探索如何将超
分辨技术引入 STCNet 中。
 
 
