Abstract
Autonomousdrivingisasmartcartechnologythatuses
computersystemstodrivewithoutadriver.Inaneweraof
boomingnewtechnologiesledbyartificialintelligence
andbigdata,thetrendofintelligentdevelopmentinthe
automotivesectorisnoexception.Inordertoobtainmore
robustandaccurateperceptualresults,autonomous
drivingusuallyneedstobeequippedwithdifferentsensors
thatcomplementeachotherindifferentoperating
conditions.Typicalsensormodesinclude:camera,radar,
Lidar,highprecisionmap,etc.Multi-modalsensorfusion,
whichmeansinformationcomplementarity,stabilityand
safety,haslongbeenanimportantpartofautonomous
drivingperception.However,theinsufficientuseof
information,thenoiseoftheoriginaldataandthe
misalignmentbetweenthevarioussensors(suchastime
stampsoutofsync),andotherfactorshaveresultedinthe
limitedfusionperformance.Thissurveycomprehensively
investigatestheadvantagesanddisadvantagesofexisting
sensors,multi-modalautonomousdrivingperception
algorithms,andfocusesonobjectdetectionandsemantic
segmentation.Inthissurveywesummarizemorethan20
literaturesandanalyzetheexistingproblemsinthecurrent
fieldandprovidereferenceforfutureresearchdirections.
1.Introduction
GooglebegantestingitsGoogleautonomousdrivingcar
in2010.Thegoalistodrivevehicleswithoutanyhuman
intervention.Figure1showsitscorearchitecture.Sofar
600,000kmhasbeentestedandalegaltestplatehasbeen
obtained[2].OthercompaniessuchasTesla,Volvo,BMW
arealsomakingdeepresearchintoautonomousdriving
technology,withtherecentpositioningofadvanced
assisteddrivingonhighways.Meanwhile,China'sBaidu,
Didi,Huaweiandothercompanieshavealsoconducteda
lotofresearchonautonomousdriving.
Automobileindustryisaspecialindustryforitinvolves
thesafetyofpassengers.Anyaccidentinthisareais
unacceptable,sotherearealmostharshrequirementsfor
safetyandreliability.Therefore,theaccuracyand
robustnessofsensorsandalgorithmsarehighlyrequiredin
thestudyofautonomousdriving.Ontheotherhand,autonomousdrivingvehiclesareproductsforordinary
consumers,socostsneedtobecontrolled.Inthepast,ithas
beendifficulttoresolvethecontradictionbetween
high-precisionsensorsthatmakealgorithmicresultsmore
accurateandexpensive(suchasLidar)[3].Today,thehigh
accuracybroughtbydeeplearningtechnologypromotes
thedevelopmentofautonomousvehiclesystemsin
multiplecoreareassuchasobjectdetection,semantic
segmentation,decisionmakingandsensorapplication.
Deeplearningtechnologies,typicalofwhichare
convolutionalneuralnetwork(CNN)[7],arewidelyused
invariouskindsofimageprocessing,andarehighly
applicabletothefieldofautonomousdriving.
Therealizationofdeeplearningputsforwardhigher
requirementsforsensortechnology,whichrequires
multi-sensorfusiontechnology.Inthedesignofsoftware
andhardwarearchitectureofautonomousvehicles,
sensors,asthesourceofdatainformation,areofgreat
importance.However,combinedwiththeapplicationand
implementationofdeeplearning,nomatterwhatkindof
sensorhasitsownadvantagesanddisadvantages.For
example,therangeinformationobtainedbyLidarisvery
accurate,buttherearesomeproblemssuchaslackof
texture,littlefeatureinformationandmuchnoise,whichis
notconducivetotheapplicationofdeeplearning.Andthe
featuresofthecameraareexactlytheoppositeoftheradar.
ThefusionofsensorslikeLidarandcameraisofgreat
significancefortheaccurateperceptionandcognitionof
autonomousdriving[2].AReviewonAutonomousDriving:
Multi-sensorObjectDetectionandSemanticSegmentation
RunzheJiang ZhuohangZhuang
SUSTECH SUSTECH
11912511@mail.sustech.edu.cn11912528@mail.sustech.edu.cn
Figure1
2.Background
2.1.VisualandThermalCameras
Imagestakenbyvisualandthermalcamerasprovide
detailedtexturalinformationaboutthevehicle's
surroundings.Thefollowersofvisualsolutionsbelieve
thathumanscanbecomequalifieddriversthroughvisual
informationandbrainprocessing.Camera+deeplearning
neuralnetwork+computerhardwarecanalsoachieve
similareffects.Visualcamerasareverysensitivetolight
andweatherconditions.Thermalimagers,ontheother
hand,aresensitivetoday-nightchangesbecausethey
detectinfraredradiation,whichisrelatedtoanobject's
heat[10].Bymeansofinfraredopticalsystemthatcanpass
infraredradiation,theinfraredradiationofthesceneinthe
fieldofviewisfocusedonthedevicethatcanconvert
infraredradiationenergyintophysicalquantitiesthatcan
beeasilymeasured--infrareddetector[10].Theinfrared
detectorthenconvertstheradiationsignalsofvarying
strengthintocorrespondingelectricalsignals.After
amplificationandvideoprocessing,videoimagescanbe
formedforhumaneyestoobserve.However,neithertype
ofcameraprovidesdepthinformationdirectly..
2.2.LiDARs
LiDARs:whichisalsonamedLaserRadar.Itisaradar
systemwhichtransmitslaserbeamtodetectthecharacteristicquantityoftargetsuchasspeedandposition.
Itworksbyfiringalaserbeam,whichisadetectionsignal,
atatargetandthencomparingthetransmittedsignaltothe
signalthatisreflectedbackfromthetarget.Whenthe
beamhitsanobject,itbouncesbackandissensedand
recordedbyasensor.Then,thesystemwillcalculatethe
distanceofthebeamtotheobjectandback.Afteraseries
ofprocessing,wecanobtaintherelevantinformationof
thetarget,suchasitsorientation,speed,attitude,distance
andshapeparameters.Andwecanthencreatea3Dmapof
theareaaroundthevehicletoenablesafenavigationofthe
vehicle.Laserradaraccuracycanreachmmlevel,suchasa
pedestrianreflectionpointcanreachtensofthousandsof
pointsorevenmore,thusgivingclear3D
three-dimensionalgraphics.Moreover,thelaserradarhas
alongdetectionrangeanditsangularresolutionisseveral
gradeshigherthanmillimeterwaveradar,easilyreaching
0.1Â°[11].Thatistosay,ifthepowerisenough,itcan
distinguishtwosmalltargetswithadistanceof3KMfora
5Mgap[6].FlashLiDARwasdevelopedtoproduce
detailedobjectinformationsimilartocameraimages.Of
course,theFMCWLidarcanprovidespeedinformation.
2.3.Radars
Radars(radiodetectionandranging)transmitsradio
wavesthatbounceoffobstacles,measuresthesignal's
traveltime,andestimatestheobject'sradialvelocity
throughtheDopplereffect.Theyarehighlyadaptabletoall
kindsoflightandweatherconditions,butitisvery
challengingtoclassifyobjectsbyradarduetoitslow
resolution.Radarisoftenusedinadaptivecruisecontrol
(ACC)andtrafficjamassistancesystems[11].In1999,
Distronic(DTR)radarcontrolsystemwasinstalledonthe
Mercedes-Benz220SeriesS-Classsedan,providingbasic
ACCfunctionbycontrollingthedistancebetweenthecar
andthecarinfrontofitat40kmto160kmperhour[8].At
present,theradarofmajorinternationalmanufacturershas
developedtothe5thgeneration,basicallymaintainingthe
Figure2(Acomplexurbanscenarioforautonomous
driving.Thedriverlesscarusesmulti-modalsignals
forperception,suchasRGBcameraimages,LiDAR
points,Radarpoints,andmapinformation.Itneedsto
perceiveallrelevanttrafficparticipantsandobjects
accurately,robustly,andinreal-time.Forclarity,only
theboundingboxesandclassificationscoresforsome
objectsaredrawnintheimage.TheRGBimageis
adaptedfrom[4].)
Figure3
updatespeedof2to3years.Atthesametime,domestic
millimeterwaveradarmanufacturershavealsohadabout5
yearsofdevelopment,theheadcompany'smass
productionproductshavebeenclosetotheinternational
advancedlevel.Althoughtheaccuracyanddetectionrange
havebeengreatlyimproved,butoverallitsdevelopment
stagehasnotyetachievedessentialbreakthrough.
2.4.Ultrasonics
Ultrasonicradarusesultrasonicwavestodetectobstacles
ahead.Theultrasonicgeneratoremitsultrasonicsound
wavesinacertaindirection.Whenthesoundwavesare
propagatedintheair,theymeetobstaclesandreturntothe
originalpath.Theultrasonicreceiverreceivestheechoand
stopsthetiming.Thedistancebetweenthesensorandthe
obstaclecanbecalculatedaccordingtothespeedandtime
differenceoftheultrasonicwaveintheair.Ultrasonic
radarisanimportantsensorforparkingfunctionduetoits
lowprice(afewyuanto10yuan)andshortdetection
distance.Inaddition,theultrasonicradar(APA)onthe
sideofthecarcanalsoactonthelateraldrivingauxiliary
alarmfunction[6].Ultrasonicradartheoreticallyworkson
allobjectsthatcanreflectultrasonicwaves,including
solidsandliquids,butitsabilitytodetectthemis
compromisedifthetargetisanangledplanethatcan
reflectsoundwavestootherangles,oranobjectsuchasa
spongeorfoamthatcanabsorbultrasonicwaves.
Therefore,theyareusuallyusedincloseobjectdetection
andlowspeedscenarios.
2.5.GNSSandHDMaps
GNSSistheabbreviationof"GlobalNavigation
SatelliteSystem".Itreferstoallsatellitenavigation
systemsingeneral,includingglobal,regionaland
enhanced,suchastheUSGPS,Russia'sGlonass,Europe's
Galileo,China'sBeidounavigationSatelliteSystem[4].
Alsoitrelatedenhancedsystems,suchasWAAS(Wide
AreaAugmentationSystem)intheUnitedStates,EGNOS
(EuropeanStaticNavigationOverlappingSystem)in
EuropeandMSAS(MultifunctionalTransportation
SatelliteAugmentationSystem)[4]inJapan,andother
navigationsatellitesystemsunderconstructionortobe
builtinthefuture.TheinternationalGNSSsystemisa
complexcompositesystemwithmultiplesystems,layers
andmodes.Inotherwords,itisalargesystemcomposed
ofmultiplesatellitenavigationandpositioningsystems
andtheirenhancedsystems.Asaresult,GNSSpositioning
accuracyismuchhigherthancivilianGPS,buttheerroris
stillseveralmeters.Toachievesafeautonomousdriving,
ensureahighprecisionGNSSisthekey.Originally
introducedasanavigationtoolfordrivingassistancein
cars,GNSSisnowalsousedinconjunctionwith
high-definitionmapsforautonomousvehiclepath
planningandpersonalvehiclelocation.Atthepresentstage,GNSSpositioningaloneisstillsomedistancefrom
fullymeetingthepositionaccuracyrequirementsof
automaticdriving.Accordingtoanalysis,autonomous
drivingonexpresswaysrequiresaminimumerrorof1.5
meters,whileautonomousdrivinginurbanareasrequiresa
centimeter-levelaccuracy.Inaddition,itiseasytolose
signalinnavigationwhenenteringtunnelsorbuilt-up
metropolitanareas,whichiswhyitisdifficulttouseGNSS
alonetolocate.
2.6.InertialMeasurementUnitandOdometer
TheInertialMeasurementUnit(IMU)andodometer
provideinternalinformationaboutthevehicle.Itcanbe
usedasaneffectivesupplementwhensensordatais
missing.TheIMUmeasuresthree-dimensionallinear
accelerationandthree-dimensionalangularvelocity,and
fromthisinformation,thevehicle'sattitude(pitchandroll
Angle),heading,speedandpositionchangecanbe
calculated.IMUscanbeusedtofillinthegapsbetween
GNSSsignalupdatesandevendeadreckoningifthe
GNSSandothersensorsinthesystemfail.Thekey
advantageoftheIMUisthatitworkswellinallweather
andgeographicalconditions.Asanindependentdata
source,itcanbeusedforshort-termnavigationandto
verifyinformationfromothersensors,andisnot
invalidatedbyweather,lensdirt,radarandlidarsignal
reflections,ortheurbancanyoneffect.Asanindependent
sensor,IMUisregardedasthesensorthatsupplementsand
confirmsthedataofothersensors,namelythe"lastsensor",
whichisusedtoensurethesafetyofthevehicleandstop
thevehicleinacontrolledmannerwhenothersensorsare
damagedordisabled.Therefore,wecallIMUtheanchorof
theautonomousdrivingsystem[2].
2.7.ObjectDetection
Objectdetectionisanimportanttaskincomputervision,Figure4
whichdevelopscomputationalmodelsforaspecificclass
ofvisualobjects(suchashumans,animalsorcars).Ifwe
thinkofobjectdetectiontodayasatechnologicalaesthetic
underdeeplearning,thengoback20yearsandwewill
witnessthe"wisdomoftheageofcoldweapons".Mostof
theearlytargetdetectionalgorithmswerebuiltbasedon
manualfeatures.Duetothelackofeffectiveimage
representationatthetime,peoplehadnochoicebutto
designcomplexfeaturesandutilizevariousacceleration
techniquestopklimitedcomputingresources.Thereare
threemainalgorithms:ViolaJonesdetector,HOG
detectionandDPMalgorithm[1].
Currently,mostoftheframeworksusedinobject
detectionarebasedonconvolutionalneuralnetworks
(CNN)[14],whichcaneffectivelyextractimagefeatures.
Becauseofdifferenttaskrequirements,objectdetectionis
dividedintotwocategories:objectdetectionalgorithmof
twostagesandobjectdetectionalgorithmofonestage.The
targetdetectionintheTwostagesistofirstgeneratea
seriesofcandidateboxesassamplesbytheheuristic
candidateregiongenerationalgorithm,andthenclassify
samplesbyconvolutionalneuralnetwork.Inonestage,the
problemoftargetframelocationisdirectlytransformed
intoaregressionproblemwithoutgeneratingcandidate
boxes.Comparedwiththetwo,eachhasitsadvantagesand
disadvantages.Thetargetdetectionalgorithmofthetwo
stageshashigherdetectionaccuracyandpositioning
accuracy,whichissuitablefortasksrequiring
high-precisionidentification.However,thetarget
detectionalgorithmofonestageisfasterandsuitablefor
real-timerecognitiontasks[7].Thetargetdetection
algorithmoftheTwostagesismainlyrepresentedbythe
RCNNseries.Themainideaistogenerateareaproposal
andseparatethetargetclassificationdetection,soasto
obtainmoreaccuratedetectioneffect.Therearealso
SPP-Netalgorithm,FastR-CNNalgorithmandFeature
PyramidNetworks[13].
AlthoughFasterR-CNN,thepeakofthetwostages,has
madegreatprogressincomputingspeed,itstillcannot
meettherequirementsofreal-timedetection.Therefore,
researchersarethinkingabouthowtoachievereal-time
targetdetection.Soamethodbasedonregressionhasbeenproposedtodirectlyreturnthelocationandtypeofthe
targetobjectfromthepicture.Thetargetdetection
algorithmofonestagecanrealizethesharingfeatureofa
completesingletraining,andthespeedisgreatlyimproved
underthepremiseofensuringacertainaccuracy.Two
typicalalgorithmsareYOLOandSSD[12].
Targetdetectioncanprovideautomaticdrivingwith
trafficsignrecognition,pedestrianandvehiclerecognition,
trafficsignalrecognitionandotherbasicfunctions.
Currently,vehiclescapableofautonomousdriving
generallyrelyonultrasonicsensorsinthefrontandrear
sidesofthevehicle,millimeter-waveradarinthecenterof
thefront,andforward-lookingcamerasundertherearview
mirrorstoachieveautonomousdriving.Frontsideandrear
sideultrasonicsensorsareusedtodetectclosedistance
obstacles,helpautomaticparking,etc.Millimeterwave
radarisusedtodetectdistantobstacles.The
forward-lookingcameracancompletethedetectionoflane
linesandobstaclesontheroadsurface.
2.8.SemanticSegmentation
Inrecentyears,withtherapiddevelopmentofdeep
learningtechnology,greatbreakthroughshavebeenmade
inmanytasksthataredifficulttobesolvedbytraditional
methodsinthefieldofcomputervision.Especiallyinthe
fieldofimagesemanticsegmentation,deeplearning
technologyplaysaparticularlyprominentrole.Image
semanticsegmentationisafundamentalandchallenging
taskincomputervision.Itsgoalistoassigncorresponding
semanticlabelstoeachpixelintheimage.Theresultisto
divideagivenimageintoseveralvisuallymeaningfulor
interestingareas,whichisconducivetothesubsequent
imageanalysisandvisualunderstanding.
Imagesegmentationtaskrequireseachpixelofthe
originalimagetopredictitscategorypixelbypixel.Since
itisnecessarytopredictthecategoryoftheobjectpixelby
pixel,thisposesgreatchallengestothedeeplearning
model.Forexample,someobjectsaresmallinsizeand
difficulttoidentify,andsomeobjectsaremostlyblocked,
Figure5Figure6(TheFasterR-CNNobjectdetectionnetwork.It
consistsofthreeparts:apre-processingnetworkto
extracthighlevelimagefeatures,aRegionProposal
Network(RPN)thatproducesregionproposals,anda
Faster-RCNNheadwhichfine-tuneseachregion
proposal.)[1]
resultinginreducedrecognition.
Althoughsemanticsegmentationwasfirstintroducedto
processcameraimages,manymethodshavebeen
proposedtosegmentlidarpoints.Anumberofdatasetsfor
semanticsegmentationhavebeenreleasedsuchas
Cityscape,KITTI,TorontoCity,MapillaryVistasand
ApolloScape[11].Thesedatasetsadvancedeeplearning
researchonsemanticsegmentationinautonomousdriving.
Semanticsegmentationcanalsobeclassifiedastwo
stagesandonestage.Intwostagepipelines,domain
proposalsarefirstgeneratedandthenfine-tuned,mainly
forinstancelevelsegmentation(e.g.R-CNN,SDS,
Mask-RCNN[1]).Forsemanticsegmentation,amore
commonapproachisthesingle-stagepipelinebasedonthe
fullconvolutionalnetworkoriginallyproposedbyLonget
al.Inthiswork,thefulljoinlayerintheCNNclassifier
usedtopredictclassificationscoresisreplacedbya
convolutionallayertoproducearoughoutputmap.These
mapsarethenup-sampledtodensepixellabelsbyreverse
convolution(i.e.deconvolution).Kendalletal.extended
FCNbyintroducingencod-decoderCNNarchitecture[13].
Thepurposeoftheencoderistogeneratealayeredimage
representationthroughtheCNNbackbone(removingthe
fullyconnectedlayer).Instead,thedecoderrestoresthese
low-dimensionalfeaturestotheiroriginalresolution
throughasetofup-samplingandconvolutionlayers.The
recoveredfeaturemapisfinallyusedforpixellabel
prediction.
3.Datasets
Asthemethodsofmulti-sensorobjectdetectionand
semanticsegmentationaremostlybasedonsupervised
learning,thequalityofdatasetsbecomesquitesignificant
togoodperceptionabilityinautonomousdriving.Insteadofthedatasetsthatonlyincludecameraimagesforother
traditionaltasksincomputervisionfield,trainingneural
networksinthiscaserequireslargequantitiesof
multi-modaldatasetswithaccuratelabels.However,to
collectsuchmulti-modaldatawillnotonlyfacedifficulties
inbothhardwareandsoftwaretechnology,butalsorequire
agreatdealoftimeandmoney.Thefollowingareseveral
aspectsofdatasetsthatshouldbetakenintoconsideration.
Eachofthemcanhaveadramaticinfluenceonthe
performance,thusindicatingthepotentialmethodto
improvetheperformance.
Pleaseusefootnotesiiisparingly.Indeed,trytoavoid
footnotesaltogetherandincludenecessaryperipheral
observationsinthetext(withinparentheses,ifyouprefer,
asinthissentence).Ifyouwishtouseafootnote,placeitat
thebottomofthecolumnonthepageonwhichitis
referenced.UseTimes8-pointtype,single-spaced.
3.1.Sensingmodalities
Intermsofsensingmodalities,themainstreammethods
mainlyfuseRGBimagestakenbyvisioncameraeither
withthermalimagesorwith3Dpointclouds.Therefore,
threemostcommontypesofdataincurrentdatasetsare
RGBimages,thermalimagesandLiDARpointclouds.For
example,KITTI[14][15],awidelyuseddatasetforvision
recognitionsystemprovidesabenchmarkdevelopedby
theirautonomousdrivingplatformincludingRGBcamera
imagesrecordedbyfourhighsolutionstereocamerasand
LiDARpointcloudsrecordedbyalaserscanner.An
GPS/IMUinertialnavigationsystemisalsoinvolvedto
providethepathandpositioninformationofthecar[15].
Haetal.[16]notonlymadecontributionstosemantic
segmentationbyproducinganewCNNarchitecture
namedMulti-spectralFusionNetworks(MFNet),butalso
creatingagroundbreakingpublicRGB-thermalimage
datasetfortraininginrelativemethods.KAIST
multi-spectraldatasetprovidedbothLiDARcloudpoints
andthermalimageswiththeirself-developedmulti-sensor
platform[17].Forall-dayvisionofautonomousdriving,
creatingadatasetinvolvingseveralsensingmodalitiesis
quitechallengingbecausemanysensorsneedtobealigned.
Tosolvethisproblem,theyproposedacalibration
techniquesothattheirautonomousdrivingplatformcan
supporttheuseofaco-alignedgroupofsensorsincluding
astereovisioncamera,anRGB-thermalcamera,a3D
LiDARandGPS/IMUsensorgroups,thusproviding
severalcombinationsofmodalities.
Figure8belowdemonstratesclearlydifferentsensing
modalitiesavailableinKAISTmulti-spectraldataset:
calibratedRGB-stereoimagepairs(the1stcolumn),3D
LiDARdata(the3rdcolumn),thethermalimages
co-alignedwiththeleft-viewRGBimageusingthe
beam-splitter(the4thcolumn)[18].Whatâsmore,the
chaoticbottomimageofthe2ndcolumnhasshownustheFigure7
weaknessofRGB-stereoimagesinprovidinginformation
atnightwhenlightconditionispoor.Tofulfill
unsuperviseddepthestimationforall-dayvision,Kimet
al.[18]proposedthemulti-taskframeworknamedthe
Multi-spectralTransferNetwork(MTN)togeneratedepth
imagesfromasinglethermalimagethatarenotsensitive
tothelightcondition.Thisindicatethesignificanceof
multi-modalfusionthatitcanleveragecomplementary
advantagesofvarioussensingmodalitiestoimprovethe
performanceofthedeepneuralnetworksinobject
detectionandsemanticsegmentation.
3.2.Environmentalconditions
Asanautonomousdrivingsystemislikelytoconfront
alltypesofcomplexenvironmentalconditionsandis
expectedtodealwiththemcorrectlyeverytimetoensure
theabsoluteroadsecurity,anidealmulti-modaldataset
usedfortrainingshouldcovervariousenvironmental
conditionsaswidelyaspossible,includingdifferent
weatherconditionssuchasrainstorm,snowstorm,thick
fog,differenttimeperiodsofthedaysuchasearlymorning,
noonandlatenight.Tofurtherimprovetheaccuracyand
stabilityofobjectdetection,thedatasetsshouldalsobe
recordedindifferentlocationstoenhanceitsdiversity.
Nevertheless,manydatasetsarelessefficientfortheirlack
indiversityofenvironmentalconditions.Althoughthe
wellknowndatasetinautonomousdriving,KITTI[14][15],
arewidelyrecognizedinthisfield,ithasgreatlimitations
asthewholedatasetisrecordedinasinglecity,Karlsruhe,
Germany.Someotherdatasetshavetakethediversityof
environmentalconditionsintoconsideration.Forinstance,
nearly20millionimagesoftheOxfordRobotCarDataset
arecollectedbytheOxfordRobotCarplatformwithits6
visualcameras,LiDAR,GPSandinertialsensorswhenit
travelalongthesamerouteinOxfordtwiceaweekfrom
May2014toDecember2015[20].Figure9[20]shows9of
over100imagestakenbytherobotcaratthesamelocation
oftheroute,illustratinggreatdifferencesbetweenthe
appearancesoveraseriesofconditions.Thegroupofthe
imagesvaryfromoveralllightingconditiontoseasonal
variations,allthesechangesproduceaprofusionof
differentcombinationofenvironmentalconditionstothis
datasetfortraining.3.3.Datasetsize
Duetothecomplexityofthemulti-modaldatasetsand
thedifficultiesincreatinganewone,datasetsfor
multi-sensorobjectdetectionandsemanticsegmentation
arerelativelysmallerandfewer.Thewidelyused
KITTI[14][15]onlyhasnearly7000framesfortraining,
whileKAISTprovidesnearly100Kframesand100K
objectsinthedatasets[17].Someotherdatasetsinclude
continuousvideoimageframesprovidingsimilar
informationthatarelessusefulfortraining.Wecansee
thatadvancedresearchesinthisfieldarestillingreatneed
oflarge-scaleandhigh-qualitydatasets.
3.4.Labels
Labelsaregiveninmostdatasetsusedfortrainingin
objectdetectionandsemanticsegmentationtasks.
Dependingonthepurposesthatthedatasetsare
constructedfor,indifferentdatasets,eventhesameobject
canbelabeledindifferentways.Forexample,KAIST
multi-spectralpedestriandatasetfocusesonrecognizing
people,thusonlyincludinglabelsfordistinguishable
singlepersonandnon-distinguishableindividualsand
cyclists[17].KITTIprovidesdifferentlabelsfor
pedestriansandthesittingpeople,itcanalsodistinguish
differentvehicleslikecars,trucksandvans[14][15].One
potentialproblemisthatinmostofthedatasets,vehicles
andpeoplearedominantinlabeledobjects,whichmight
weakentheabilityofthetraineddeepneuralnetworksto
recognizeotherobjectslikeroadsignsortrees.
Figure8[18]Differentsensingmodalities
inKAISTdatasetsFigure9[20]Someoftheimagestakenbythe
OxfordRobotCarplatformatthesamelocation
underdifferentenvironmentalconditions
4.Methods
4.1.Processingpointclouds
WhenfusingLiDARpointcloudswithRGBimagesin
multi-sensorobjectdetectionandsemanticsegmentation
tasks,oneofthekeypointsisthatbesidesthevisual
informationprovidedbyRGBimages,3Dobjectdetection
alsoneedsaccuratedepthinformationprovidedbyLiDAR
pointcloudstoestimatethedistancebetweenthevehicle
andtheobject.Sothereareseveralresearches
concentratingonavarietyofmethodstoproperlyrepresent
andprocessthepointclouds,sothatthedeepneural
networkscanlearnthefeaturesasaccurateaspossible.
4.1.13Dvoxelization
Onemethodistodividethe3Dspaceintolittle3D
voxels,wheretheconceptofavoxelinthe3Dspacecanbe
likenedtoa2Dpixelofa2Dimage,sothe3Dpointcloud
canbeassignedtothevoxelsrespectively.Theadvantage
ofthismethodisthatvoxelizationcanretainrichspatial
informationofthe3Dobject.Buttherearemany
remainingemptyvoxelsbecausethepointcloudsare
usuallysparse,makingthecorresponding3D-CNNand
clusteringmethodsquitetime-consumingwhenprocessing
suchdata,whichisanunacceptablecaseinreal-time
autonomousdriving.TohandlesparseLiDARpointclouds,
Zhouetal.[21]proposedVoxelNet,auniversal3D
detectionend-to-endtrainabledeepneuralnetwork.They
introducedaVoxelFeatureEncoding(VFE)layer,as
shownintheFigure10,toprocessthepointcloud
efficiently.Byconductingexperimentsonwidelyused
KITTIdataset,theVoxelNethasshownoutstanding
performanceamongthestate-of-the-artmethodsbasedon
3DLiDARpointcloudswithaninferencetimeofonly
225ms.
4.1.2Directlearningoverpoints
Insteadofvoxelization,somerelatedworkbasedon3D
LiDARdirectlyconductsdeeplearningonpointsets.
PointNet++isabletolearnindividualfeaturesfromeach
pointintheLiDARpointcloudsandusemaxpoolingto
aggregatefeaturesofasetofpoints[22].Itleveragesthepointsinneighborhoodwithgroupingatdifferentscalesto
makethemethodrobustandaccurateindetail.
4.1.3Projectionto2Dfeaturemaps
Anotherwaytoprocessthe3Dpointcloudistoutilize
theideaofprocessing2Dimages.Thatis,wecanproject
theLiDARpointcloudtovarious2Dfeaturemaps,and
thenleverage2Dconvolutionallayerstoprocessthem.
Figure4displaystheRGBimageandseveraltypesof2D
featuremaps,suchassphericalmap,camera-planemap
andBirdâsEyeView(BEV)map,ofastreetscene[1].The
sphericalmapcanrepresentthepointcloudinaideally
densewaybyprojectingthepointsonasphere.However,
withoutpreprocessing,suchsphericalmapisdifficultto
alignwithRGBcameraimagesformulti-modalfusing.
Thoughthecamera-planemapcandirectlybeusedfor
fusing,emptypixelsexistinthesparsemap,thusrequiring
up-samplingmethodstofillthesepixelsandturnitintoa
densemapforfurtherprocessing.TheBEVmapisanother
choicethatkeepthedistributionoftheobjectsonthe
groundplane,includingtheirlengths,widthsanddistances
betweeneachother.
4.2.Processingcameraimages
Mostcurrentmethodsofmulti-sensorobjectdetection
andsemanticsegmentationtakeRGBimagesfromvision
cameraorinfraredimagesfromthermalcameraasoneof
thesensingmodalities.Theyincluderichvisual
informationdemonstratingthesurroundings.Thermal
cameraisparticularlygoodatcapturingimageswhen
lightingconditionislimited.However,oneproblemisthat
foranobjectinancameraimage,theshapecanbe
distortedandthesizecanvaryonthecameraplane,thus
affectingobjectdetection.Tosolvethisproblem,Roddick
etal.[23]developanOrthographicFeatureTransform
(OFT)algorithmtoprojectthecameraimagestoaBEV
representationthatiscommonlyusedforprocessinga
pointcloud,whichlargelypreservesthefeaturesofthe
objects.
Figure11[1]Different2DLiDARrepresentations
Figure10[21]Voxelfeatureencoding(VFE)layer
4.3.Fusingpointcloudsandcameraimages
MostofrelatedworksfuseLiDARpointcloudsand
cameraimagesonthenetworklevel.Thatis,theyproject
thepointcloudsto2Dfeaturemapsandusethe
convolutionalneuralnetworkstoprocessthemaswe
describedinPartIII-1-3).Thesemethodsfusetwo
modalitiesbyfusingthefeaturesobtainedfromthe
networks.Somemethodsconductclusteringdirectlyon3D
pointcloudstogeneratelabelsforregionproposals.
Severalworksalsoleveragetheideaofprojectiontoalign
featuresacrossdifferentsensingmodalities.Forexample,
projectingRGBimagesontoBEVplanewhenthepoint
cloudisrepresentedasaBEVmap.
5.Challenges
5.1.Datadiversity
Idealdatasetswithgreatdiversityinenvironmental
conditionsandobjectlabelswithseveralsensing
modalitiesareneededtoimprovetheaccuracyand
robustnessofmulti-sensorobjectdetectionandsemantic
segmentation.Sofar,availabledatasetsarestillsmallin
scaleandfewinnumber.
5.2.Dataquality
Eventhedatalabeledmanuallymightincludesome
errorsinlabels.Theseerrorsmightlowertheaccuracyof
objectdetectionwhenthelabeleddataareusedfortraining.
Whatâsmore,differentsensorsthatarenotcalibrated
accuratelycanhavetemporalandspatialdeviations,which
mightproducecriticalerrorsindatasets.Furtherresearches
areneededforsensorcalibrationtoimprovethequalityof
thedatasetsandtheefficiencyofcalibration.
5.3.Sensorredundancy
Currentworksmainlyfocusonfusingdifferentsensing
modalities,butfewconductresearchonfusingdata
recordedbyseveralsensorsofthesametype.Suchsensor
redundancycannotonlycollectmoreinformationfromthe
surroundings,butalsoimprovesafetyofautonomous
driving.
5.4.Networkarchitecture
Mostoftheworksuseconvolutionalneuralnetworks
forreal-timeperceptionwithoutconsideringprevious
states.Inthefuture,newnetworkarchitecturesthatcan
processtimeseriesmightbeintroducedtomulti-sensor
objectdetectionandsemanticsegmentationtask.
References
[1]FengD,Haase-SchutzC,RosenbaumL,etal.Deep
Multi-ModalObjectDetectionandSemanticSegmentation
forAutonomousDriving:Datasets,Methods,andChallenges[J].IEEETransactionsonIntelligent
TransportationSystems,2020,PP(99):1-20.
[2]WangZ,WuY,NiuQ.Multi-SensorFusionin
AutomatedDriving:ASurvey[J].IEEEAccess,2020,
8:2847-2868.
[3]J.Kim,J.Koh,Y.Kim,J.Choi,Y.Hwang,andJ.W.Choi,
âRobustdeepmulti-modallearningbasedongated
informationfusionnetwork,âarXiv:1807.06233[cs.CV],
2018
[4]G.Neuhold,T.Ollmann,S.R.Bulo,andP.Kontschieder,
âThe`MapillaryVistasdatasetforsemanticunderstanding
ofstreetscenes,âinProc.IEEEConf.ComputerVision,Oct.
2017,pp.5000â5009.
[5]Y.Yan,Y.Mao,andB.Li,âSecond:Sparselyembedded
convolutionaldetection,âSensors,vol.18,no.10,p.3337,
2018.
[6]C.Urmsonetal.,âAutonomousdrivinginurban
environments:Bossandtheurbanchallenge,âJ.Field
Robotics,vol.25,no.8,pp.425â466,2008.
[7]B.XuandZ.Chen,âMulti-levelfusionbased3Dobject
detectionfrommonocularimages,âinProc.IEEEConf.
ComputerVisionandPatternRecognition,2018,pp.
2345â2353.
[8]S.Gupta,R.Girshick,P.Arbelaez,andJ.Malik,âLearning
richfeaturesÂ´fromRGB-Dimagesforobjectdetectionand
segmentation,âinProc.Eur.Conf.ComputerVision.
Springer,2014,pp.345â360.
[9]K.Bengler,K.Dietmayer,B.Farber,M.Maurer,C.Stiller,
andH.Winner,âThreedecadesofdriverassistancesystems:
Reviewandfutureperspectives,âIEEEIntell.Transp.Syst.
Mag.,vol.6,no.4,pp.6â22,2014.
[10]E.Arnold,O.Y.Al-Jarrah,M.Dianati,S.Fallah,D.Oxtoby,
andA.Mouzakitis,âAsurveyon3dobjectdetection
methodsforautonomousdrivingapplications,âIEEE
TransactionsonIntelligentTransportationSystems,pp.
1â14,2019.
[11]A.Garcia-Garcia,S.Orts-Escolano,S.Oprea,V.
Villena-Martinez,andJ.Garcia-Rodriguez,âAreviewon
deeplearningtechniquesappliedtosemanticsegmentation,â
arXiv:1704.06857[cs.CV],2017.
[12]L.Liuetal.,âDeeplearningforgenericobjectdetection:A
survey,âarXiv:1809.02165[cs.CV],2018.
[13]B.Li,T.Zhang,andT.Xia,âVehicledetectionfrom3dlidar
usingfullyconvolutionalnetwork,âinProc.Robotics:
ScienceandSystems,Jun.2016.
[14]A.Geiger,P.LenzandR.Urtasun,"Arewereadyfor
autonomousdriving?TheKITTIvisionbenchmark
suite,"2012IEEEConferenceonComputerVisionand
PatternRecognition,2012,pp.3354-3361,doi:
10.1109/CVPR.2012.6248074.
[15]A.Geiger,P.Lenz,C.Stiller,R.Urtasun.Visionmeets
robotics:TheKITTIdataset.TheInternationalJournalof
Robotics Research. 2013;32(11):1231-1237.
doi:10.1177/0278364913491297
[16]Q.Ha,K.Watanabe,T.Karasawa,Y.UshikuandT.Harada,
"MFNet:Towardsreal-timesemanticsegmentationfor
autonomousvehicleswithmulti-spectralscenes,"2017
IEEE/RSJInternationalConferenceonIntelligentRobots
andSystems(IROS),2017,pp.5108-5115,doi:
10.1109/IROS.2017.8206396.[17]Y.Choietal.,"KAISTMulti-SpectralDay/NightDataSet
forAutonomousandAssistedDriving,"inIEEE
TransactionsonIntelligentTransportationSystems,vol.19,
no.3,pp.934-948,March2018,doi:
10.1109/TITS.2018.2791533.
[18]N.Kim,Y.Choi,S.Hwang,I.S.Kweon(2018).
MultispectralTransferNetwork:UnsupervisedDepth
EstimationforAll-DayVision.ProceedingsoftheAAAI
ConferenceonArtificialIntelligence,32(1).
https://doi.org/10.1609/aaai.v32i1.12297
[19]ThenuScenedataset.nuScene.[Online].Available:
https://www.nuscenes.org/
[20]W.Maddern,G.Pascoe,C.Linegar,andP.Newman,â1year,
1000km:TheOxfordRobotCardataset,âInt.J.Robotics
Research,vol.36,no.1,pp.3â15,2017.
[21]Y.ZhouandO.Tuzel,âVoxelNet:End-to-endlearningfor
pointcloudbased3dobjectdetection,inProc.IEEEConf.
ComputerVisionandPatternRecognition,2018.
[22]C.R.Qi,L.Yi,H.Su,andL.J.Guibas,âPointNet++:Deep
hierarchicalfeaturelearningonpointsetsinametricspace,â
inAdvancesinNeuralInformationProcessingSystems,
2017,pp.5099â5108.
[23]T.Roddick,A.Kendall,andR.Cipolla,âOrthographic
featuretransformformonocular3dobjectdetection,â
arXiv:1811.08188[cs.CV],2018.