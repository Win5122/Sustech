第30 组  常见皮肤病智能分析  技术报告  
组长：李思锐  12012719
组员：徐剑  12010923 ，张富珩 12011322
 
一、项目背景  
随着计算机技术的发展，基于人工智能的辅助诊疗系统已经陈伟一个主要的研究和发展趋势。到目前为
止，这些系统已经在眼科、肿瘤学科和放射科学领域开发和研究，在皮肤科领域，一系列研究已经研究
了良性和恶行皮肤肿瘤的分化，如良性痣、恶性黑色素瘤和角质细胞癌，例如反射共焦显微镜、皮肤镜
和皮肤超声。然而，根据临床照片确定诊断良性色素性皮肤病尚无先例。我们的目标是提高医疗诊断的
数字化、网络化以及基层医院的服务水平，减少医师培训时间，在提高医生的效率同时降低误诊率。
 
二、数据集介绍  
我们本次使用的数据集是由香港大学深圳医院的曾丽华医生所提供的八分类皮肤病数据集。详细信息如
下：
 
三、工作介绍  
（一）训练框架  
我们使用较为传统的深度学习框架来进行我们的训练。首先我们设置了随机种子，以确保得到稳定结
果。其次我们令 learning rate=0.0001 、 epochs=100 ，同时我们以交叉熵函数 (CrossEntropy) 作为 Loss  
function ，使用 adam 作为优化器，并使用配套的余弦退火调度器 (CosineAnnealing Scheduler) 来进行
学习率的调整。 Batch Size 我们将在不同的网络模型上进行微调。上图为我们参数设置的具体代码。
考虑到原始数据集数据总量较少，仅有不到 3000 张输入图片，我们还使用了基础的数据增强来增强输入
数据的多样性。我们所使用的数据增强有： RandomHorizontalFlip ， RandomVerticalFlip 以及
RandomRotation 。
（二）不同网络模型对比  
我们主要对比了三种比较主流的图像处理模型： Resnet50 ， Efficientnet-b4 以及 Densenet201 。考虑到
Efficientnet-b4 与 Densenet201 的网络大小较大，其 batch size 若与 Resnet50 设置成一样的话可能会导
致显卡内存不够而使得训练无法正常进行，因此我们将二者的 batch size 调设置为 8 ， learning rate 调整
为0.001 ，而 Resnet50 的 batch size 设置为 100 ， learning rate 仍然为 0.0001 。我们所使用的网络都已经
过ImageNet 的与训练。
最终训练出来的效果如下图所示：
由此可见， Efficientnet-b4 是最适合我们此次训练任务的网络模型。后续我们的一系列尝试也将使用
Efficientnet-b4 作为 Baseline 。（三）图像混合策略使用  
通过查阅文章我们得知，图像混合策略能够比起传统的旋转等数据增强更有效地增加输入特征的多样
性，使得训练出模型的泛化性与鲁棒性得到提升。因此，我们选用了 mixup 以及 Cutmix 两种简单但却有
效的图像混合策略，以期得到模型性能的提升。两种策略的简介如下：
mixup  
mixup 通过叠加两张不同的输入图像来产生新的图像。我们可以控制超参数混合比例 lam ∈ [0,1] 来
控制不同输入在混合图像中的显著程度。由于在图像混合的同时标签也会有所混合，而常见网络并
不会处理带小数的软标签，因此我们需要把混合图像在两张输入图像的所属类下分别计算 loss ，再
将loss 以混合比例 lam 进行叠加。
Cutmix  
Cutmix 与 mixup 原理类似，只是产生新图像的方式由叠加混合变成将输入图像 1 的一部分挖去，再
填上输入图像 2 的对应部分来代替。混合比例 lam ∈ [0,1] 主要是指两个输入图像在输出图像中所占
的面积。 Loss 部分的计算与 mixup 相同。
我们在保留 （二）不同网络模型对比 中Efficientnet-b4 训练参数的基础上，分别加入了上述两种策略，
取得的结果如下：
可以看到，相较于 mixup ， Cutmix 在皮肤病数据集上取得了相对优秀的结果。我们分析这是因为皮肤疾
病图片特征较少且相对明显， mixup 所带来的两张图片混合叠加可能会带来相对较多的不必要特征混
合，因此效果可能稍微逊于 Cutmix 。（四） Focal Loss 的尝试  
我们发现，初始的数据集其实呈现的是长尾分布 (Long-tailed Distribution) ，其特征为部分类别数据量较
大，而部分类别数据量较少。这样的数据分布会使得深度学习网络在训练的过程中过度关注头部类（多
数类）的特征，而忽略尾部类（少数类）的特征，从而降低训练出模型的性能。
通过查阅文献我们得知， Focal Loss 是在解决长尾分布问题中应用较为广泛的一种 Loss function ，其在
传统 CrossEntropy 的基础上加入了一个权重系数，使得头部类分类分错给模型参数调整带来的影响降
低，而使尾部类分裂分错带来的影响提升。 Focal Loss 与 CrossEntropy 在具体数据上 Loss 的对比如下
图：
但是比较遗憾的是，我们尝试在我们的数据集上引入 Focal Loss 时，模型的性能并没有得到很好的增
加，甚至比起 Baseline 略有降低，而且我们一直未能找到原因所在。这是我们整个项目目前为止最大的
遗憾，希望我们能在未来成功的找到问题所在！
我们在 Efficientnet-b4 中加入 Focal Loss 所得的训练结果如下：
但是，我们注意到 Focal Loss 的核心思维是 “ 加权重 ” ，那么别的形式的权重是否也能起到类似的效果？我
们按  总样本数  / 该类样本数 生成了新的权重，并将其加入了 CrossEntropy 中：
我们将加权重后的模型进行训练，并将加权重后的模型与 Cutmix 进行结合，取得了较为不错的最终结
果：
（五）知识蒸馏使用  
知识蒸馏是一种模型压缩方法，是一种基于 “ 教师 - 学生网络思想 ” 的训练方法。我们将已经训练好的模型
包含的知识，蒸馏提取到另一个模型里面去。
温度 T 是知识蒸馏中的一个重要超参数，下面这个公式是原始的 softmax 函数 :
直接使用 softmax 层的输出值作为 soft target, 这又会带来一个问题 : 当 softmax 输出的概率分布熵相对较
小时，负标签的值都很接近 0 ，对损失函数的贡献非常小，小到可以忽略不计。因此 " 温度 " 这个变量就派
上了用场。下面的公式时加了温度这个变量之后的 softmax 函数 :
T 越高， softmax 的输出分布越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训
练将更加关注负标签，从而学到更多信息。 
在我们的实验中，则是运用了多个教师模型的知识蒸馏，我们将数据集的类别按照样张的数量分到三个
子集中，每个子集中样本的数量差距较小，在一定程度上改善了原本数据集的不平衡问题。然后，我们
在这三个数据集中分别训练出教师模型。再用这三个教师模型共同指导学生模型的训练，完成知识蒸
馏。
在结果中我们可以看到，尽管整体的 acc 提升不大，但在这些样本较少的类别中， f1-score 有了显著的提
升。这意味着模型对少数类的辨别能力有了很大的提升，从而说明这一方法起到了作用。
未来工作  在使用知识蒸馏这一方法时，我们发现，教师在关系子集上的表现仍然有提升空间。这是因为，即使划
分了关系子集，每个子集中各个类别的样本数量仍然存在着一定的不平衡的情况。针对这一问题，我们
计划在教师的训练过程中融合进更多的训练方法，例如上文中提到的图像混合策略和 Focal Loss 。在提
升了教师模型的表现后，我们认为蒸馏出的学生模型的表现也会有所改善。
课程学习  (Curriculum learning, CL)  是近几年逐渐热门的一个前沿方向。 Bengio 首先提出了课程学习
（Curriculum learning ， CL ）的概念，它是一种训练策略， 模仿人类的学习过程，主张让模型先从容易
的样本开始学习，并逐渐进阶到复杂的样本和知识 。CL 策略在计算机视觉和自然语言处理等多种场景
下，在提高各种模型的泛化能力和收敛率方面表现出了强大的能力。在这样的不平衡数据集中，我们可
以先将从数据集中提取出各类别平衡的数据，让模型在这个子数据集上进行训练。随着训练的进行，我
们将剩余的样本逐渐加入训练集中，从而实现一个从易到难的训练过程。
除此之外，由于我们在这一项目中使用的方法大多可以广泛应用在数据不平衡这一问题上。这意味着我
们的数据集将可以不局限于皮肤病，在视网膜眼底图像等数据集中我们的方法应当同样适用。我们将会
结合更多类型的数据集对我们的方法进行进一步的验证。