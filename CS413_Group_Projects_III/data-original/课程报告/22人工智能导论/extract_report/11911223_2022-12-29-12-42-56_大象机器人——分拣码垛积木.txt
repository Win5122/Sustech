图1 ：大象机器人 - 人工智能套装大 象 机 器 人 —— 分 拣 码 垛 积 木
一 、 项 目 背 景 & 目 的
二 、 实 验 环 境 搭 建
1. Ubuntu + ROS
2. Anaconda + Python + 大 象 机 器 人 开 源 代 码 库
三 、 目 标 检 测 算 法
1. 综 述
2. 传 统 目 标 检 测 算 法
3. 深 度 学 习 算 法
4. RGB 颜 色 识 别 算 法
四 、 机 械 臂 控 制
1. 位 置 计 算 & 码 垛 抓 取
2. 码 垛 收 集
3. 机 械 臂 复 原
五 、 总 结 与 提 升
六 、 小 组 成 员
大象机器人 —— 分拣码垛积木  
一、项目背景 & 目的  
随着计算机、电子、机械等学科的不断发展，机器人越来越多地被应用于各个领域，如医疗、智能制
造、应急救援等领域。其中，六轴机器人由于具有可编程、拟人化、通用性、机电一体化、高自由度等
特性，在各个领域都广受欢迎。本项目旨在使用大象机器人的 myCobot 协作机械臂，进行积木的识别和
抓取功能，并借此了解 UR 构形的六轴协作机器人相关知识及操作，包括坐标系概念，机器人运动学，运
动轨迹规划，计算机视觉等。
该项目的硬件环境使用的是 大象机器人 的 人工智能套装 ，其中机械臂型号为 myCobot 280-M5 ，自重仅
850g ，有效载荷 250g ，有效工作半径 280mm ，可搭配多种末端执行器适配多种应用场景，也可支持多
平台软件的二次开发。 myCobot 是 大象机器人 和M5stack 联合出品的一款， 世界最小最轻的六轴协作
机器人 ，是一个拓展生产力和想象力边界的有力工具。对体积小巧但功能强大的六轴机器人的探索，可以为 科研教育、智能家居，商业探索 等各种场景积累宝
贵的经验，并给予创造性的启发。
本项目期望实现 1. 通过摄像头对场景中的目标码垛识别； 2. 控制六轴机械臂移动至相应位置； 3. 使用空气
吸泵对目标码垛进行抓取； 4. 控制机械臂移动码垛至目标收集点； 5. 释放码垛， 6. 回归初始位置并可以进
行下一轮目标抓取操作；这六个主要操作。
二、实验环境搭建  
实验环境搭建方面我们进行了诸多尝试，初期我们尝试使用 Ubuntu ，但由于该版本的 myCobot 机械臂
不支持 Ubuntu 20.04 的版本，后续我们又改用 Anaconda+Python 的环境进行开发。
1. Ubuntu + ROS  
Ubuntu 是一个以桌面应用为主的 Linux 操作系统。由于 Linux 在包管理方面相较于 windows 有很大优
势，我们在项目初期计划安装 Ubuntu 并搭建 ROS 开发环境来进行项目的开发。我们使用 Vmware 进行了
Ubuntu20.04 的安装，并安装相应版本的机器人操作系统  ROS Noetic 、 MoveIt 以及  git 版本管理器。
但由于 myCobot280-M5 型号的机械臂不支持 Ubuntu20.04 的使用，效率起见，我们使用更加友好的
Anaconda + Python 作为我们的实验环境。
2. Anaconda + Python + 大象机器人开源代码库  
Anaconda 是一个可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。
Anaconda 包含了 conda 、 Python 在内的超过 180 个科学包及其依赖项。我们的实验环境基于 python3.7
搭建，主要使用 openCV 开源库 和 大象机器人开源代码库 进行开发。
三、目标检测算法  
1. 综述  
目标检测任务是找出图像或视频中人们感兴趣的物体，并同时检测出它们的位置和大小。不同于图像分
类任务，目标检测不仅要解决分类问题，还要解决定位问题，属于 Multi-Task 问题。目标检测的发展脉
络可以划分为两个周期：传统目标检测算法时期 (1998 年 -2014 年 ) 和基于深度学习的目标检测算法时期
(2014 年 - 至今 ) 。而基于深度学习的目标检测算法又发展成了两条技术路线： Anchor based 方法 ( 一阶
段，二阶段 ) 和Anchor free 方法。
2. 传统目标检测算法  
传统目标检测算法主要基于手工提取特征，思路大多不复杂，但也有一些较为明显的缺点，如识别效果
不够好，准确率不高；计算量较大，运算速度慢；可能产生多个正确的识别结果等。一般传统检测算法
流程可概括如下：
选取感兴趣区域，选取可能包含物体的区域
对可能包含物体的区域进行特征提取
对提取的特征进行检测分类
其代表算法有 Viola Jones Detector ，HOG Detector ，DPM Detector 等。
3. 深度学习算法  
2012 年卷积神经网络 (Convolutional Neural Networks, CNNs) 的兴起将目标检测领域推向了新的台阶。
基于 CNNs 的目标检测算法主要有两条技术发展路线： anchor-based 和 anchor-free 方法，而 anchor-
based 方法则包括一阶段和二阶段检测算法 ( 二阶段目标检测算法一般比一阶段精度要高，但一阶段检测
算法速度会更快 ) 。 
图2 ：识别蓝色和红色码垛
图3 ：码垛识别、分类、收集总流程Anchor-Based 中的代表算法有 RCNN ，SPPNet ，Fast RCNN ，Faster RCNN ，Cascade RCNN ，
YOLO v1 ，SSD ，RetinaNet ，YOLO V5 等
Anchor-Free 的代表算法有 CornerNet ，CenterNet ，FSAF ，FCOS ，SAPD 等
4. RGB 颜色识别算法  
在我们的机器人识别码垛任务中，由于任务场地固定，背景单一，没有必要使用过于复杂的深度学习模
型。基于这些考虑，我们计划 Python+OpenCV 进行基于颜色的目标识别。
首先通过机器人自带的摄像头获取目标的 RGB 图片，利用 opencv 对目标颜色进行识别和提取，并计算目
标颜色的区块大小，过滤掉轮廓围成面积较小的物体，提取图像的轮廓，并计算中心点，最后将中心点
至机械臂操作端进行抓取操作。
四、机械臂控制  
图 4 ：智能机械臂控制下的码垛抓取与收集1. 位置计算 & 码垛抓取  
通过俯视摄像头以及 RGB 颜色识别算法，我们向机械臂控制模块返回了目标码垛所在的平面坐标系中的
中心点位置。由于我们所用的六轴机器人固定基地啊与检测范围相对固定，我们可以据此计算出使得六
轴机器人的吸泵端接触目标码垛所需的六个自由度上的具体数值 —— 旋转 (S 轴 ) ，下臂 (L 轴 ) 、上臂 (U 轴 ) 、
手腕旋转 (R 轴 ) 、手腕摆动 (B 轴 ) 和手腕回转 (T 轴 ) 以及相应的移动速度，移动机械臂至指定位置并启动吸
泵，完成目标码垛的抓取操作。
2. 码垛收集  
在 RGB 颜色识别算法部分，我们在传给机械臂控制模块时，同时传递了码垛的位置和颜色信息。机械臂
会根据获得的颜色信息，以及坐标系转换计算，将拾取到的码垛移动至对应颜色的收集小框上方，释放
吸泵，使得目标码垛被准确收集。
3. 机械臂复原  
在完成码垛释放后，我们会控制机械臂回到初始位置，同时再次使用俯视摄像头拍摄新状态下的码垛状
态，重复步骤 1-3 ，最终完成所有目标颜色码垛的识别，拾取，与分类操作。
 
五、总结与提升  
myCobot 机器人操作简单且能与人协同安全工作，我们通过不断地尝试与探索完成了 myCobot 机械臂分
拣目标码垛实验，并了解了基本的 UR 构形的六轴协作机器人相关知识及操作，包括机器人坐标系概念及
转换，机器人运动学，运动轨迹规划，计算机视觉等知识。
在未来提升方面，在实验过程中，我们发现由于硬件设备的空气吸泵吸力较大，不能很好地完成积木块
的释放操作，后续可以对吸泵控制进行进一步的优化。
六、小组成员  
吴一凡，金冬阳，朱世博，蓝晨溪