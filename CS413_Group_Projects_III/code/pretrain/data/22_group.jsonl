{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "SUSTech ReportPages 1–5\n2022\nDeep Learning Based Medical Image Registration\nShuwen Dong, Qiushi Nie, Fenglin Wei\nInnovation Project\nDepartment of Computer Science and Engineering\nJune 8 2022\nKeywords: Medical Image, Image Registration, Multi-mode\nABSTRACT\nThere are many imaging modes in ophthalmology, but it is difficult\nfor doctors to make effective use of complementary information among\ndifferent modes to make an effective diagnosis. Multi-modal image\nregistration is an effective solution to fuse images of different modes\ntogether. Therefore, this project intends to use the strong feature extraction\nability of deep learning to realize multi-modal image registration based on\nlearning, mainly using common hidden spatial information of images or\ncombining segmentation to carry out multi-modal image registration.\n1 INTRODUCTION\nDoctors sometimes use different detection methods to diagnostic the\nsame pathological area. In the diagnosis of ophthalmic diseases, doctors\ncan use FA (Fluorescein Angiography), Color Fundus, and OCTA\n(Optical Coherence Tomography Angiography). Several images of the\nsame pathological area are often analyzed together, this can help doctors\nhave a better diagnosis. However, because different images have a\nrelative position, it is an inconvenience to diagnose using multi-modal\nimages. Therefore, image registration is essential for a proper and more\nconvenient diagnosis.\nHowever, registering two multi-modal images directly is more difficult\nthan registering two images in the same mode. Therefore, we consider\ntransferring the image into another mode, then register these two images.\n2 BACKGROUND\n2.1 Multi-modal medical image in ophthalmic diseases\n2.1.1 Color Fundus Fundus photography documents the retina, the\nneurosensory tissue in our eyes which translates the optical images we\nsee into the electrical impulses our brain understands. The retina can\nbe photographed directly as the pupil is used as both an entrance and\nexit for the fundus camera’s illuminating and imaging light rays. Fundus\nphotography can be performed with colored filters, or with specialized\ndyes including fluorescein and indocyanine green.[7](Fig.1)\n2.1.2 OCTA Optical coherence tomography angiography (OCT-\nA) has emerged as a non-invasive technique for imaging the\nmicrovasculature of the retina and the choroid. The first clinical studies\nusing this innovative technology were published in 2014.[1]\nOCT-A technology uses laser light reflectance of the surface of moving\nred blood cells to accurately depict vessels through different segmented\nareas of the eye, thus eliminating the need for intravascular dyes.[2]\nOne asset of OCT-based approach is that it provides a quantitative\nanalysis of the retinal vessels (in addition to the qualitative analysis\ndone on standard angiography). Moreover, and contrary to the ”2-D”\nconventional angiograms, OCT-A technology provides ”3-D” imaging\ninformation of the macula and visualizes peripapillary capillaries thatsupply the retinal nerve fiber layer.[3](Fig.2)\nFig.1 Color Fundus Fig.2 OCTA\n2.2 Image Registration\nImage registration is defined as the process of mapping the input images\nwith the help of reference image. The intent of image registration is to\nalign images with respect to each other. The input for this process is\ntwo images: the original image is known as the reference image while\nthe image that will be aligned with the reference image is known as\nthe sensed image. The ultimate goal of the registration process is to\nalign the corresponding features in some sensed images with respect to a\nreference.[6]\n3 RELATED WORK\nThere are many classical methods for multi-mode image registration\nbased on a predefined similarity measure.[8] They extract predefined\nfeatures from multi-mode pictures and try to minimize it according to\nsimilarity measure to figure out some most similar parts of the two images\nwhich are used to do registration. This method have highly explanatory,\nsince they are based on edges or points which are sensible to human.\nHowever, it is not easy to define a good feature extraction as well as\na good similarity measure by a human. This is also the problem for\nintensity-based measures.\nTo solve the problems mentioned before, deep regression models are\nraised. Because deep regression models learn parameters by themselves,\nwe do not need to define feature extractions and similarity measures.\nThey can find a good one if we train them well. Apart from the\ndesigning of measures, deep regression networks can also do complex\nimage reprocessing that can provide more information for the registration\nnetwork to ease its work. For example, the color on multi-mode images\nis always different, then the registration network can only use structure\ninformation. However, Deep Learning is not the final solution for image\nregistration. Because Deep Learning generate feature automatically,\npeople can not understand what is going on inside this black box and\nadjust model.\nA template made for University of Birmingham by Harry Cooke. 1Shuwen Dong, Qiushi Nie, Fenglin Wei\nSo we come up with an idea of combining Deep Learning and\ntraditional features. In such way, we can get feature extraction\nautomatically with high explanatory.\n3.1 Superpoint and SuperGlue\nWe are not the first to rise the idea that combine. SuperPoint[4] is a\ndeep learning based feature extraction and feature descriptor generation.\nSuperglue[5] is a deep learning based feature matching. We used these\ntwo methods to match the feature. We find it some features matching\nresults deviate a lot from the ground truth. This may be caused by\nmultiple-modes. Fig.3 shows the result of feature extraction and feature\nmatching using SuperPoint and Superglue.\nFig.3 Use SuperPoint and Superglue to match features\nSince mode may effect the accuracy of image registration, we can\ntransform images’ mode by neural network, which can convert 2 images\nfrom 2 modes to 1 mode. Using single-mode image, registration network\ncan have more information, such as color, to determine the affine\ntransformation between 2 images.\n3.2 Adversarial generative network\nGAN (Adversarial generative network)[9] is a famous neural network\nwhich is often used in mode transformation. There are two parts in GAN,\nfirst is Generator and the other is Discriminator. To transform our image\n(e.g from CF to OCTA), we use Generator to transform CF to OCTA\nmode and use Discriminator to discriminate the real OCTA and fake\nOCTA. After the accuracy of Discriminator is high enough, we fix the\nDiscriminator and upgrade Generator until Discriminator can not tell the\ndifferent between real and fake image. CycleGAN is a promoted version\nof GAN, which can generate more similar pictures. In this way, we can\ntrain a Generator to transform modes which can even cheat people’s eyes.\nFig.4 real CF and fake OCTA\nFig.5 real OCTA and fake CF\nWe combine CycleGAN and SuperPoint + SuperGlue together and get\nresult below. This result shows that modal transformation plus imageregistration dose work.\nFig.6 Use SuperPoint + SuperGlue on OCTA and fake OCTA\n3.3 Unet\nUnet[11] is originally put forward to do medical segmentation. It use\nskip connection to combine feature in shallow networks and deep\nnetworks together to supplement missing information, which achieves\ngood performance in segmentation area. We use OCTA500[12] to train\nUnet and find good performance on testing our IMED dataset.\nFig.7 CF, fake OCTA (convert from CF by CycleGAN),\nvessel fake OCTA (predict by trained Unet on fake OCTA)\nOCTA, vessel OCTA (predict by trained Unet on OCTA)\n3.4 Candidates for Registration Network\nWe also find another way of registration by Deep Learning without\nfeature points and feature descriptors.\nAmong papers of registration, we finally find a network called\nArbicon-Net [13]. Arbicon-Net is a deep nerual network that predicts\ndense and smooth (which is proved in their paper) displacement field. It\nhas three main components:\n(1) Feature Extractor: It gets the input two images and extracts the\nfeature of them with CNN. Then, the two feature maps will be sent into\na 4D correlation estimator, generating a 4D correlation tensor.\n(2) Transformation Descriptor Encoder: The 4D correlation tensor is\nsent into a 4D convolution block and produces transformation descriptor\nd.\n(3) Displacement Field Predictor: To predict the transformed point,\nsource points are concatenated with the transformation descriptor dand\nsent into a shared weight MLP. The output is the offset of the points.\nSince Arbicon-Net extracts feature from the whole images, which is\nsuitable for our research, so we plan to test and combine it to our network\nin the following phase of work.\nWe first use the model provided by the author, which is trained on\nthe pf-pascal and a synthetic dataset. The PF-Pascal dataset is a dataset\nfor image semantic correspondence. The synthetic dataset consists of\nman-made image pairs by warping a normal image by random generated\ntransformation parameters (affine and deformable are both supported).\nHowever, the code is in old style so we encountered so many exception\nand it costs us so long to run it successfully.\n2Shuwen Dong, Qiushi Nie, Fenglin Wei\nFig.8 PF-Pascal dataset’s source, target, and merge image\nFig.9 CF-OCTA checkboard result\nThe result on PF-Pascal seems great as the main object (in Fig.8,\nthe two planes) is always aligned successfully. However, when CF and\nOCTA image pairs are sent to the model, the result becomes worse. For\nmost of the case, only the macular is successfully aligned, together with a\nsmall part of vessel. We think this is because the model treat the macular\nas the main object of the image pairs and it don’t cares about the vessels.\nWe also tried the OCTA and fake OCTA image pairs generated by our\nCycleGAN which look a little bit better.\nFig.10 OCTA-fake OCTA’s origin and result difference image\n(left: origin, right: result)\nFinally, we use the vessel image pair segmented by UNet as input to\nsee the result. The result seems even better as it abandons the macular.\nFig.11 vessel’s origin and result difference image\n(left: origin, right: result)\nWe then train the network using our dataset with CF and OCTA image\npairs. The performance becomes more precise than before. This proves\nthe availability of ArbiconNet.\nFig.12 newly trained network performance compared to provided model\n4 OUR METHOD\n4.1 Network Architecture\nFig.13 structure with combining encoders of edge extraction\nand registration together\nOur network architecture is as shown in Fig.13, we first use pre-trained\nmoddal transformer to convert image real CF to image fake OCTA, so\nthat the target image has the same modal as the reference image. then we\nextract features of the image pairs, and put them into both Segmentation\nNetwork and Registration Network, and we finally compute the loss to\nrestrict those two networks.\nFig.14 deep view of our registration network\nIn our registration network, we extract the features of the image pairs\nFAand F B, each channel fij∈Rcin the feature Fi∈Rh×w×cholds\nthe local semantic information. We use normalized cosine similarity to\nmeasure the similarity between two channels. For each channel fA\nijin\nFA, we compute the similarity with each channel fB\nklinFB, and then we\nget a 4D correlation tensor C∈Rh×w×h×w, where each element cijkl\nis computed by normalized cosine similarity:\ncijkl=< fA\nij, fB\nkl>\n||fA\nij||2||fB\nkl||2(1)\n3Shuwen Dong, Qiushi Nie, Fenglin Wei\n4.2 Loss Functions\n4.2.1 Segmentation Loss\nDiceLoss = 1−2(|X| ∩ |Y|)\n|X|+|Y|(2)\nDice loss in equation 2 measures the overlap between two point sets.\nCrossEntropy =−X\np(x)log(q(x)) (3)\nCross-entropy in equatio n1 measures the difference between two\nprobability distributions for a given random variable or set of events.\n4.2.2 Registration Loss Chamfer Distance is a method used to\nmeasure distance between point set, it is usually used in 3D point cloud.\nFirst, we count distance to nearest point for every pixel in target image,\nget dist. Then, we multiply source image on dist and calculate the sum.\nThe loss value varies on different position between source and target\nregardless of their overlap, and it can measure distance according to all\npoints, instead of few.\nChamferDistance (T, I) =1\n|T|P\nt⊆TdI(t) (4)\nHowever, we find out that this Loss function will lead transform matrix\nto 0 or shift image out of window. This problem is because the loss is\ndone by nonoverlapping points. So if transform matrix shrink the vessel\nto a point or rotate it out of window to reduce the number of point of\nvessel, the loss will definitely drop. However, this is not what we want.\nTo solve this problem, we also apply Reverse-Hing Loss.\ntarget source loss\nvessel(1) non-ves(0) loss(1)\nvessel(1) vessel(1) no loss(0)\nnon-ves(0) non-ves(0) no loss(0)\nnon-ves(0) vessel(1) no loss(0)\nreverseHing = max {0, target −source } (5)\nUsing reverseHing loss, the maximum loss is the sum of target (though\nthere is no vessel on the other image). Even if there is a little overlap, the\nloss will reduce. In this way, source image is encourage to stay in window\nand overlap as much vessel as possible.\nThe overall loss function is\nLoss =DiceLoss\n+CrossEntropy\n+reverseHing\n+ChamferDistance(6)5 EXPERIENCE RESULTS\nFig.15 test results of our network\nWe also choose two other famous model to compare with us. SIFT[15]\nis a traditional model used for image registration, SuperPoint[4] +\nSuperGlue[5] is a model based on deep learning for image registration.\nThey are both based on feature points and feature descriptors. We trained\nand test them on same CF-OCTA dataset from IMED. There are 13\npictures in total.\nFig.16 multi-modal image registration baselines\nOur Model SIFT SuperPoint + SuperGlue\nMSE 0.119 – –\nRMSE 0.344 – –\nFail(13 in all) 0 13 13\nFirst, we tried multimoding images. SIFT and SuperPoint+SuperGlue\nfailed on all of them since they can not find feature points between\ndifferent mods. However, our model can handle them since we have a\nmod transformation on CF to fake OCTA.\n4Shuwen Dong, Qiushi Nie, Fenglin Wei\nFig.17 single-modal image registration baselines\nOur Model SIFT SuperPoint + SuperGlue\nMSE 0.119 0.138 0.090\nRMSE 0.344 0.371 0.300\nFail(13 in all) 0 12 3\nSecond, we test these three models on OCTA and fake OCTA (convert\nby CycleGAN). This time, SIFT still fail on 12 out of 13 pictures,\nthis is caused by detail difference between fake mod and real mod,\nwhich shows that traditional human designed feature extraction does not\nwork well on different mods, even if they looks very similar. However,\nSuperPoint+SuperGlue fail only on one picture and reaches a slightly\nbetter performance than us. This proves that automatic feature extraction\nis better on precision but it still have the problem of finding feature points.\nThough our model performs a little worse than SuperPoint+SuperGlue,\nwe can run on any pictures successfully due to our feature based\nregistration without with relatively good performance.\n6 RESEARCH PROCESS\nWe have outlined our research plan into 3 phases. Each phase commits to\nan action for a specific objective.\n6.1 Phase I (2022.02-2022.03)\nIn this phase, we learn the basic concept of modal conversion and image\nregistration, also the feature and the structure of them. We also study the\nwork of the existed image registration method based on deep learning.\n6.2 Phase II (2022.03-2022.05)\nIn this phase, we focus on run the exist registration method on our\ndatasets, and find the defects of the method when running on medical\n6.3 Phase III (2022.05-2022.06)\nIn this phase, we will test our modified model on our dataset, then fine\ntune the model. We will also using some tricks to improve our modified\nmodel.\n7 CONCLUSION\nWe think we are on a great road of image registration and we strongly\nbelieve that it will lead us to a paper. In the future, we will work hard to\nachieve the plans we made.\n"}, {"role": "assistant", "content": "最终打分：9.3 (范围0-10分)\n1. 结构完整性得分：10, 占比20%\n2. 逻辑清晰度得分：9, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：9.5, 占比20%\n5. 参考文献规范性得分：8, 占比10%\n6. 课程知识掌握度得分：10, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "Depth Estimation and Defocus\nDeblurring of Microscope Images Based\non Deep Learning\nTeam Member: Xinyi Zhou (11812031), Yingquan Zhou (11812636)\nInstructor: Jiang Liu, Yan Hu\nSouthern University of Science and Technology\nJune 8, 2022\nI. Introduction\nFigure 1: Limited view during ophthalmic surgery.[12]\nIn some ophthalmic surgeries, the doctors should\ndo the surgery under the microscope, using tiny\nsurgical instruments. For example, in epiretinal\nmembranes peeling surgery, the view (shown in\nFig.1) is limited and most importantly, it is hard to\nestimate the depth of the surgical instrument, which\nmay hurt the retina accidentally. Severe cases may\nlead to retinal detachment, which is a kind of medical\nmalpractice. Estimating depth may be a helpful way\nto aid to ophthalmic surgeries. Therefore, we plan\nto estimate the depth information from microscopic\nophthalmic surgery images with the technology of\nKnowledge Distillation.\nHowever, in the microscopic scene, it is difficult to\nobtain monocular depth ground truth. But it is\nrelatively easy for calculating the depth map for\nbinocular images via disparities. Hence, we want to\nuse binocular images as the assistant for predicting\nbinocular depth.\nWhat’s more, during our observation of surgery\nimages, we found that some regions were blurry due\nto the microscope’s limited depth of field. As seen in\nFig.1, some details of the blood vessels are defocused\nand hard to recognize. Doing depth estimation withclear images is difficult enough, let alone with blurry\nimages under high magnification microscopes.\nThus, we separate the project into the two following\ntasks:\n1. Defocus Deblurring of Microscope Images\n2. Depth Estimation based on Microscope Images\nReviewing related methodologies, we plan to adopt\nthe following methods for each task:\n1. Multi-focus Image Fusion\n2. Multi-scale Depth Estimation\nII. Related Work\ni. Defocus Deblurring\ni.1 Defocus Blur\nDefocus Blur is a result of imperfect light conver-\ngence. A point light source will form a circle on the\nsensor plane instead of a point if the light ray can-\nnot focus perfectly after going through the len. This\ncircle is called Circle of Confusion (CoC) . When the\nradius of CoC is within a certain range, the human\neyes can perceive it as a single point. Under this cir-\ncumstance, the whole image will look sharp to the\neyes. If the size of CoC exceeds the range, the im-\nage will have regions identified as blurry to the eye.\nWhenmovingtheobjectfromfartonear, thereexists\na distance that either farther or nearer will cause the\nimage to have blurry regions. This special distance\nis called the Depth of Field (DoF) .\ni.2 Multi-focus Image Fusion\nThe approach of multi-focus image fusion[4] aims to\ncombine the focused parts from a stack of multiple\nregional out-of-focus images into one all-in-focus im-\nage. In another word, sharp information about each\n1pixel with different spatial coordinate is contained in\nthis image stack. Therefore, the job is to select the\nmost focused ones and fuse them into one image.\nMultiple approaches have emerged to in this\nfield[10]. Transform domain-based solutions as [9]\nfirst convert the source images into the transform do-\nmain, in which operations based on predefined fusion\nrules can be applied. The results are then inversely\nconverted back to the original domain. The conver-\nsion between the two domains makes this kind of so-\nlution time-consuming. Pixel-based methods[20], as\nthe most representative methods of spatial domain-\nbased multi-focus image fusion solutions, utilize spa-\ntial features of the source image to generate a weight\nmap and calculate each pixel value of the fused image\nas the weighted average of all source images.\nWith the wide spread usage of deep neural networks\n(DNN) in computer vision, many multi-focus image\nfusion works adopted deep learning to reconstruct\nall-in-focus images. They can be divided into two\nkinds, classification methods and regression meth-\nods[10]. Classification methods use DNN to gener-\nate the focus weight map replacing the manual focus\nmeasure standards. Regression methods seek ways to\ngenerate all-in-focus images from an end-to-end ap-\nproach, so as to get rid of calculating the weighted\naverage of all source images with a weight focus map.\nii. Depth Estimation\nii.1 Monocular depth prediction\nEstimating depth information from images is one of\nthe basic and important tasks in computer vision.\nThe task could be divided into several categories be-\nlow:\nSensor-based methods\nRGB-D and LIDAR cameras are designed for direct\ntransmission of the image information directly. The\nRGB-D camera is designed to be used as an image,\nand the pixel map of the RGB image can be used as\nan image, and it can be used to display the image\nof the device[14]. The LIDAR in the pharmaceuti-\ncal industry is described in [19], and is described in\n3D.Othertowingdevicesareusedinthefieldandthe\npower supply of the power amplifier (RGB-D and LI-\nDAR cameras) is used in connection with the robotic\npower supply. Auxiliary equipment is used, the con-\nnection and the connection to the monocular camera\nare connected to the mains of the data card.\nDeep-learning-based methods\nWith the rapid development in deep learning, deep\nneural networks show their outstanding performance\non image processing, like image classification, objec-tive detection and semantic segmentation , etc. A va-\nriety of neural networks have manifested their effec-\ntiveness to address the monocular depth estimation,\nsuch as convolutional neural networks (CNNs)[4], re-\ncurrent neural networks (RNNs)[17], and generative\nadversarial networks (GANs)[1].\nii.2 Building dataset\nMicroscopic Eye Surgery Dataset\nThe emergence and continuous enrichment of oph-\nthalmic surgery datasets has greatly promoted the\ndevelopment of deep learning methods in the field\nof ophthalmology. Scientists typically record surgi-\ncal information in the form of videos or pictures.\nThe CaDIS (Cataract Dataset for Image Segmenta-\ntion) dataset [5] contains 50 videos of cataract surg-\neries performed at the University Hospital of Brest.\nThe videos were captured during phacoemulsification\nsurgery using a camera mounted on a surgical micro-\nscopetofocusonthepatient’seye. Thevideoshadan\naverage duration of 10 minutes and 56 seconds and\nwere recorded at 30 frames per second (fps). The\ndataset contains instrument labels and surgical stage\nlabelsannotatedbyclinicalprofessionals, andintotal\nincludes 4671 frames of fully annotated data.\nVirtual dataset\nVirtual data has emerged in many different fields.\n3D-front[3]isa3Ddatasetofinteriorscenes, modeled\nbyCAD(Computer-AidedDesign)software,covering\n31 scene categories, object semantics (such as cate-\ngory, style, and material labels), and 18,968 rooms\nwith 3D furniture objects. It can be applied to 3D\nscene reconstruction, 3D scene segmentation, SLAM\nand other 3D scene research directions. This dataset\nis also suitable for data-driven design studies such as\nfloor plan synthesis and other tasks.\nIII. Methodology\ni. Multi-focus Image Fusion for Defocus De-\nblurring\nBased on the principles of our baseline model, the\nAiFNet[11], we modified the framework to adopt it\nto our ophthalmic microscopic scenario.\nFigure.2 shows the general architecture of our\nmethod. TheoriginalnetworkofAiFNetiscontained\nin the Fusion Network (FN). Before inputting mul-\ntiple defocused images to the FN, we use the Canny\nEdge detector to extract the edge information used\nas a loss item. We also use the Focus Measure Block\n(FMB) to estimate each pixel’s focus level as those\nin the traditional methods. The channel-by-channel\n2focus level feature maps are then concatenated with\nthe input 3-channel image to be sent into FN for fu-\nsion.\nThe details about the FMB and FN are shown in Fig-\nure.3 and Figure.4. The FMB adopts the method of\nSum-Modified-Laplacian to extract the focus level,\nwhich is proved to have a relatively better focus\nlevel measurement[8]. Also, it uses thresholding and\nwindow-size summation to accommodate for spatial\nvariation in images.\nThe FN is a block where different branches of defo-\ncused images are processed. It encodes the features,\nselects pixels with maximum importance, and fuses\nthem into one images under the supervision of loss\nitems.\nFrom Figure.2, we can see there are four loss items.\nTheLedgeis for recovering detailed object edges, the\nLaifandLperceptual are for all-in-focus supervision,\nand the Lidentityis to maintain the original color\nmapping.\nFigure 2: General framework of our Defocus Deblurring\nMethod.\nFigure 3: Details of the Focus Measure Block.\nii. Depth Estimation\nWe have done a survey for the reasons of the blurred\nand distorted boundary. As the point that came up\nFigure 4: Details of the Fusion Network.\nwith by Zhao M[10] in 2018, lossing spatial resolu-\ntion in the estimated depth maps would make the\nestimation in the boundaries be unclear.\nFigure 5: Comparison among low and high resolution\nestimations.\nFrom the above comparative experiments, we can\nfind that depth estimation with higher resolution is\nmore clear. Therefore, we need to use both the\nmethod that abstract feature from high-resolution\nlevel and data with high resolution to keep our re-\nsults good. However, we also find that data with low\nresolution is not completely useless. For example,\nhigh-resolution data could express the detailed in-\nformation of the images. While low-resolution data\nhas more advantages to abstract the details of ob-\nject shapes. So we need to use a method that could\ncombine the features abstracting from multiple reso-\nlutions to archive the best performance. The multi-\nscale depth estimation model came up by Hu, J et al\n[7] mets our requirement. The structure of the model\nis listed below. The network uses ResNet-18[6] to ex-\ntract depth feature from the scale of 64, 128 and 256\nand combine them to a total feature vector. The\nclass of residual networks was proposed to solve the\ndegradation problem of deep learning models. As the\nnumber of layers of deep learning networks become\ndeeper, the network can refine more complex feature\ninformation, but the problem of gradient disappear-\nanceorexplosioncomeswithit,makingtheover-deep\n3network less effective. The residual learning can be\na good solution to this problem. So ResNet could\nhave effectively extract information from the input\nimages.\nFigure 6: Network structure of multi-scale depth model.\nAnother novelty of this method is that the loss\nfunction has not only the information of depth, but\nalso the information of gradient and normal.\nldepth =1\nnnX\ni=1F(ei)\nlgrad=1\nnnX\ni=1(F(∇x(ei)) +F(∇y(ei)))\nlnormal =1\nnnX\ni=1 \n1−nd\ni, ng\nip\nnd\ni, nd\nip\nng\ni, ng\ni!(1)\nwhere F(x) = ln( x+α). It is shown that these two\nimprovements enable to attain higher accuracy than\nthe current state-of-the-arts.\nIV. Experiments and Results\ni. Multi-focus Image Fusion for Defocus De-\nblurring\ni.1 Dataset\nThe dataset contains 3718 microscope fundus images\ncaptured with a fake eye model. Six image stacks are\ncollected under commonly used magnifications of 10,\n16, and 20 in different numbers of images acording\nto different DoF. Among them, five stacks of each\nmagnification are for training and another for test-\ning. All images are rescaled to 256x256. The de-\nblurred ground-truth images are generated by [20],\nfor its relatively good result.\ni.2 Experiments\nWe trained the dataset with our modified network.\nAt the same time, we did comparitive experiments\non other methods: Wang[16], ASR[18], IFCNN[21],\nand the baseline AiFNet[11].\nFigure 7: Sample focal stack from the dataset.\ni.3 Results\nThe metrics we choose are Structural Similarity\n(SSIM), Peak signal-to-noise ratio (PSNR). SSIM is\nto estimate the similarity level of estimated images to\nthe ground-truths while PSNR is to quantify recon-\nstruction quality for images. The quantitative results\nare shown in Table.1 and the qualitative results are\nshown in Figure.8\nModel Type SSIM PSNR\nWang[16] Spatial 0.6782 25.04\nASR[18] Transform 0.8054 29.26\nIFCNN[21] Deep Learning 0.8583 29.35\nAiFNet[11] Deep Learning 0.9054 31.91\nOurs Deep Learning 0.9230 32.10\nTable 1: Quantitative Results Comparing to Midterm.\ni.4 Discussions\nWe can see from the comparison experiments that\nour model exceeds other models in this ophthalmic\nmicroscopic image dataset. Also, comparing the im-\nage results, our model fixed the problem of having\nblurring blood vessel boundaries and enhanced its\ncapability of maintaining the identical color mapping\nbyaddingedgeloss, identityloss, andtheFocusMea-\nsure Block.\nHowever, the image results still have potentials to\nbe more fine-grained, which is a task left for future\nworkers.\n4(a) Baseline\n (b) Ground Truth\n(c) Midterm\n (d) Our Model\nFigure 8: Image Results.\nii. Depth Estimation\nii.1 Review\nIn the previous experiments, we have tried some al-\ngorithms to calculate the ground truth of depth map\nof microscopic image of surgical instruments. SFM\n(Structure From Motion)[15] is a method is for esti-\nmating 3D structure from a series of multiple 2D im-\nage sequences containing visual motion information.\nThis requires estimating the rotation and translation\nmatrix R,t of the photo from multiple views, and\nrecovering the sparse point cloud structure of the ob-\nject. This methods could only be used for monocular\nimages. The results calculated by SFM is listed be-\nlow:\nWe could find that the performance of depth map\ncalculation for monocular depth estimation is not\nidea enough even after filtering and flood-filling.\nHence we still need to try other possible methods.\nHence, we used a method that is specifically for com-\nputing depth maps for microscope images[13]. The\nresults we obtained are listed below:\nWe can find that the both the results have a obvi-\nous flow: the boundary of the instruments is blurred\nanddistorted. Therefore, weneedfindothermethods\nto deal with this problem.\nFigure 9: Estimation results of SFM.\nFigure 10: Estimation results of microscopic method.\nii.2 Data\nTo solve the problem that our data are in low reso-\nlution, we built a simulated microscopic ophthalmic\nsurgery dataset simulating the real scenes. Blender\nis a free, open source 3D authoring suite. It sup-\nports the entire 3D authoring process - modeling,\nbinding, animation, simulation, rendering, composit-\ning and motion tracking, even video editing and\ngame creation. Also, Blender Blender’s powerful fea-\ntures make it a great tool for free-hand modeling.\nBlender’s powerful features make it a great software\nfor free-hand modeling. The Python interface pro-\nvided by the software also provides the ability to\ncalculate real values after building the basic model.\nThe Python interface also provides a way to calculate\nreal values after building the basic model. Blender-\nProc[2] is a Python library that interfaced with the\nBlender interface. This means that BlenderProc can\ntake advantage of features such as ray tracing and\n5hardware accelerated rendering. It supports multi-\nchannel parallel computing and is therefore suitable\nfor generating realistic images for training convolu-\ntional neural networks that require large amounts of\ndata. BlenderProc can compute data for segmenta-\ntion, depth, normal and pose estimation, etc. We\nconstructed 64 different virtual scenes based on real\ncataract surgeries on Blender and calculated the cor-\nresponding depth and segmentation ground truth for\nthem. By now, we have collected 614 groups of data.\nFigure 11: Samples of virtual dataset.\nInstrument Amount Distance to camera\nScissors 313 6-10cm\nRing 325 8-12cm\nLatch needle 302 10-14cm\nTweezers 318 8-12cm\nCornea Scissors (curved) 315 4-8cm\nCornea Scissors (straight) 320 4-8cm\nTable 2: Statistics of virtual data.\nii.3 Train\nWe have trained the model on our virtual dataset for\n10000iterations. Thevariationcurveofthelossfunc-\ntion is as follows: According to the figure, the value\nFigure 12: Variation curve of the training loss.\nof the loss function decreases significantly with theincrease of the number of training iterations in the\nearly training period and then stabilizes at a lower\nrange. This can also explain the effectiveness of our\ntraining to some extent.\nii.4 Result Analysis\nWe tested both the virtual data and the microscopic\ndata under our trained model.\nThe estimation results on the virtual data is good.\nThe depth map could shows the depth changes in\nthe instrument and the eye clearly. The predictions\ncan even show changes in the depth of the bound-\nary between the anterior and posterior anterior seg-\nments. However,the estimation of the real data still\nFigure 13: Samples of test results of virtual data.\nhaveflaws. Forexample, thepredictionoftheblurred\nareas at the tip of the instrument is very poor. And\nthe dark color in the background of the image also\ninterferes with the depth prediction. Therefore, we\nthink there are still some necessary improvements:\n1. Optimize the way real images are taken. Avoid\nreflective or blurred camera positions when shooting,\nand adjust the background of the image to reduce the\ninterference of the background on the prediction.\n2. Increase the size of virtual data sets. The current\nsize of virtual datasets is small, which may result in\nweak generalization of the trained model and thus\nnegatively affect the prediction results.\n6Figure 14: Samples of test results of real data.\nV. Summary\ni. Multi-focus Image Fusion for Defocus De-\nblurring\nLooking back to our work in defocus deblurring this\nsemester, we did the following things:\nFirst, we finished the data collection. First, we\nmeasured the Depth-of-Field value under each mag-\nnification to determine the number of images we need\nto capture under each magnification. Then, we uti-\nlized the ophthalmic microscopes in the lab to col-\nlect a defocus microscopic image dataset. Although\nthis dataset is collected using fake fundus model, it\nis enough to illustrate the problem of defocus deblur-\nring in ophthalmic surgery.\nThen, we trained the baseline model of [11] on\nour dataset. We discovered from the results that it\ncontains blurry blood vessel curves and mismatched\ncolor mapping between the input images and the out-\nputs. Therefore, wefixedtheproblembyaddingedge\nloss, identity loss, and Focus Measure Block onto the\noriginal network.\nIn the future, the defocus algorithm can be com-\nbined with depth estimation method to help the sur-\ngeons get better view and lower the patients’ risks\nduring the surgery.\nii. Depth Estimation from Knowledge Dis-\ntillation\nIn this semester, we have tried different methods to\nobtain the depth estimation of microscopic dataset\nand solved a series of relative problems.\nFirst, we applied different traditional methods to\ncalculate the ground truth of depth of our self-token\nmicroscopic data. The problem we encountered wasthat traditional methods can only effectively calcu-\nlate depth maps for certain images taken at specific\nlocations and angles but performed badly at large\nnumber of random data.\nWe did a survey for the above problem and find\nthat due to some characteristics of microscope data,\nsuch as the non-distinct texture and color of the ob-\nserved object material, it is difficult for conventional\ndepth prediction methods to effectively calculate the\ndepth data of microscope images. Hence we found a\nmethod that is specially designed for the depth es-\ntimation of microscopic data. This method is effec-\ntive in predicting the overall depth variation of mi-\ncroscopy data, but the predictions at the boundaries\nare often blurred or distorted.\nFinally, we solved the problem of blurred and dis-\ntorted boundaries by combining the depth feature\nextracted from multiple resolution scales and built\na virtual data set to obtain high-resolution images\nand corresponding ground truth. The predictions are\nsatisfying on virtual test dataset. But there are still\nsome improvements need to be done before we could\narchive idea estimation of real microscopic data.\n"}, {"role": "assistant", "content": "最终打分：9.3 (范围0-10分)\n1. 结构完整性得分：9, 占比20%\n2. 逻辑清晰度得分：9, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：9.5, 占比20%\n5. 参考文献规范性得分：10, 占比10%\n6. 课程知识掌握度得分：10, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "基于cSSO 图像的关键参数提取及脑疾病预测研究\n11911702 郭一潼\n11910831 刘季冬\n2022 年 6 月 8 日\n第一部分 研究背景\n1 临床需求\n中国是全球阿尔茨海默症患者人数最多的国家之一。有老年痴呆患者500万人之多，占世界总病\n例数的四分之一，且每年平均有30 万新发病例。然而阿尔茨海默症等脑疾病严重依赖于核磁共振成\n像技术，此技术对老年人身体危害大，同时成本高。\n2 研究目的\n关于眼底彩照的研究技术已经比较成熟，数据集和论文发表比较多，模型以及算法也较成熟，而\n关于cSSO 的文献较少，我们希望以眼底彩照为基础，迁移到cSSO图片上，通过提取cSSO图片上\n的特征，最终达到临床诊断脑部疾病的目的。\n3 图片简介\n眼底彩照是医生利用眼底相机采集到的图像，其上会呈现视网膜上主要组织结构。\ncSSO 图像是共聚焦眼底成像。它以激光共聚焦扫描显微术为基础，利用人眼自身的屈光特性，实\n现对眼底视网膜的动态成像，成像面积有所提高，相对于眼底彩照有更好的信噪比、更高的分辨率以\n及清晰度。\n第二部分 数据采集\n数据集可以分为眼底彩照数据集以及cSSO数据集两类。其中眼底彩照数据集共88张，来自于公\n共数据集DRIVE,STARE和CHASEDB1，用于血管分割。cSSO数据集中患有阿尔茨海默症的cSSO\n共58 张，正常253张，患有脑血管疾病的共152张。\n第三部分 现状\n4 项目难点\n(f) 脑血管疾病 cSSO\n • cSSO 图像无公共数据集和相关论文。不同于科研界广泛流行的眼底彩照，cSSO 图像目前只在\n医院临床使用，还并未正式走进科研界。因此，我们无法查找到任何相关研究文献或公共数据集，\n造成了比较大的困难。\n• 临床数据样本量过小且噪声多。由于数据全部来自于医院临床，受患者数量限制以及采集图像的\n特殊性，cSSO 的数据量远远达不到支持神经网络训练。同时，在真实数据采集过程中，采集环\n境的光线以及患者睫毛位置等都会对图像明暗造成影响，导致噪声出现。\n• 无数据血管分割真实值。由于我们想采取对cSSO图像进行血管分割从而进行更多的参数提取，\n神经网络训练时血管的真实值不可避免。在这种情况下，cSSO图像作为临床真实数据，并没有\n真正在科研界流行，其血管分割的真实值也并无专家标注。\n5 研究价值\n作为一个眼脑联动项目，脑部图片同样比较模糊且噪声较多，与cSSO图像有着较大的共同点。同\n时，cSSO 图像比眼底彩照图像的信噪比更优，也可以清晰的显示更多图像细节。另外，作为对科研界\n来说全新的图片，cSSO图像是一个绝佳的创新点，也有着很大的研究价值和研究潜力。\n第四部分 期中进展\n5.1 深度学习\n我们采用Ladder-Net 来进行眼底彩照的血管分割。Ladder-Net 有多对编码器-解码器分支，并且\n在每一层的每一对相邻的解码器分支和解码器分支之间都有跳过连接，因此有了更多的信息流路径。\n如图1所示，1、3是encoder 分支，2、4 是 decoder 分支，A-E 是不同级的特征。整个模型没\n有使用池化层，用的是一个步长为2的卷积层代替，通道数在encoder部分逐级翻倍。我们所使用的\nLadder-Net 相当于把两个 U-Net 相连，有很多条信息流路径，比如 (1) A1 A2 A3 A4, (2) A1 A2 A3\n B3 B4 A4, (3)A1 B1 B2 B3 B4 A4, (4) A1 B1 C1 D1 E1 D2 C2 B2 A2 A3 A4。每一条信息流都可以\n看做是一个FCN网络。路径数量随着U-Net 的个数和级数指数级增长，所以Ladder-Net 有捕获复\n杂特征的能力，并且能获得高准确率。\n为了减少过多编码器-解码器分支造成的参数增加，网络中又使用了共享权重残差模块。图片右侧\n的两个卷积是共享参数的，同时添加了dropout层防止过拟合。\n示意图如图1所示。\nFN：False Negative, 被判定为负样本，但事实上是正样本。\nFP：False Positive, 被判定为正样本，但事实上是负样本。\nTN：True Negative, 被判定为负样本，事实上也是负样本。\nTP：True Positive, 被判定为正样本，事实上也是正样本。\nAccuracy =\n TP+TN\n TP+FP+FN+TN\n模型评估参数: Accuracy: 0.958\n我们试图把这个模型迁移到cSSO图像上，如图3所示。\n5.2 Matlab 分割模型及去噪\n同时，我们还试图采用Matlab集成算法对cSSO图像边缘进行处理，从而达到血管分割的目的。\n通过观察上述图片我们可以发现，matlab分割产生的结果图中，噪音主要分为两部分：第一部分\n为血管间断不连续，第二部分为白点状噪声。对于第二部分噪声，中值滤波和自适应滤波器的效果很\n好，对于第一种噪声，采用图像处理领域中的闭操作从而达到使图像中血管更连续的目的。\n第五部分 风格迁移\n5\n经过分析，我们认为前期模型迁移效果不好的主要原因是眼底彩照与cSSO图像相差过大。虽然\n它们都为3通道图片，但通过测量图片的灰度分布图如图6，可以发现较大差异。为了达到更好的迁移\n效果，我们决定调整眼底彩照的灰度，以达到使眼底彩照更加接近cSSO图像的目的\n5.3 手动调整灰度分布\n首先，我们尝试手动调整眼底彩照的灰度分布。分别读取图片每一个像素，并按照范围乘系数进\n行线性调整。但由于眼底彩照灰度集中在0-20，同时分布极其不均匀，从图7中可以看出，手动调整的\n结果并不尽如人意\n5.4 风格迁移\n在这种情况下，我们通过文献调研，决定采取新的方法：风格迁移。神经网络中，较低层的特征\n描述了图像的纹理、颜色等具体视觉特征，而较高层的特征则描述了图像的内容相似性，在网络中采\n用了欧氏距离来计算loss。对于风格特征，由于如果两幅图像的风格相似但内容完全不同，则欧氏距\n离会产生较大误差。在这种情况下，我们采取Gram矩阵计算不同响应层的联系。不同于文献中输入\n白噪声图片、风格图片和内容图片，我们采取输入一张风格图片和一张内容图片的方式，即输入一张\ncSSO 图片和一张眼底彩照，再通过预训练的VGG16网络对这两张图进行训练并更新眼底彩照图像。\n在风格迁移后，我们把眼底彩照图像变成了图9所示。\n风格迁移后，眼底彩照与 cSSO 的相似度有了显著提高，我们再次把眼底彩照放入前面提到的\nLadder-Net 中进行训练并迁移，得到了更好的迁移效果如图10。左侧是以0.1为临界值二值化的血管\n分割图，右侧则是模型迁移后的原图。虽然并不能真实的分离出所有血管，但效果还是有了显著提高。\n第六部分 直接分类\n由于我们的最终目标是使用cSSO 图像实现脑疾病的分类，因此我们也尝试了使用ResNet34 网\n络对cSSO图像进行直接分类。数据集中，有58张ADcSSO，253张ControlcSSO和152张CSVD\n cSSO，为了确保各类别数据量的一致性，我们选择每类使用58张进行训练并分类。在训练350轮后，\n准确率达到了0.97。\n第七部分 未来规划\n1. 继续完成血管标注工作。\n2. 风格迁移提高模型迁移成功率后，进一步调整神经网络。\n3. 尝试提取血管参数。\n"}, {"role": "assistant", "content": "最终打分：8.4 (范围0-10分)\n1. 结构完整性得分：8.5, 占比20%\n2. 逻辑清晰度得分：8.5, 占比20%\n3. 语言连贯性得分：8.5, 占比20%\n4. 内容独特性和创新性得分：9, 占比20%\n5. 参考文献规范性得分：6, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "Iris Fast Labeling System Based on OCT Image           11911737 林⼩璐 11910311 周⼀凡 11911425 李怀武1Abstract  With the development of computer technology, especially the rapid progress of deep learning in recent years, the computer-aided diagnosis system has become a powerful assistant for doctors. However, the effect of deep learning relies on a large number of accurately labeled images, especially in the field of medical images, and the high cost and difficulty of labeling hinders the popularization of deep learning technology, and there is a lack of labeling software on the market that can adapt to the case of data defects. To address this problem, we propose a fast iris annotation system based on OCT images by combining weakly supervised techn iques. The system separates the front and back ends, which is beneficial for project management and data security, and integrates three weakly supervised algorithm models for three data defect cases, and the corrected annotated images are sent back to the back end for model correction. For iris images, a simple and efficient key point annotation method can be used. For other medical images, it can also be used to improve the annotation efficiency of users and reduce the labor cost according to their needs. 2Project Design 2.1Front-end Design  In the front-end design, we use the html+css+js architecture, and the main annotation functions are implemented in js. At the front end, we have implemented basic labeling functions including point, line, polygon and rectangle labeling, data exchange with the back end, temporary storage of labeling data, and prediction results. Now the front-end has supported users to load data from the back-end, annotate data on the front-end, store the annotated data in the back-end or get annotated data from the back-end, and display the back-end prediction results. 2.1.1Total view: 2.1.2Remote Panel:  Click the \"Remote\" button and a remote panel will pop up below. In the train module, click \"Start Train\" to start training, click \"Stop Train\" to stop training, and there is a scrolling training progress display animation below. In the image module, click \"Pre Image\" to switch to the previous image, and \"Next Image\" to switch to the next image. Click the \"Load Labels\" button to load the label from the back end. Clicking on the \"Generate Labels\" generates label images that can be trained. In the predict module, click \"Predict All\"to prediction all the images, click \"Predict\" to predict current images, and there is a list of models below to choose.2.1.3File List:  Click the file button, and the file list will pop up on the right. Click “选择⽂件” to select a dataset locally. Click the “Load Files\" button to load the remote cloud dataset. Scroll through the list of images and tap to select an image.\n2.1.4Labeling View:2.1.4Labeling View:  Clicking the “Tools” button will bring up a selection list for the Labelers. Among them are polygons, rectangles and lines. Select one to annotate on the image. Click the \"Edit\" button to modify the label. Click the \"Eraser\" button to erase any of the label points.\n2.1.5Save Labels:  Click the “Save” button and the save selection box appears. Click “Remote Save” to save the annotations to the cloud server. Click “Local Save” to save the label locally.2.1.6User Panel:  Click the \"User\" button and a user panel will pop up below. The user needs to log in, and the user info module displays the user's information after logging in. Click on the Online Users module to see other users who are online at the same time and their user information.\n2.2Back-end Design2.2Back-end Design 2.2.1Annotation Function  After the back-end receives the annotation information from the front-end, the back-end will store the annotation information from the front-end into the postgres database, and we will put the data into different tables in the database according to the annotation data types. Currently, the polygon data is stored in the polygon table, while the point and line segment data is stored in the point and line segment tables. The design of polygon table is shown above, divided into three columns which are id, imageid and label information . Id and label are one-to-one correspondence, and then each imageid corresponds to an image stored on the server. To establish the link between the image and imageid, we create the name table on the purpose of finding image by image id. Since we store the images in a folder, we can get the images stored on the server simply by the name of the image we need to store. When we have the imageid, we can get the image, and when we have the image path, we can find the corresponding imageid in the database and then find the corresponding annotation information.2.2.2Training Function  When the user has finished the annotation for one day, the user can click the training button to train the model. If the local data is not found in the database, then we know that the image is not labeled. After obtaining and generating the training labels, we feed all the image data and the labeled data into the training of the weakly supervised model, which is trained and ready to be used when the user performs the next labeling process.2.2.3Prediction Function  When the user needs to perform annotation based on the model's prediction, the user will click the predict button. When the predict button is clicked, the backend framework will call the previously trained weakly supervised model to predict the current image. The prediction result is converted into a set of sequential point coordinates and sent to the front-end. The predicted result is presented by the front-end. Since the weakly supervised model has not yet been implemented, our current display system calls the model with the trained fully supervised model UNET.3Framework and Functionality  This semester we update our architecture and features, the main change being that we will be updating our front-end architecture and doing new features on that architecture that the previous architecture could not accommodate. We will use Electron + Vue.js instead of html + js + css, the advantage of which is that we can separate the user interface from the server, in the last semester the user markup interface relied on the ssh connection between the client and the server, in this semester we will separate the user markup interface, the advantage of which is as follows. The first is that when we build our interface and client with electron+vue.js, we have the ability to annotate on multiple platforms. Users from Windows, Linux, or Mac can use the software to annotate. Secondly, we can complete the offline annotation function from the server, after the user annotation application is ready, we can complete the offline annotation function, so that if the data previously transmitted by the server is cached in the client, the user can annotate according to the cached image data, store the annotation information locally, and wait until we can communicate with the server to synchronize and update the annotation information. Thirdly, the function of simultaneous annotation by multiple users can be realized. When the client exists independently of the server, many users can work on their personal computers at the same time, and multiple clients can connect to the server at the same time. This way we only need to improve the synchronized update of annotation data under multi-user use to realize the multi-user simultaneous annotation function. In addition to these benefits of the new architecture, we also improve the communication between the client and the server. Users can communicate with the server in real time to obtain information about the current training situation and labeling progress, as well as real-time control of the server training model. This can solve the problem that the user can not reach the server to control the server state.4Model and Dataset 4.1Normal Weak-supervised Model/Algorithm Cross Pseudo Supervision[1]Cross Consistency Training[2]Mean Teacher[3]Deep Adversarial Networks[4]Uncertainty Aware Mean Teacher[5]4.2Dataset Tomey Iris DatasetOxford-IIIT pet Dataset[6]COCO Dataset[7]Pascal Voc Dataset[8]5Application Value and Prospect  This project focuses on solving the problem of \"difficult annotation\" in the field of medical images in deep learning, using weakly supervised and incremental learning techniques to subtly reduce the annotation threshold, which is conducive to the research and development of deep learning in the field of medicine. This project is based on iris annotation, but not limited to iris; based on OCT images, but not limited to OCT images. The annotation system implemented in this project can be applied to other scenarios as a more general tool. This project has completed the design of the overall front-end and back-end system architecture and the initial construction of the front-end and back-end. The front-end functions include the annotation of points, lines and contours in the image data. The back-end functions include the extraction and storage of annotated data in the database, and the construction, training, and prediction of deep learning models. The project has added the function of relying on iris feature points for deep learning training to the existing deep learning model, which is based on the location of the feature points as a priori knowledge to enhance the model training effect.6Future Work Schedule6Future Work Schedule  In mid to late May, we will continue to improve the front-end functionality of the software and try to implement the related multi-player collaborative tagging feature. After that, we will implement a variety of weakly supervised models, and if we have enough time, we will also improve on the models. Our list of models includes Cross Consistency Training, Mean Teacher, Deep Adversarial Networks, Uncertainty Aware Mean Teacher, and others. In early June, we will deploy the weakly supervised model and compare our experimental results. The results will include three time comparisons: using traditional data labeling, using weakly supervised model-assisted labeling, and using multiple people using weakly supervised assisted labeling. We will analyze the time differences of the approaches and the advantages and disadvantages of each.7Expected Outcome and Benefits  After users deploy the system to their servers, experts can annotate it on the web side. The results hit the pain points of medical image segmentation problems, as the system has the following features:A corrected annotation model designed for the problem of difficult annotation and automatically adds the corrected data to the training to reduce the burden of experts.A front- and back-end separation model designed for the problem of data security.A variety of weakly supervised algorithm models are integrated for data accuracy and precision defects to improve accuracy\n"}, {"role": "assistant", "content": "最终打分：7.8 (范围0-10分)\n1. 结构完整性得分：7, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：8.5, 占比20%\n5. 参考文献规范性得分：7, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "基于医生知识引导的眼底多疾病诊断模型研究\n侯伊林11912636\n黄晨露11910935\n 2022 年 6 月 8 日\n1 研究背景\n1.1 糖尿病性视网膜病变\n1.1.1 定义\n糖尿病性视网膜病变（DR）是慢性进行性糖尿病导致的视网膜微血管渗漏和阻塞从而引起一系\n列的眼底病变，如微动脉瘤、硬性渗出、棉絮斑、新生血管、玻璃体增殖、黄班水肿甚至视网膜脱\n离。\n在临床上, 通常根据是否出现新生血管,将DR分为两大类:增殖型糖尿病性视网膜病变和非增\n殖型糖尿病性视网膜病变.其中,非增殖型糖尿病视网膜病变依据患病程度又分为一级、二级、和三\n级。这里我们明确本研究提到的早期DR为非增殖性视网膜病变的一级，其表现为眼底出现少数微\n动脉瘤(MA)。在眼底彩照中，微动脉瘤表现为暗红色的孤立小圆点，它们的直径通常小于125um。\n1.2 临床诊断需求\n早期DR的诊断对医生专业水平要求较高，需要医生经过专业培训，经验积累才能做出较为准\n确的诊断。导致在大规模筛查中医疗资源紧张。最后，由于早期DR仅表现微动脉瘤，临床上漏诊\n率较高。因此，利用计算机图像处理技术对眼底图像进行检测，进行大规模筛查眼底病是一种有价\n值的方案。\n1.3 青光眼\n青光眼（glaucoma）是一组以视乳头萎缩及凹陷、视野缺损及视力下降为共同特征的疾病，病理\n性眼压增高、视神经供血不足是其发病的原发危险因素，视神经对压力损害的耐受性也与青光眼的\n发生和发展有关。在房水循环途径中任何一环发生阻碍，均可导致眼压升高而引起的病理改变，但\n也有部分患者呈现正常眼压青光眼。青光眼是导致人类失明的三大致盲眼病之一，它的疾病进程极\n为缓慢，在一段很长的时间后慢慢使得病人失去视力，因此，青光眼的早期诊断十分重要。\n青光眼的眼底表现为视盘中带红色神经组织的丧失，和视盘凹陷的增加，即中央白色的部位增\n大，以及视盘边缘有出血现象。\n1.4 老年黄斑变性\n老年黄斑变性(AMD)是一种影响眼睛后部视网膜(黄斑)中央区域的疾病。黄斑变性通常是高\n龄退化的自然结果，随着年龄增加，视网膜组织退化，变薄，引起黄斑功能下降。老年病变常常开\n始只影响一只眼睛，且早期症状不明显。随着病情的发展，会表现出中心视力模糊，有时候会出现\n黑斑或黑点，错误判断距离和高度，识别颜色困难等症状。AMD是世界上第三大最常见的致盲原\n因，也是人口老龄化的高收入国家的主要致盲原因。全球约有5%的失明是由AMD引起的。据估\n计，2020 年全球有1.96 亿人患有 AMD，2040 年将增加到2.88 亿人。\nAMD 的眼底表现为早期可见黄斑区色素紊乱，中心凹反光不清，有散在的玻璃疣。发病晚期，\n黄斑部可有金属样反光，视网膜色素上皮萎缩呈地图状，可见囊样变性。\n1.5 研究目的\n本研究通过眼动追踪技术获取医生诊断时的注意力，将医生注意力作为临床先验知识，设计一\n种知识注入型的神经网络模型，在眼底图像上实现对不同眼底疾病的精确诊断。\n2 前期工作\n2.1 医生注意力数据集制作\n本项目组在组内医生的辅助下，挑选了正常和早期DR的眼底彩照。我们将使用Tobii屏幕式\n眼动仪，记录医生诊断时的眼动注意力，以热点图的形式叠加在原图像并导出。\n2.2 眼动仪实验流程\n在本研究中，每一张热点图都由专业医生进行诊断，组内成员全程在旁辅助，使用眼动仪并对\n机器进行操作，进行对数据的导入导出。\n1. 设置眼动仪参数\n2. 将挑选后的原始眼底彩照按设置参数导入眼动仪。\n3. 人机相对位置标准化,受试者视线垂直于眼动仪显示屏，面部与屏幕相距55cm。\n4. 眼动仪校准，受试者配合视线紧盯屏幕上的运动圆点。\n5. 开始实验。实验中保证受试者不被干扰，眼动仪半径一米以内无人员，防止眼动仪测量出现\n偏差。\n6. 试验结束后，对可视化图像进行参数设定（如下），导出可视化图像（热点图）。\n热点半径 30px\n背景 透明\n透明度 100%\n 2.3 视觉误差处理\n在实验过程中，我们发现生成的热点图存在视觉误差，即医生的注视区域和真实病灶区域存在\n微小偏差。热点图的生成原理是利用高斯曲线对注视点周围区域进行拟合，我们可对该高斯分布关\n联区域的大小进行设定。将r=30px转变为60px, 使得热点区域可完全覆盖病灶区域。\n2.4 数据标准化处理\n2.4.1 收集 DR 数据\n在原有的组内采集的数据集的基础上，用公共数据集（kaggle 数据集）进行了补充，现有206\n张早期DR眼底图像，506张正常眼底图像。\n2.4.2 去除低质量眼底图像\n主要去除过曝、过暗和被污染的眼底图像。\n2.4.3 统一尺寸\n以眼球半径为基准，将所有图像调整至同一尺寸。对于每一张眼底图像，因为图像基本满足上\n下对称，所以我们取图像正中间一行，记录有颜色的像素个数，除以2即可得到半径。根据这个半\n径对图像进行缩放。\n2.4.4 去除黑边\n与2.1.3 的方法类似，找出每一张眼底图像黑边的范围，裁剪去除即可。\n3 使用U-Net 框架建模\n3.1 U-Net 架构\nU-Net 是一个 U 型对称的网络，常用来解决图像分割问题。U-Net借鉴了FCN网络，其网络\n结构包括两个对称部分：前面一部分网络与普通卷积网络相同，使用了 3x3 的卷积和池化下采样，\n能够抓住图像中的上下文信息（也即像素间的关系）；后面部分网络则是与前面基本对称，使用的是\n3x3 卷积和上采样，以达到输出图像分割的目的。此外，网络中还用到了特征融合，将前面部分下\n采样网络的特征与后面上采样部分的特征进行了融合以获得更准确的上下文信息，达到更好的分割\n效果。\n医疗图像的数量相对于自然图像要少，获得有临床意义的标签的难度也更大，所以用于医疗图\n像的网络模型机构不宜太复杂，否则易造成过拟合，影响模型效能。U-Net的模型参数可以通过调\n成模型层数、每层的通道数来缩小，与其他模型比做到轻量级，十分适用于医疗图像的训练。\n3.2 结合眼动图的U-Net 模型\n在本实验中，我们提出了U-Net 架构框架中的分类问题，并在瓶颈层增加了一个多类分类块。\nU-Net 的编码臂和瓶颈臂可以是任何标准的预训练分类器，不需要全连接层。这两个组合将作为分\n类器的特征编码器。该网络的CNN解码器部分运行反卷积层来预测静态眼球注视热图。该方法的\n优点是可以联合训练输出注视静态注视热图，并进行多分类预测。对于测试集的眼底彩照图像，输\n入一张眼底彩照原图到该网络，可以输出预测的诊断结果，并生成与疾病相关的最重要位置的概率\n热图。\n我们使用一个预先训练的EfcientNet-b0 作为编码器和瓶颈层。分类头为自适应平均池，然后\n是增肥层、dropout 层和线性输出层。该解码器CNN由三个卷积和上采样层组成。损失函数是由分\n类损失函数和分割损失函数加权得到，两者都采用了二元交叉熵损耗函数。\nLoss = w1 ∗L_classification + w2 ∗ L_segmentation\n 3.3 实验结果\n我们用训练好的模型导出关于训练集的CAM图与医生注意力图进行对比。图9中左图是医生\n注意力热图，图中有明显红色区域的三个部分都有病灶（微血管瘤）的分布，表明这几个区域在医\n生诊断该图片时受到了更多的光顾。右图是用训练好的模型导出关于训练集的CAM图，如图所示，\n在左上角一块区域热力图颜色明显更深，证明在训练网络过程中该区域的特征对训练网络的影响最\n大。这两幅图对比可以得出，两个热力图的共同点是都关注到了左上方的部分区域，说明我们的网\n络结构确实学习到了部分医生的注意力，从而帮助分类器分得正确的结果。\n经过调参后，模型的准确率最高达到了0.703.以0.4982为阈值，大于阈值时判定为1，小于阈\n值判定为0。分类的敏感性最高可以达到0.80，特异性最高可以达到0.617。\n4 评价标准\n本实验的卷积神经网络为二分类模型。我们定义诊断为 DR 为正样本，诊断为正常为负样本。\n我们利用以下几个指标对模型性能进行评判。\naccuracy =\n tp+tn\n tp+fp+fn+tn\n precision = tp\n tp+fp\n recall = tp\n tp+fn\n specificity = fp\n fp+tn\n f −score = (1+β2) precision∗recall / β2∗precision+recall\n 5 研究计划\n5.1 增加补充实验\n增加一组实验：用普通人注意力图注入模型。\n5.1.1 实验背景\n普通人的注意力图的特点：相比专业医生，普通人只能了解到眼底图像和疾病的基本知识，不\n具备诊断疾病的能力，所以观察眼底图像获得的注意力图会有较大噪声。\n5.1.2 结果预测\n实验分组：（a）不加入注意力图；（b）加入医生注意力；（c）加入普通人注意力。\n预想的实验结果：（b）要优于（a）和（c）。且由于（c）有较大干扰，（a）可能优于（c）或者\n和（c）准确度相似。\n5.1.3 实验进度\n已采集DR疾病的普通人注意力图共1200张，DR0和DR1各600张。采集注意力图所用的\n眼底图像与采集医生注意力图一样，其他采集流程也保持一致。\n5.1.4 实验计划\n（1）补充更多医生和普通人的注意力图，注入模型进行实验。\n（2）改进原有的医生注意力数据集：因为医生的注意力也会存在噪声干扰。可以利用普通人观\n察眼底图的特点，减弱这些干扰。\n（3）增加模型的实际作用：可以通过区分注意力图像，来判断一个人是否已经具备了区分DR0\n和DR1的能力，即是否可以被称为“医生”。\n5.2 研究进度和未来规划\n阶段\n研究进度\n第一次答辩\n文献、方法调研;\n完成DR注意力数据集整理;\n选择模型并设计实验思路;\n第二次答辩 将整理好的数据集用于当前模型进行分类;\n第三次答辩\n整理收集青光眼、黄斑变性等眼底数据集;\n继续对数据集做注意力标注;\n对当前模型进行改进和调试;\n由普通人进行注意力实验（组内成员），对比医生诊断图片和普通人\n诊断图片的注意力;\n"}, {"role": "assistant", "content": "最终打分：8.2 (范围0-10分)\n1. 结构完整性得分：8, 占比20%\n2. 逻辑清晰度得分：7.5, 占比20%\n3. 语言连贯性得分：8.5, 占比20%\n4. 内容独特性和创新性得分：9, 占比20%\n5. 参考文献规范性得分：7, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "图1.1.1\n图1.1.2基于超分辨的眼科病灶提取算法  ⼀、前期⼯作回顾 1.1 项⽬重点及模型结构  本项⽬的⽬标是提出⼀套基于深度学习的⾼效轻便的眼科病灶提取算法，研究重点为如何引⼊超分辨技术来缓解由于Resize导致的眼底图像分割精度下降的问题。简略流程如图1.1.1所示\n 在前期的⼯作中，我们搭建了超分辨分⽀与分割分⽀的融合模型，并在⾎管分割上初步达到预期效果，后续将继续在渗出分割上进⾏调整，提⾼⽹络在眼科病灶分割上的通⽤性。图1.1.2展示了模型的结构。\n图1.2.2\n图1.2.31.2 中期⼯作总结  在上⼀次的答辩中，我们主要针对渗出分割的难点，即类别不均衡和病灶位置不固定来调整⽹络。其中针对类别不均衡问题我们进⾏了两种类型的尝试，⼀种是图像预处理，尝试了对图像进⾏Patch，即裁剪成⼩图像后删除没有病灶的⼩图像来减少负样本数量。另⼀种是调整模型损失，尝试将模型中的分割和融合损失替换成CBCE Loss或Focal Loss。在进⾏⼀系列尝试之后，我们发现CBCE Loss能够在⼀定程度上缓解渗出分割类别不均衡问题，因此将模型中的分割和融合损失替换成CBCE Loss。\n 图1.2.2和图1.2.3分别展示了改进后的模型的消融实验结果和IDRid数据集上的可视化结果。\n图1.3.1\n图1.3.21.3 中期问题及解决⽅向  虽然模型改进后类别不均衡问题有所缓解，但我们可以看到，改进后的模型分割结果边界定位不准确，出现外扩问题（如图1.3.1所示）。我们猜测是由于病灶的边界的定位与周围区域存在⼀定的联系，⽽现有的分割损失基于逐像素进⾏计算，忽略了像素间的相关性，⽽未能准确地定位边界。因此中期答辩时我们提出的改进⽅向为引⼊基于区域的损失以最⼤化超分分⽀和分割分⽀的互信息。\n 同时，我们在中期答辩时将现有实验数据与SOTA进⾏⽐较（如图1.3.2所示），认为我们的实验结果虽然与SOTA还存在⼀定的差距，但我们的融合模型能够以更⼩的图像作为输⼊，同时达到较⾼分割精度。但是我们之前并没有做验证我们的模型使⽤更⾼分辨率图像作为输⼊能够达到更⾼分割精度的实验，在这次的⼯作中我们尝试设计实验进⾏验证，发现现在的模型backbone在⽬前有限计算资源下⽆法⽀持⾼分辨率图像输⼊进⾏训练，因此我们计划更换为轻量级的backbone进⾏实验。\n   ⼆、实验结果图2.1.1\n图2.1.2\n图2.1.3⼆、实验结果 2.1 基于区域的损失RMI Loss实验  基于上次答辩总结的⽅向，经过调研，我们调研到RMI Loss[1]。不同于⼤多数分割⽹络的损失，如CE Loss，是基于基于单个像素进⾏计算⽽忽略了像素之间的关系，RMI Loss ⽤⼀个像素及其周围的⼋个像素组成的区域来表示该像素，通过最⼤化预测值和ground truth的区域互信息来减⼩两者的差异。其区域表示及损失计算公式如图2.1.1和图2.1.2所示。\n 在之前⽹络的基础上（如图2.1.3），我们将融合模块损失调整为RMI Loss进⾏实验，如图2.1.4所示。\n图2.1.4\n图2.1.5\n图2.1.6\n 实验结果如图2.1.5所示。我们发现，分割效果在IDRiD数据集上有显著提升，在DDR数据集上不如先前模型。\n 我们⾸先就IDRiD数据集上的可视化结果进⾏分析，如图2.1.6所示，我们可以看到，将融合模块的损失换成RMI Loss之后，CBCE Loss过度惩罚负样本的问题在⼀定程度上得到了缓解（即将⿊⾊背景部分预测为病灶，也就是图中的红⾊区域）。\n图2.1.7\n图2.1.8\n图2.1.9\n图2.1.10 接着我们尝试分析加⼊RMI Loss后在DDR数据集上效果不如先前模型的原因，我们⾸先对DDR数据集进⾏分析，发现DDR数据集中⽆病灶图像较多，如图2.1.7所示，DDR数据集训练集中总图像有383张，其中138张是没有病灶的。同时我们也发现DDR数据集图像存在⿊⾊边框，如图2.1.8所示。这两个特点使DDR数据集相⽐于IDRiD数据集来说拥有更多负样本，⽽RMI Loss基于区域进⾏计算，在⿊⾊背景上预测值与ground truth易迅速趋于⼀致，导致RMI Loss在训练中迅速降低，影响⽹络对病灶区域的判断。\n 因此，我们对DDR数据集中⽆病灶的样本进⾏删除，并裁剪图像⿊边部分进⾏实验，得到的实验结果如图2.1.9所示。我们可以看到，虽然相⽐与没有处理的数据集上进⾏的实验来说，分割效果是有所提升的，但相⽐于先前模型来说，IoU仍然是下降的（图2.1.10）。\n 我们进⼀步对处理后的数据集上的分割可视化结果进⾏分析，如图2.1.11所示，我们可以看到CBCE Loss过度惩罚负样本的问题在很⼤程度上得到缓解，但边缘定位仍然不是很准确。\n图2.1.11\n图2.1.12 我们基于DDR数据集和IDRiD数据集的对⽐进⾏了进⼀步的分析（如图2.1.12所示），发现相⽐于IDRiD数据集来说，DDR数据集图像中病灶区域较⼩，因此在RMI Loss计算以区域为单位的损失时受到周围⿊⾊像素影响更⼤。\n 2.2 backbone 调整实验  \n调整原因\n原始的 Unet 模型加上超分辨分支之后，模型参数量比较大，限制了训练时输入图像的分辨率，项目中输\n入图像的分辨率较低，而低分辨率的图像由于图像下采样倍数较高，损失的细节信息较多，在此基础\n上，引入超分辨来补充细节信息为分割所带来的提升是比较有限的。因此本项目希望可以进一步提高分\n辨率来增加超分辨信息引入所带来的提升。然而，现有模型无法支持更高分辨率的输入图像进行训练，\n因此项目对 backbone 的模型大小进行调整，降低模型参数量，从而提高输入图像的分辨率，以验证提\n高图像分辨率，能够提高分割的准确性。\n调整方式\n项目对于 Unet 的调整是将其每一层的特征图的输出通道数减半。\n实验结果\n图2.2.1 展示了在调整后的轻量 Unet 上，不同输入大小下模型的测试结果，在 IDRID 数据集上，项目将输\n入图像的分辨率提升到 720x480 ，可以看出增大图像分辨率后，模型的 IoU 提升比较明显，将近 3 个点。\n                      图2.2.1\n实验进一步将当前的最高测试结果与 sota 进行比较，如图 2.2.2 所示，当前模型的测试结果与 sota 仍然存\n在一定的差距，但是也可以发现，当前模型的输入图像仅为 sota 的一半，由上述实验结果可知，提高输\n入图像的分辨率能够提高分割的准确性，因此项目希望在此 720x480 这一分辨率上进一步提高分辨率，\n但是在轻量的 Unet 上可以达到的最高输入分辨率依然比较小，因此项目希望将现有的 Unet backbone\n更换为更加轻量的 backbone ，从而进一步提高输入图像的分辨率，达到接近 sota 的水平。\n                      图2.2.2\n因此项目对于轻量的分割 backbone 进行了调研，调研到一篇比较适合引入超分辨的分割网络\nSTDCNet[2]，图 2.2.3 为该模型的结构图，这个模型轻量且高效，有突出的两个特点，一个是在分割网\n络中引入了双分支结构，使用 Context info 以及 Spatial info 两个分支分别提取图像的语义以及空间信\n息，并使用一个 FFM 模块来融合这两方面的信息。第二个特点是在 Spatial info 分支上引入了 detail  \nhead 用来生成一个 Binary_Ground-truth 来学习细节信息。                      图2.2.3\n该网络于项目模型有两处类似的地方。一是模型框架相似，都使用了双支路的结构，用来学习不同的图\n像信息。二是网络的 Spatial info 分支与项目模型的超分辨分支作用十分类似，都是用来为分割分支补充\n细节信息，从而得到精度更高的分割结果。因此，项目认为，在该网络中引入超分辨技术是比较合适\n的。项目已经在该网络中进行了 baseline 实验，如图 2.2.4 所示，后续将在如何将超分辨技术引入该网络\n方面进行探索与尝试。\n                         图 2.2.4\n \n三、学期总结  \n本学期的工作主要围绕三个重要问题展开，并逐一针对三个问题对模型的结构进行调整。\n重要问题\n1. 对超分辨与分割的交互类型进行总结，发现在医学影像中两个分支交互区域的特点，分割只关注病\n灶区域，而超分辨关注全图。\n2. 对渗出数据集的分割难点进行调研，发现图像存在病灶区域只占极小一部分的问题，存在类别不均\n衡的问题\n3. 发现当前模型的分割准确性受限于训练时输入图像的分辨率，需要验证模型在增大训练时输入分辨\n率会，准确性能够提升。\n模型调整\n1. 针对医学影像中两个分支交互区域的特点，对模型的融合模块进行改进，引入空间注意力机制，提\n取两个分支共同关注的区域进行交互。\n2. 对损失函数进行调整，引入两种类型的损失。一是使用 CBCE Loss 缓解类别不均衡的问题，二是\n使用 RMI Loss 来最大化两个分支的区域互信息，对于边界进行限制，提升边界定位的准确性。3. 减小 backbone 大小，从而增大输入图像的分辨率。进行了两种尝试，一是在原始 Unet 上进行调\n整，将每一层特征图的输出通道数减半，提高了输入图像的分辨率，得到更高的分割精度，以 sota\n二分之一的输入图像分辨率，达到接近 sota 水平。二是为了进一步提高输入图像分辨率，对于轻量\n模型进行调研，最终确定在 STDCNet 上进行研究，已经完成 baseline 实验，后续则探索如何将超\n分辨技术引入 STCNet 中。\n \n \n"}, {"role": "assistant", "content": "最终打分：7.3 (范围0-10分)\n1. 结构完整性得分：6, 占比20%\n2. 逻辑清晰度得分：7, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：8.5, 占比20%\n5. 参考文献规范性得分：6, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "Cataract Surgical Analysis Based on Cross Domain Data\nMingyang Ou, Zemin Wu, Zihan Ma\nI. INTRODUCTION\nSemantic segmentation is one of the classic tasks in\ncomputer-aid system for surgical scenarios. Pixel-wise an-\nnotations separate boarderlines between anatomy structure\nand instruments, helping clinicians make better decisions\nand operations. In recent years, deep neural networks(DNNs)\nbecome the leading trend for settling semantic segmentation\ntask. However, 1)mass training data and heavy manual anno-\ntation works make generating datasets difficult, especially in\ncataract surgery. Meanwhile, 2)performance of semantic seg-\nmentation model drops due to cross-domain data in cataract\nsurgery scenario. To desirably settle this two problems, a\ndomain adaptation model is proposed combining the features\nof spectrum analysis, FFT domain randomization and multi-\nview learning.\nII. BACKGROUND\nA. Cataract\nCataract is a common ophthalmic disease. The devel-\nopment of its related knowledge and treatment needs our\nattention. Cataract is a common blinding eye disease. The\nlens in the eyes of normal people is transparent, and the light\nreaches the retina smoothly, so that people can clearly see\nexternal objects. The loss of vision caused by lens opacity is\ncalled cataract. According to the statistics, 33% of the visual\nimpairment and 51% of the blindness in the world come\nfrom cataracts that are not treated in time. According to the\nophthalmology branch of Chinese Medical Association, the\nincidence rate of cataract among people aged 60 to 89 is\n80%, while the incidence rate of cataract is higher than 90%\nin people over 90 years old.\nFig. 1. The eyes with cataract\nAt present, surgery is still the only effective method\nto treat cataract. In 1967, Charles Kelman invented cataractphacoemulsification, which developed cataract surgery from\ngeneral vision restoration surgery to refractive surgery with\nhigh quality, high efficiency and high satisfaction. Since then,\ncobble optimized the surgical process, greatly reduced the\nendothelial damage, and inspired many doctors to develop\nspecial surgical techniques and skills suitable for cataract\nwith different machine modes, different hardness, different\npatients and surgeons. There is still a long way to go for\nthe progress of surgery. In order to successfully implement\ncataract surgery and effectively promote the recovery of\nphysiological visual function, it is necessary to continuously\nimprove the surgical incision, capsulorhexis shape, operation\nefficiency and intraocular lens design, so as to restore the\npatient’s ocular physiological function. In the process of\ncataract surgery, surgeons need to operate a variety of pre-\ncision surgical instruments to cut cornea, emulsify, remove\naging crystals, inject intraocular lenses and so on. However,\ndue to the similarity of light, blood and surgical instruments\nin the surgical scene, the surgeon is prone to misjudgment\nand there is a risk of eye damage to the patient. Therefore,\ncataract surgery requires the surgeon’s rich experience.\nB. Semantic Segmentation and Domain Adaptation\nSemantic segmentation can help doctors avoid some\nmisjudgments to a certain extent. Semantic segmentation\nprocesses the cataract surgery scene, divides the surgical\ninstruments, pupil, iris, cornea and other areas, and assists\nthe surgeon in precision operations such as lens removal.\nAt present, many semantic segmentation algorithms based\non cataract data have good results. However, in clinic,\nthe performance of these algorithms cannot play their best\nperformance because they cannot deal with the diversity of\nsurgical scenes, that is, the generalization ability is insuffi-\ncient. Semantic Segmentation\nFig. 2. Semantic Segmentation\nThe current research challenges are roughly divided into\ntwo categories. First, deep convolution network has attracted\nextensive attention because of its good performance in se-\nmantic segmentation. However, the performance of deep\nnetwork driven by data is based on a large number of training\ndata and manual marking, and the cost of data acquisitionis high. Second, AI algorithms are greatly affected by the\ndiversity of clinical data. After a large amount of data learn-\ning, the deep neural network may not generalize the learned\nknowledge to the real application scenario, that is, there is\na domain shift problem (data distribution drift problem).\nEven subtle differences may cause significant performance\ndegradation of the model.\nThis requires the use of domain adaptation and the full\nuse of cross domain data to train the model. Through\ndomain adaptation, the model has better robustness in clinical\napplication. Domain adaptation is a field of computer vision,\nwhere our goal is to train a neural network on a source\ndataset and secure a good accuracy on the target dataset\nwhich is significantly different from the source dataset.\nIII. P ROBLEM DEFINITION\nA. Definition\nBased on the background investigation, two primary prob-\nlems are to be settled. 1)First, the cataract surgery data\navailable for analysis is insufficient. Specifically collecting\ncataract surgery data and giving manually annotations are\ncostly. 2)Second, the semantic segmentation algorithms for\ncataract surgery has the problem of domain shift. Such\ndomain shift makes semantic segmentation model degenerate\nin clinical cataract surgery scenarios.\nB. Solution\nOur solution is to designed a cataract surgery semantic\nsegmentation neural network with domain adaptation ability.\nSuch network can train with unlabled data, which solves\nthe first problem(expensive workload to collect and annotate\ncataract surgery data). At the same time, the model could\nmake full use of cross-domain data and show robust per-\nformance on inputs from other distribution, settling problem\n2.\nC. Procedure\nProcedure of our solution implementation can be conclude\nin two steps:\n•We first design a semantic segmentation model for\ncataract surgery. We proceed domain adaptation on two\nrealistic cataract surgery datasets, one with pixel-level\nannotations(source domain) and one without(target do-\nmain). Expected outcomes would be model is trained on\nsource domain with annotations and have good quality\nsemantic segmentation on target domain. Once we make\nexcellent result, our semantic segmentation model has\ngood generalization ability, therefore problem 1 could\nbe solved.\n•Then we try to carry out domain adaptation task from\nsynthetic dataset to realistic dataset. If we get com-\npelling result, we could use abundant synthetic data for\ntraining which are paired with annotations and apply\nthe trained model on realistic surgery scenario withouts\nperformance degradation.IV. F LOW CHART\nProcedures to settle the problems are concluded in\nfollowing flow Fig. 3. Since problems are defined based\non previous background research, necessary related works\nare needed to give inspiration for solutions. Then a basic\nmethodology should be proposed, following by verification\nof feasibility of each components. Dataset preprocessing is\nalso essential for experiments like comparison experiments or\njust quantified measurement. Afterwards, model integration\nand baseline experiment should be carried out. Comparison\nexperiment help us fine tune our model and if our model\nshows convincing domain adaptation results, we can have\nfurther exploration. As showing in the flow chart, blue part\nis what we have done this period.\nV. R ELATED WORKS\nThree papers are presented in detail,which includes\nthe main idea of our model. [1]introduce a method that\nimproves the unsupervised domain adaptation, which is by\nswapping the low-frequency spectrum of source and tar-\nget the difference between them is reduced. We can get\na image that has the object of target with the style of\nsource. [2]describe Frequency space Domain Randomiza-\ntion. It first converts the images into multiple frequency\ncomponents(FCs) with discrete cosine transform; divide FCs\ninto two groups: domain-invariant FCs(DIFs) and domain-\nvariant FCs(DVFs); then keep the DIFs and randomizing\nDVFs. It can save the origin semantic structures, while the\noutcome image has different style. [3] Class-level adversarial\nnetworks (CLAN) are introduced, which aim to address the\nsemantic inconsistency caused by global feature alignment\nduring unsupervised domain adaptation (UDA). The model\nin the paper consists of a generator G and a discriminator\nD. After co-training, the weights of the two classifiers in the\ngenerator are different, providing us with different semantic\npredictions.\nVI. M ETHODOLOGY\nOur methodology are separated into three parts.\n•First, we identify domain invariant and variant fre-\nquency components of source domain using spectrum\nanalysis.\n•Second, we replace domain variant frequency compo-\nnents of source domain with corresponding frequency\ncomponents of target domain using FFT domain adap-\ntation method.\n•Finally, we input the resulting source domain images\nwith target images to multi-view adversarial model to\nconduct domain alignment.\nA. Spectrum analysis\nSince filtering out the domain invariant can enhance the\neffect of domain adaptation. We first transform the source\ndomain image to the frequency domain space through the\nDCT transform (Discrete Cosine Transform).Fig. 3. Flow Chart\nThen for the transformed frequency domain space, we di-\nvide it into 128 frequency band FCs (Frequency Com-\nponents).Each group of frequency bands has part of the\ninformation of the input image, and we use band-stop filters\nto filter different groups of frequency bands respectively.\nThe remaining frequency bands will be used as input to\nthe classification model for training. Since different fre-\nquency bands carry information of different input images, the\nclassification performance of the model will decrease after\nthe frequency bands containing important domain invariant\nfeatures are filtered out. Therefore, the trained models have\nperformance differences in classification tasks. By comparing\nthe worst performing models we filter out frequency band\ncorresponding to the domain invariant.\nB. Frequency Domain Randomization\nWe filter out the corresponding frequency bands of multi-\nple groups of domain invariant, and then adopt the method\nof domain randomization to integrate the style target do-\nmain image into the source domain image.Same as the first\npart.First, both the source domain and the target domain\nare projected into the frequency domain space. Then the\ncorresponding frequency bands of the domain invariant fea-\ntures are excluded, the remaining frequency bands of the\nsource domain and target domain are randomly selected for\nreplacement, and finally the inverse FFT transform is used\nto obtain source domain images fused with target domain\nstyles.\nC. Multi-View Adversarial Learning\nThe adversarial neural network adopts the structure of the\ndiscriminator on the basis of the ordinary neural network,\nand the discriminator is trained so that it can distinguish\nthe input of the source domain or the target domain.The\ndiscriminator identifies the characteristics of the source or\ntarget domain and the generator reduces the domain shift\ncharacteristics of the source domain and the target domain to\nform a confrontation relationship, constraining the generator\nto learn domain-invariant features.The multi-view learning\nmethod utilizes the characteristics of the dataset with multi-\nple attribute sets, trains multiple classifiers at the same time,\nimposes difference constraints and requires them to have thesame segmentation effect to enhance the performance of the\nmodel.\nThis project envisages combining the adversarial idea of\nadversarial neural networks and the advantages of multi-\nattempt learning, alternately inputting source domain and\ntarget domain images, training an improved dual-classifier\nsemantic segmentation model based on the classic segmen-\ntation model U-Net, and convolution neural network is used\nas a discriminator constraint to train a cataract surgery image\nsegmentation model with high generalization ability and\naccurate segmentation.\nVII. D ATASET\nDataset name # of images # of categories\nCADIS(training) 3550 36\nCADIS(validation) 534 25\nCADIS(test) 586 28\nInSegCat(training) 237 13\nInSegCat(validation) 61 13\nInSegCat(test) 88 13\nCataract surgery game(training) 6890 17\nCataract surgery game(validation) 1200 17\nCataract surgery game(test) 600 17\nTABLE I\nDATASET OVERVIEW\nA. Domain adaptation between realistic datasets\nFor the first step, we evaluate our proposed model on two\nchallenging cataract surgical datasets. The two datasets are\nCADIS[4] and InSegCat[5] as we introduce from Table I.\nFig. 4. Phases of cataract surgery\nWe treat CADIS dataset as source domain data where\nannotations are accessible, while data from InSegCat datasetstend to be the target domain data which we assume it does\nnot contain ground truth label. Phases of the cataract surgical\nprocess are in Figure 4 and comparison of images from\ntwo datasets in the same phases are showed in Figure 5.\nFrom Figure 5, we can tell that styles including bright-\nness, saturation and etc. are distinguished between the two\ndatasets. In other words, domain shift exits between the two\nrealistic datasets, which lay research foundation for further\nexperiments.\nFig. 5. Comparison of images of two different domains\nB. Domain adaptation from synthetic data to realistic data\nWe already generated a datasets from a cataract surgical\nvideo game. We decompile the cataract surgical game with\nthe help of decompiler JPEX. We replace the color of vector\nimages of surgical tools and generate both images and masks\nof surgical scenarios using JPEX. For the second problem\nsetting, we replace CADIS with the synthetic dataset to be\nthe source domain.\nC. Data process of the dataset\nThe label of InSegCat dataset only contain surgical instru-\nments, comparing to other datasets(Figure 6), it is imperfect.\nIn order to have more intuitive display of segmentation\nperformance, to calculation of segmentation loss, and to\nprepare for follow-up comparative experiments, we relabeled\nthe InSegCat. With the help of the doctors in the research\ngroup, we learned the relevant knowledge of cataract surgery,\ndivided the labels into 15 categories, added the pupil and iris,\nand classified and supplemented the surgical instruments.We\ntry to lable it using Pair with the new 15 lables. Thus, we can\nalign labels of the two datasets and can use the dataset which\nhas small number of data as the test dataset. There are two\nmajor step of the relabel progress to deal with difficulty of\nit:\n•Improve the contrast of the picture to have better clear\nboundaries between different domain.\n•Review the surgery video to differentiate similar surgi-\ncal instruments.\nFig. 6. Label comparison between CADIS(left) and InSegCat(right)VIII. M ODEL OPTIMIZATION AND INTEGRATION\nExisting work world be the verification of each module. At\nthis term, we first make optimization on each module. After-\nwards, modules are integrated and compares with other se-\nlected domain adaptation module for semantic segmentation\nto test the performance of our model. Here we demonstrate\nhow we make optimization of each module and integrate\nthem into a complete model.\nA. Spectrum Analysis\nSince last term, we have already determined that frequency\ncomponents (3, 128] contains more DIFs, we did not make\nmuch improvements this term.\nB. Frequency Domain randomization\nAs stated, we replace frequency components of source\ndomain images with components of target domain images.\nWhile in previous implementation, we used 5 particular\ntarget images for randomization and stored the newly gen-\nerated dataset in local. Drawbacks would be such 5 specific\nimages could not properly emulate the distribution of the\ntarget domain. As a result, the generated new source domain\nimages could not transmit characteristic of target domain\nwell. We come up with the solution of dynamically swapping\nfrequency components in the process of training. Frequency\ncomponents of each source domain image are swapped\nwith one randomly selected target domain images before\nsent into neural networks. In such way, each image from\nsource domain has possibility to merge characteristic of every\ndomain image, greatly increasing randomness.\nC. Multi-View Adversarial Learning\nDue to the limitation of computing resources, our batch\nsize ranges from 1 to 3. [6] shows that model is sensitive to\nbatch size when it applies batch normalization, where small\nbatch size could lead to increase of error rate(performance\ndrop). Therefore, to reduce the effect of small batch size. We\nuse instance normalization instead.\nAfter refinement of each module, we integrate them to be\na whole segmentation model for cataract surgery scenarios\nwith domain adaptation ability, as shown in Fig 8\nIX. C OMPARISON EXPERIMENTS\nComparison experiments are necessary and essential to\ngive evaluation of model performance. Therefore, we select\n2 semantic segmentation models with domain adaptation\nability to compare with our model.\nA. Model Selection\nTwo domain adaptation models are selected: Fourier\nDomain Adaptation(FDA) [7] and FCNs in the wild [8].\n1) FDA domain adaptation model holds the idea of low\nfrequency contains more style-related information and style\ndifference is one of the causes of domain gap. Therefore,\nusing Fast Fourier Transformation(FFT) to swap low fre-\nquency components of source domain and target domain\ncould transfer style of target domain to source domain andFig. 7. The data after label\nFig. 8. Model Overview\nconsequently enhance semantic segmentation performance\non target domain. 2) FCNs in the wild reduce domain shifts\nin direction different from FDA. Using an encoder, input\nspaces of source domain and target domain are cast into\nfeature spaces. FCN in the wilds uses adversarial loss to\nreduce distance between source domain features and target\ndomain features, therefore improving segmentation results on\ntarget domain.\nB. Metric Selection\nMetrics should also carefully selected as they should\nobjectively reflect the capability of models. Here Pixel\nAccuracy(PA) and Frequency Weight Intersection over\nUnion(FWIoU) are chosen. Pixel accuracy are calculated\nsimply by dividing number of pixel correctly predicted by\ntotal number of pixels:\nPA =PC\nn=0pnn\npN(1)\nSuch metric indicates the overall accuracy of prediction of\nthe model. Meanwhile, FWIoU is a bit more complicated.Formula of FWIoU is given by:\nFWIoU =1\npN·CX\nm=0pmmPC\nn=0pmn+PC\nn=0pnm−pmm\n(2)\nwhere pNrepresents total number of pixels, Crepresents\nthe number of categories and pmnis the number of pixels\nof class mthat are predicted as nby the model. Accord-\ningly, the evaluation metric assigning frequency weights to\nclasses is able to comprehensively assess the segmentation\nperformance. While PA evaluates model on a more general\nperspective, FWIoU introduces more category accuracy of\nthe model. Therefore, we can gain impersonal insight into\nthe capability of models.\nC. Results and Analysis\nVisualization of our comparison experiments are demon-\nstrated above. For each input image, structure can be di-\nvided into foreground and background, where foreground are\nmajorly surgical tools and background are mostly anatomy\nstructures. Comparing with FDA and FCNs in the wild, our\nmodel achieve superior segmentation performance on mostFig. 9. Visualization of Comparison Experiment\nModel Pupil Iris Cornea Skin FWIoU PA\nFDA 75.49 49.49 52.96 46.19 51.28 64.54\nFCNs in the wild 67.96 31.13 53.72 54 50.25 65.71\nOurs 75.79 48.87 64.08 54.17 57.78 73.11\nModel Pri.Knife Pha.Cystotome A/I.Handpiece Lens Injector Cannula Eye Retractor\nFDA 3.87 0.73 4.47 1.13 20.55 14.21\nFCNs in the wild 15.18 1.92 6.07 1.28 13.76 20.27\nOurs 5.01 3.88 3.64 1.47 16.28 31.99\nTABLE II\nQUANTIFIED RESULTS\nof the background categories such as iris, pupils, cornea and\nskins. The segmentation masks are more consecutive and\ncomplete. For foreground surgical tools, our models does\nnot overwhelmingly surpass the other two models. Tools like\nCannula and Micro-manipulator can be carefully segmented\nby our model, while other tools like primary knife and sec-\nondary knife are clearer in segmentation results of the other\ntwo models. Quantified results are also listed in Table II.\nFor each category, we calculate intersection over union(IoU)\nand then we calculate FWIoU and PA for every model. The\nfirst three rows show iou of background categories and the\noverall performance of model evaluated by PA and FWIoU,\nwhile the last three rows demonstrate the foreground surgical\ntools. Our model attains superior performance on most of\nthe background categories while for foreground categories,\nour model does not always do the best. Such quantitative\nresult matches the observation in visualization of comparison\nexperiments. However, our model still get the best score both\nin PA and FWIoU. Therefore, we can confidently claim that\nour model is capable carrying out semantic segmentation task\nof cataract surgical scenarios over domain gap.X. F URTHER EXPLORATION\nIn comparison task, we prove that the proposed domain\nadaptation model for cataract surgical scenario semantic\nsegmentation can have convincing performance. However,\nwe also found that, there is still much space to improve. As\ndiscussed before, our model does not have clear segmentation\nof foreground categories, normally surgical tools. After ade-\nquate research and discussion, we conclude the two possible\ncauses and provide solution respectively.\na) Pixel level adversarial learning VS Feature level\nadversarial learning: Shown in visualization images, FCNs\nin the wild distinguish surgical tools than our model does.\nDifference between the two models is that our model applies\nadversarial learning on output pixel space, while FCNs in\nthe wild applies on feature space such that the model focus\nmore on the similarity between extracted features from two\ndomain instead of output masks. Therefore, model in FCNs\nin the wild grasp more knowledge of surgical tools than\nour model does. Base on this idea, we try to combine both\npixel level adversarial learning with feature level adversarial\nlearning. Results show improvements than our original mod-\nels. Which could segment both foreground and background\ncategories(Fig 10).Fig. 10. Improvement I\nFig. 11. Improvement II\nb) Adversarial learning VS Cycle Consistency: The\nother reason might be the difficulty of training adversarial-\nbase network. Adversarial-base networks are well known\nfor hard to train. Therefore, we can not always find the\nideal solution for our model. Solution could be replacing\nadversarial-base learning with cycle consistency learning.\nThe methodology is that if our model can have proper se-\nmantic segmentation, then based on the segmentation mask,\nwe can train a constructor to reconstruct the input image.\nSuch method is easy to train and shows potential of properly\ndistinguishing surgical tools(Fig 11).\nXI. C ONCLUSION\nIn cataract surgery scenarios, semantic segmentation aids\nsurgeons with better decision making and operation. How-\never, existence of domain gap(domain shift) leads to de-\ngrading of semantic segmentation models. To solve this\nproblem, a domain adaptation model is proposed. Spectrum\nanalysis and FDA frequency domain randomization transfer\nstyle of images from target domain to images from source\ndomain. Then, a semantic segmentation model with multi-\nview adversarial learning method is trained on the gener-\nated input images and target images reducing the effect of\ndomain shift. Comparison experiments are conducted and\nsuperior performance of proposed model is proved. Further\nexploration is also carried out, pointing out possible further\nimprovement of the model.\n"}, {"role": "assistant", "content": "最终打分：9.2 (范围0-10分)\n1. 结构完整性得分：9.5, 占比20%\n2. 逻辑清晰度得分：9, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：9, 占比20%\n5. 参考文献规范性得分：10, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "2022\n基于跨模态学习的甲骨\n文识别系统的研发\n指导老师：刘江章晓庆\n组员：桂宸麓孙清扬1一、研究背景\n甲骨文是早在商周时期就已出现的文字，是世界上最古老的文字\n之一，也是中国及东亚已知最早的成体系文字。甲骨文的研究不仅对\n了解中国文字起源与演化十分重要，而且对中国乃至全世界文字文化\n的溯源与传承有非比寻常的意义。甲骨文的发现不但促进了世界对于\n人类早期文字的研究，也促进了对甲骨文研究的“甲骨学”学科的诞生。\n刻有甲骨文的龟甲于1899年被清末官员、金石学家王懿荣首次发现，\n这一重大发现随即引起中外学术界轰动。百年间，曾有数十次以甲骨\n文相关历史材料为目标的考古发掘活动。迄今为止，作为甲骨文主要\n发掘地的河南省安阳市殷墟已经出土了超过15万片的字甲骨[1]。2017\n年12月26日，甲骨文成功入选“世界记忆名录”。甲骨文的重要性已\n经得到中国乃至全世界的公认。\n图1博物馆甲骨展品\n尽管甲骨文自被发现之后，其重要性得到广泛承认，其衍生的甲\n骨学发展迅速。然而作为距今3000年的文字体系，甲骨文字符的含2义仍然较难破译。在出土的15万余片甲骨中，大约包含4500个不同\n的甲骨字符，然而仅有2000个左右被成功破译解读[2],破译量不足二\n分之一。目前甲骨文识别主要依赖于学科专家的识别工作，往往需要\n专家有丰富的甲骨文有关知识与经验。这种专家识别方式的成本较高\n且效率较低。而对于大众来说，目前大部分普通人对甲骨文认识较为\n浅显，仅仅停留在它作为文字符号的层面上，很少了解文字涵义。这\n种现象的主要原因是大众很少受到甲骨知识熏陶。\n图2中国文字博物馆面向社会面征集甲骨文破译成果的公告\n近年来，人工智能技术已经成功应用于很多领域，如图像识别和\n自然语言处理。本项目拟与考古专家合作，基于多模态甲骨文图像和\n利用人工智能技术研发一个先进的跨模态甲骨文智能识别系统，加快\n考古专家识别未破译甲骨文效率并普及甲骨文的大众教育，具有较高\n的研究价值和应用意义。3二、文献调研\n经过调研，发现多篇使用人工智能辅助汉字识别的研究。基于这\n些研究使用的技术与方法，可大致分为利用机器学习进行文字识别与\n利用深度学习进行文字识别两类。\n就机器学习进行文字识别的工作而言，刘永革等人《基于SVM\n的甲骨文字识别》[3]颇具代表性。该研究选择使用支持向量机SVM完\n成汉字分类任务，训练集包含1300张图片，15个单字字符，其识别\n流程如图3。然而，这项研究的局限性也比较明显。其一，其数据集\n的样本量较少，且均来自甲骨摹本，是一个单模态的数据集；其二，\n其分类任务较简单，实验结果不具有复杂情况下的普适性；其三，该\n研究未使用深度学习方法，深度学习方法实现简单，识别精度高，目\n前在文字识别领域被广泛使用。\n图3《基于SVM的甲骨文字识别》识别流程图4对于使用深度学习方法进行文字识别的工作，刘芳等人的《基于Mask\nR-CNN的甲骨文拓片的自动检测与识别研究》[4]较为新颖。该研究选\n取约3000张甲骨文拓片建立数据集。为了丰富数据集样本容量，该\n研究应用了一些简单图像变换以对原图像处理，得到新的训练与测试\n样本。该研究基于MaskR-CNN建立识别模型，并应用TripletLoss\n等算法对识别效果进行优化，其识别流程如图4。然而该研究也有一\n定局限性。其一，其同样存在数据集样本量不够大的问题；其二，数\n据集样本不均衡，常用字出现频率较高，而生僻字仅包含个别样本；\n其三，其数据集均采用甲骨拓片来构建，因此，该研究建立的是单模\n态的神经网络。\n图4《基于MaskR-CNN的甲骨文拓片的自动检测与识别研究》识别流程图5三、项目计划\n（一）研究内容\n本项目拟基于甲骨拓片与甲骨摹本图像，并利用人工智能技术研\n发一种跨模态学习的甲骨文智能识别系统，用于普及甲骨文教育和辅\n助考古专家识别尚未破译的甲骨文。\n拟基于以下三个研究内容来构建基于跨模态学习的甲骨文智能\n识别系统:\n1.跨模态甲骨文数据库构建\n目前甲骨文字图像可以分为甲骨摹本图像与甲骨拓片图像两类。\n甲骨拓片指使用宣纸与墨汁，将龟甲、兽骨上的纹路、图样拓印下来\n的一种技术。拓片作为我国一项古老的传统技艺，是千百年来传承我\n国文化艺术的一种载体。\n甲骨摹本指甲骨学者手工临摹甲骨纹路得到的摹本。这种保存形\n式较甲骨照片更为清晰，修复了甲骨文图像的残缺与噪声。然而甲骨\n摹本需要较多人力物力得到摹本，同时对参与临摹的学者专业水平要\n求较高。\n由于缺少训练样本，仅使用甲骨拓片图像对模型进行训练很难取\n得较高的识别精度[5]。同时,由于甲骨摹本具备一些甲骨拓片文字类别\n没有的训练样本,甲骨文字的增量识别也是辅助甲骨文字专家进行语\n言研究的重要手段。因此，本研究构建了一个基于甲骨摹本与甲骨拓\n片的跨模态甲骨文数据库，从而提高模型的识别精度。6\n图5甲骨摹本（左）与甲骨拓片（右）\n2.智能甲骨文识别框架\n根据常识，汉字的偏旁一定程度反映了汉字的含义，且人类在认\n识汉字时根据偏旁进行分类，如“芷”、“芬”、“芳”等字具有相同的偏旁，\n且都与花草相关。同样地，面对不认识的汉字，人类可根据偏旁推断\n含义，如“芅”。虽然该字较为生僻，但仍可根据其偏旁推知其与植物\n有一定相关性。人类在观察汉字时先注意到较重要的偏旁，以此辅助\n识别汉字，这启发了本研究在建立识别模型时使用注意力机制。\n人类能够自然而有效地发现复杂场景中的显著区域，如图6。基\n于这一观察，注意力机制被引入计算机视觉，目的是模仿人类视觉系\n统的这方面，将注意力转移到图像最重要区域而忽略无关部分的方\n法称为注意机制。注意力机制基于数据域可以分为通道注意、时间\n注意、空间注意、分支注意，空间时间注意、空间通道注意[6]，如\n图7。7\n图6人类的视觉注意力\n图7注意力机制基于数据域的分类\n3.智能甲骨文识别系统\n图8是甲骨文识别系统的实现流程。识别系统主要由4部分组成：\n（1）图像上传模块：用户将待识别的甲骨文图像进行上传。8（2）图像处理模块：将图片去噪及尺寸归一化。\n（3）图像识别模块：利用已经训练好的分类器对甲骨文图像进\n行识别。\n（4）结果输出模块：将预测结果概率分布最大的3张图片及对\n应简体字输出到前端界面。\n图8甲骨文识别系统实现流程\n（二）项目阶段时间规划\n1.2.15-3.31背景调研，知识学习与建立甲骨数据集\n(1)与考古专家讨论研究方法和收集相关资料；\n(2)学习深度学习及相关代码库知识；\n(3)甲骨数据采集与建立数据集。92.4.1-4.30进一步扩充数据集与训练识别模型\n(1)进一步扩充数据集，形成足够大的训练样本；\n(2)建立甲骨文识别模型并提高识别效果。\n3.5.1-final提高模型识别效果与设计前端展示界面\n(1)不断改进模型，提高模型的识别准确率并减少推断时间；\n(2)设计前端展示网页并不断美化。\n四、项目总结\n（一）跨模态甲骨文数据库构建\n（1）甲骨拓片的爬取\n由于目前几乎没有甲骨拓片的公开数据集，因此本研究需自行创\n建甲骨拓片数据集作后续研究使用。选取甲骨文拓片书籍《甲骨金文\n拓本精选释译》，将书籍中拓片图像手动爬取并进行数据标注。\n（2）甲骨拓片的标注\n本研究对爬取的拓片进行了手动标注，标注拓片约900张。该标\n注任务难度较大，首先，《甲骨金文拓本精选释译》中的字符标注为\n手写体汉字（如图9）。其次，甲骨拓片图像与字符标注需自行对应。\n由于部分甲骨拓片存在破损严重的情况，因此在标签中标注了字\n符的句间位置，即该字符位于第几篇文章中的第几个字，以便后续可10能进行的使用NLP的方法根据上下文对字符推断的研究。\n图9甲骨拓片书籍中标注示意\n值得注意的是，部分甲骨文字并没有简体字对应的字符，因此采\n取“字海”网站（一个收录了《康熙字典》全部字符并编号的网站）的\n编号作为标注。此外，还存在一个甲骨字符对应多个简体字的情况，\n暂时使用多个简体字做标注。\n（3）甲骨拓片预处理\n甲骨拓片往往噪声大，边缘不够清晰。为提高甲骨拓片的训练效\n率与识别准确率，本研究需对甲骨拓片进行数据预处理，以清洗甲骨\n拓片图像的噪音，并对甲骨拓片的大小归一化，并且保留其形态学信\n息。\n相比将图像中的像素点传入神经网络进行训练，本研究尝试先对\n甲骨拓片进行边缘检测、特征提取，然后以图像的特征进行训练。因\n此本研究尝试了基于甲骨拓片的图像特征提取的创新探究。手写文字11图像的边缘特征多为简单的直线和曲线，因此适合用贝塞尔曲线拟合。\n使用贝塞尔曲线拟合可以降低图像特征的参数量，拓片图像有几百个\n像素点，而三阶贝塞尔曲线约具有10个参数。\n因此本研究先采用Caddy算法进行图像边缘检测，通过“拐点”\n将边缘进行分割为多条曲线，然后使用贝塞尔曲线分别进行拟合。本\n研究还尝试了使用多线程加速单张图片处理的过程。然而在图片处理\n中，出现了分辨率过低与图像拐点密集问题。为了更好地进行边缘分\n割，本研究还做了放大光滑实验，探究了超分辨率算法。\n在进行超分辨率算法时，对于放大倍数、模糊核等可选取的超参\n数存在一个多参数多目标优化的问题。本研究还遍历了可选取参数的\n解，找到了一组极具鲁棒性的超参数，可以使得各张图片的形态学特\n征提取效果最好。\n在设计边缘分割算法时，需要使得边缘像素点序列化。为了探索\n具有鲁棒性的算法，固定左上一点为起点，做修改后的广度优先探索。\n目前，能将闭合边缘，宽度不超过2像素的边缘序列化后使前后两点\n彼此相邻。然而，更为复杂的边缘分割，如汉字中存在分支的字形，\n需要更为复杂的算法，目前算法的时间复杂度为O（n）。\n为了更好地以贝塞尔曲线拟合边缘。将序列分为多段，以用更低\n阶的贝塞尔曲线拟合。线性拟合的R2会出现震荡的问题，因此不能\n用二分搜索解决，于是增加限制，要求每个点的xy误差值不大于3，\n然后使用二分法。\n贝塞尔四阶曲线由四个点与变量t∈[0,1]控制，该多项式方程可12以通过最小二乘法的线性拟合求出系数，然后利用系数解出贝塞尔曲\n线的起终点与控制点。\n将提取到的甲骨拓片信息送入神经网络学习，识别准确率约为\n2%，误差大于2万，均不理想。这是因为深度学习的方法对“位置”或\n者“等位性”更加敏感，而对边缘特征不敏感。后改进网络，使用共享\n参数的方式，识别准确率达到4%。由于该分类任务为558类，因此\n随机分类的识别准确率约为0.2%，可见使用边缘特征让网络进行学习\n的方式还是有一定潜力的，未来我们将继续探索。\n（二）智能甲骨文识别框架\n(1)VisionPermutator\n基于对汉字的认知，汉字存在一定空间结构特征，例如：上下结\n构、左右结构、半包围结构等。这些汉字的空间结构可以辅助汉字识\n别，以一些左右结构的字为例，仅凭借字的左半边或右半边信息即可\n推断其具体字形。并且，不同区域对于汉字识别的重要性可能不同。\n因此，我们联想到在识别汉字时可以将汉字切成“小块”处理。\n这种处理汉字的方法与VisionPermutator（ViP）[7]的核心思想相\n近。其处理图像识别任务时通过将图片“切片”以进行编码，这种处理\n图像的方式可以充分利用汉字的空间结构特征；此外，ViP是一种类\nMLP的神经网络结构，其参数量较少，因此其训练与推断更为高效。\nPermutatorblock是ViP的核心结构，这正是ViP与其他类MLP13结构，如Mlp-mixer[8]的区别的核心之处。在处理token时，Mlp-mixer\n在空间维度将token展平，然后在空间维度线性投影以编码空间信息，\n其输入是二维的，导致了部分空间信息的丢失。与之不同，ViP保持\n了原来的空间维度，以三维向量作为输入，分别沿着宽和高两个维度\n编码空间信息，保证token具有特定方向的信息，避免了空间信息的\n丢失。\n图10ViP结构示意图\n经过一定的数据预处理工作后，根据现有甲骨文数据集中的文字\n种类进行分类，统计各类字符的样本数量情况。\n经过运行相关代码进行统计，该数据集包含18000张图像，558\n种单字，统计结果如图11。然后，基于该甲骨摹本数据集，本研究\n设计相关程序将数据集划分为训练集与测试集。本研究还编码了甲骨\n文字母表，用于划定推断范围。14\n图11现有数据集样本类型统计\n本研究尝试以多种网络搭建识别框架，其中VGGNet在测试集上\n的准确率为87.731%，在测试集上准确率为80.105%(200epoch)。\nResNet的准确率分别为88.539%与80.476%(200epoch)，ResNet-SE:\n的准确率分别为90.801%/81.891%(200epoch)，如图12。\n而以ViP搭建识别框架时遇到了一定的问题。该网络在训练集表\n现很好，识别准确率有94.83%。然而在测试集上表现不佳，仅有50%\n左右。经过探索，我们发现ViP的训练依赖于预训练模型。\n于是我们从ViP处理图片的方式中汲取了思路，依旧采用将汉字\n分块处理的方式，并使用注意力机制中的协调注意力（Coordination\nAttention）[9]。15\n图12ResNet-SE的训练过程示意\n于是我们从ViP处理图片的方式中汲取了思路，依旧采用将汉字\n分块处理的方式，并使用注意力机制中的协调注意力（Coordination\nAttention）[9]。\n以SE（Squeezeandexcitation）、CBAM（ConvolutionalBlock\nAttentionModule）与CA（CoordinationAttention）做对比，如图\n13。SE只考虑了通道上的注意力，空间上的信息被GAP直接池化为\n1∗1的值，损失了空间信息。而CBAM虽然同时考虑了通道注意力\n与空间注意力，然而其感受野为7*7，不足够感受全局信息。而CA\n通过双分支编码空间信息，融合了水平和竖直方向的大感受野，空间\n信息得以保留，同时融合了空间的注意力和通道的注意力，因此可以\n更有效编码空间信息。\n我们以将汉字切块处理的思路，并基于CA改进了甲骨文识别框\n架，如图14。CA中编码空间信息使用两个分支，分别沿着长与宽，16将空间信息压缩为“一条线”，而改进后的甲骨文识别框架分别沿着图\n片上半部分与下半部分编码空间信息。这样能更好地利用汉字特有的\n空间信息。\n图13SE、CBAM与CA的对比\n图14基于CA的甲骨文识别框架\n这种改进后的甲骨文识别框架在训练集上达到94.967%的识别准\n确率，在测试集上准确度达到84.358%（图15）。可以看出，这个结\n果是优于RESNet-SE的。\n17\n图15基于CA的甲骨文识别框架的识别效果\n（三）智能甲骨文识别系统\n(1)甲骨文化展示界面\n为科普甲骨文化，本研究制作了甲骨文科普网页。该网页包含甲\n骨文科普短视频、甲骨文科普文字、甲骨文科普音频等多种展示形式。\n此外，该网页还科普了甲骨文简单字形，旨在提升公众对甲骨文\n化的兴趣，如图16。\n图16甲骨文字形科普\n(2)甲骨文识别窗口\n该识别窗口基于nodejs实现。首先在后台运行node服务器，服\n务器将会接收前端传入的图片，并上传到前端展示。后端的甲骨文识18别程序同时会读入图片，并且进行识别，并将识别的结果反馈到前端，\n前端将在指定位置展示识别结果与简体字图片，便于公众对比字形变\n化，演示如图17。\n图17甲骨文识别窗口演示\n五、下一阶段目标展望\n1.尝试使用拓片特征信息；\n2.继续改进识别模型；\n3.继续美化识别平台"}, {"role": "assistant", "content": "最终打分：8.5 (范围0-10分)\n1. 结构完整性得分：8.5, 占比20%\n2. 逻辑清晰度得分：8.5, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：8.5, 占比20%\n5. 参考文献规范性得分：10, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "DMINet: A lightweight dual-mixed\nchannel-independent network for cataract\nrecognition\nYu Chen1, Xiao Wu1, Qiuyang Yang1, Yuhang Zhao1, Xiaoqing Zhang1, Yuan\nJin3, Risa Higashita1,2, and Jiang Liu1,4\n1Research Institute of Trustworthy Autonomous Systems and Department of\nComputer Science and Engineering, Southern University of Science and Technology,\nShenzhen 518055, China\n2TOMEY Corporation, Nagoya 4510051, Japan\n3Zhongshan Ophthalmic Center, Sun Yat-sen University, Guangzhou 510060, China\n4Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation,\nSouthern University of Science and Technology, Shenzhen 518055, China\nAbstract. Cataracts are the leading causes for visual impairment glob-\nally, attracting attention from society. Over the years, researchers have\ndeveloped many convolutional neural networks (CNNs) to recognize sever-\nity levels of cataract based on different ophthalmic image types. However,\nmost existing works focused on improving the cataract recognition per-\nformance while ignoring deployment limitations on resource-constrained\nmedical devices. To this problem, this paper proposes a novel dual-mixed\nchannel-independent convolution (DMIConv), which takes advantages of\nthe multiscale convolution kernel sizes in a depthwise convolutional layer\nand depthwise dilation convolutional layer. Moreover, we use the DMI-\nConv as a drop-in replacement of the depthwise convolution to design a\nlightweight dual-mixed channel-independent network (DMINet) to recog-\nnize severity levels of cataract. To verify the effectiveness of our DMINet,\nextensive experiments are conducted on a clinical anterior segment op-\ntical coherence tomography (AS-OCT) dataset of nuclear cataract (NC)\nand a public OCT dataset. The results show that our proposed DMINet\nkeeps a better tradeoff between the model complexity and the classi-\nfication performance than state-of-the-art methods, e.g., DMINet out-\nperforms MixNet by 3.34% of accuracy while reduced by 4.87% in\nparameters.\nKeywords: cataract ·DMIConv ·classification ·convolution ·lightweight.\n1 Introduction\nCataract is the first cause of blindness and the main cause of visual impairment\nin the world. With the global ageing population, the number of cataract patients\nwill grow rapidly because the cataract is also an age-related ocular disease.\nCataract patients can improve their vision and life quality with early intervention2 F. Author et al.\nand cataract surgery, which are two main methods to reduce the burden on\nsociety [1].\nOver the years, scholars have developed a lot of advanced machine learning\nmethods for automatic cataract screening and recognition based on different oph-\nthalmic images [24], aiming at to detect cataract as soon as possible and improve\nthe efficiency and precision of clinicians in clinical cataract diagnosis. For exam-\nple, Li et al. [11] proposed a slit lamp image-based nuclear cataract (NC) grading\nframework, which is a combination of the lens detection, feature extraction, and\ngrading. Xu et al. [19] used the bag-of-features (BOF) method to extract fea-\ntures for improving the grading results of NC based on slit images. Zhang et\nal. [21] used Watershed and Markov random fields (MRF) methods for poste-\nrior subcapsular cataract (PSC) detection based on anterior retro-illumination\nimages.\nWith the advent of deep learning techniques, deep neural networks have\nwidely been utilized to recognize the severity levels of cataract. [17] uses faster\nR-CNN to classify the severity levels of NC based on slit lamp images in a\nend-to-end manner. Zhang et al. [25] developed an adaptive feature squeeze\nnetwork (AFSNet) to classify severity levels of NC based on anterior segment\noptical coherence tomography (AS-OCT) images. Xu et al. [18] proposed a hy-\nbrid global-local representation convolutional neural network (CNN) for fundus\nimage-based cataract screening. Most deep learning-based cataract classification\nworks mainly focused on the improvements of cataract classification and rare\nworks aimed at developing lightweight networks for automatic cataract classi-\nfication, which is easy to be deployed on resource-constrained medical devices\nand mobile devices.\nIn recent years, researchers have spent much effort on designing lightweight\nand efficient convolutional neural network (CNN) architectures for computer\nvision tasks. SqueezeNet [8] is the first research to focus on lightweight CNN ar-\nchitecture design. Following SqueezeNet, various advanced lightweight CNNs are\npresented to tackle different learning tasks [20, 15, 13] based on efficient convo-\nlution methods. Depthwise convolution method is a widely method to construct\nefficient CNNs, such as MobileNets [7, 13], ShuffleNets [22], NASNet [27], and\nEfficientNet [14]. Unlike the standard convolution method, the depthwise con-\nvolution (DW-Conv) applies a convolution kernel to each channel separately,\nreducing the computational burden by a factor of C (C denotes the number\nof channels). MixNet [15] proposed a mixed depthwise convolution (MixConv)\nmethod to prompt the DW-Conv, which adopts multiscale convolution kernel\nsizes in a depthwise convolutional layer by splitting channels into several groups\nand applying different kernel sizes to each group of channels. However, MixConv\nuses large convolution kernel size, increasing computational cost and parameters.\nIn order to achieve a good balance between the performance and the com-\nplexity of model, this paper develops a novel dual-mixed channel-independent\nconvolution (DMIConv) method, which exploits the advantages of multiscale ker-\nnel sizes in a depthwise convolutional layer and depthwise dilation convolutional\nlayer sequentially by partitioning channels into multiple groups according to theTitle Suppressed Due to Excessive Length 3\nnumber of different convolution kernel sizes. Furthermore, we treat the DMIConv\nas a simple drop-in replacement of the vanilla depthwise convolution method and\nthen construct an effective yet lightweight dual-mixed channel-independent net-\nwork (DMINet) to recognize severity levels of cataract. We conduct experiments\non a clinical AS-OCT dataset of NC. Results show that DMINet better balances\nthe model complexity and the classification performance through comparisons\nto other advanced lightweight CNNs. To further validate the effectiveness of our\nDMINet, a publicly available OCT is also adopted, and experimental results\nshow that our DMINet achieves competitive performance compared with strong\nbaselines and published methods.\nOutput Tensor\n...\nInput Tensor\nOutput Tensor\n...\n1x1\n3x3 (1)\n3x3\n2x2 (2)\n5x5\n3x3 (3)\nInput Tensor\nOutput Tensor\n3x3\n5x5\nkxk\nInput Tensor\nkxk\nDepthwise convolution\nOutput Tensor\n...\nInput Tensor\nOutput Tensor\n...\n1x1\n3x3 (1)\n3x3\n2x2 (2)\n5x5\n3x3 (3)\nInput Tensor\nOutput Tensor\n3x3\n5x5\nkxk\nInput Tensor\nkxk MixConv\nOutput Tensor\n...\nInput Tensor\nOutput Tensor\n...\n1x1\n3x3 (1)\n3x3\n2x2 (2)\n5x5\n3x3 (3)\nInput Tensor\nOutput Tensor\n3x3\n5x5\nkxk\nInput Tensor\nkxk Our proposed DMIConv\nFig. 1. Dual-mixed channel-independent convolution (DMIConv): Unlike depthwise\nconvolution sets the same convolution kernel size for channels and MixConv applies\ndifferent convolution kernel sizes to channels by group strategy, our DMIConv not only\nuse mixed convolution kernel sizes but also exploits advantages of depthwise convolu-\ntion and depthwise dilated convolution.\n2 Method\nAs previously introduced, the key motivation for devising the dual-mixed channel-\nindependent convolution (DMIConv) method is to improve both NC classifica-\ntion results and efficiency. In this section, we will first introduce DMIConv in\ndetail.\n2.1 Dual-mixed channel-independent convolution\nThis section first revisits the original depthwise convolution method, as shown in\nFig. 1(a). Given any input feature map DH×DW×Mand generates the output\nfeature map DH×DW×Nwhere DH,DW, M, and N represents the spatial\nheight, spatial width, the number of input channels, and the number of output\nchannels (Specifically, N is equal to M). In the depthwise convolution method,4 F. Author et al.\nwe apply a filer with convolution kernel size K×Kto each channel separately,\nwhich can be formulated as follows:\nGk,l,n=X\ni,j,mKi,j,m,n ∗Fk+i−1,l+j−1,m (1)\nThe original depthwise convolution only uses the same convolution kernel\nsize for all channels, e.g., the commonly-used 3 ×3 convolution kernel. It over-\nlooks that multiscale convolution kernel sizes that can boost the classification\nperformance. To alleviate this shortcoming of depthwise convolution, Tan et al.\ndesigned a variant of depthwise convolution method, termed MixConv. As shown\nin Fig. 1(b), MixConv divides channels into multiple groups based on the num-\nber of different convolution kernel sizes and then applies each convolution kernel\nsize to each group. One shortcoming of MixConv is that it uses large convolution\nkernel sizes, which increases the computational cost.\nTo overcome the shortcoming of MixConv, this paper designs a dual-mixed\nchannel-independent convolution method, as shown in Fig. 1(c), which is a com-\nbination of three efficient convolution methods: multiscale convolution kernel\nsizes, depthwise convolution (DW-Conv), and depthwise dilated convolution\n(DW-D-Conv). Like MixConv, we first adopt the group strategy to partition\nthe channels into multiple groups according to different convolution kernel sizes.\nSpecifically, input feature maps are partitioned into zgroups: < D H×DW×\nm1, DH×DW×m2, ..., D H×DW×mz>, where m1+m2+...+mz=M.\nIn each convolution kernel size group, we use a DW-D-Conv and a DW-Conv\nto continuously capture long-range and local patterns from input feature maps.\nPrevious works have demonstrated that dilated convolution method can increase\nthe receptive field size with smaller convolution kernel sizes through comparisons\nto regular convolution and pointwise convolution (Conv1 ×1) method. Hence,\nthis paper uses a depthwise dilated convolution to build the long-range rela-\ntionships among feature representations in a feature map. It is followed by a\ndepthwise convolution, which is used to construct local relationships among fea-\nture representations in a feature map. Moreover, our proposed DMIConv can be\ntaken as a simple alternative to depthwise convolution for designing lightweight\nCNN architectures.\nFurthermore, this paper also designs three variants of DMIConv (DW-D-\nConv+DW-Conv): DW-D-Conv+DW-Conv+Conv1 ×1, DW-Conv+Conv1 ×1 ,\nand DW-D-Conv+Conv1 ×1 to test which affects the performance of our DMI-\nConv. Conv1 ×1, also named pointwise convolution method, which is a widely-\nused method to construct lightweight CNNs [6, 13].\n2.2 Network architecture\nFig 2 presents a general architecture of our DMINet based on the proposed\nDMIConv, which adopts the same network depth as MobileNetV1 used to en-\nsure performance comparison fairness. In the DMINet, we first use the same\nstem as MixNet adopts, followed by a series of dual-mixed channel-independentTitle Suppressed Due to Excessive Length 5\n3x3 (1), 2x2 (2), 3x3 (2)3x3 (1)Stem\n112x112x16224x224x3\n56x56x24\n56x56x24\n28x28x40\n3x3 (1), 2x2 (2), 3x3 (2), 4x4 (2), 3x3 (3)\n3x3 (1), 2x2 (2), 3x3 (2), 4x4 (2)\n1x1, 3x3, 3x3, 3x31x1\n3x3 (1)\n1x1\n1x1, 3x3, 3x3\n3x3 (1), 2x2 (2)\n1x1, 3x3\n3x3 (1), 2x2 (2)\n1x1, 3x328x28x40\n3x3 (1), 2x2 (2)\n1x1, 3x3\n3x3 (1), 2x2 (2), 3x3 (2)\n1x1, 3x3, 3x3\n3x3 (1), 2x2 (2)\n1x1, 3x3\n3x3 (1), 2x2 (2)\n1x1, 3x3\n3x3 (1), 2x2 (2), 3x3 (2)\n1x1, 3x3, 3x3\n3x3 (1), 2x2 (2), 3x3 (2)\n1x1, 3x3, 3x3\n3x3 (1), 2x2 (2), 3x3 (2)\n1x1, 3x3, 3x3\n1x1, 3x3, 3x3, 3x3, 5x5\n3x3 (1), 2x2 (2), 3x3 (2), 4x4 (2)\n1x1, 3x3, 3x3, 3x328x28x40\n28x28x40\n14x14x80\n14x14x80\n14x14x80\n14x14x120\n14x14x120\n7x7x120\n7x7x200\n7x7x200\n7x7x200\nDepthwise dilated convolution Depthwise convolution\nFig. 2. A framework of the dual-mixed channel-independent network (DMINet), which\nis built on the proposed DMIConv.\nconvolutional layers, a global average pooling (GAP) layer, and a classifier layer.\nThis paper uses the commonly-used softmax and cross entropy loss functions as\nthe classifier and loss, respectively.\n3 Metrics and experiment setting\n3.1 Metrics\nFollowing previous works on cataract classification tasks [23, 2], this paper uses\nfour commonly-adopted evaluation measures to assess the classification perfor-\nmance of methods: accuracy (ACC), sensitivity (Sen), precision (PR), and F1\nmeasure. ACC and F1 are usually utilized to evaluate the overall performance\nof a method. Sen is a very significant indicator for clinical disease diagnosis. The\nfollowing equations can obtain these four evaluation measures:\nACC =TP+TN\nTP+FN+TN+FP(2)\nSen=TP\nTP+FP(3)\nPR=TP\nTP+FN(4)\nF1 =2×Sen×PR\nSen+PR(5)\nwhere TP, FP, TN and FN denote the numbers of true positives, false positives,\ntrue negatives and false negatives, respectively.\n3.2 Dataset\nThis paper uses two ophthalmic image dataset and one fundus image dataset to\ndemonstrate the performance of the DMINet: a clinical AS-OCT dataset of NC\nand a public OCT dataset [10].6 F. Author et al.\nAS-OCT dataset. It is a clinical dataset of NC, which is collected by CASIA2\ndevice (TOMEY Company, Japan). The dataset contains 534 participants: 422\nright eyes and 440 left eyes, and the total number of AS-OCT images (as shown\nin Fig 3 is 16,201. It contains three severity levels of NC: normal, mild, and\nsevere, and we split it into three disjoint datasets based on the participant level:\ntraining, validation, and testing. Table 1 lists three severity level distributions\nof NC for disjoint datasets.\n(a) AS-OCT\n (b) Normal\n (c) Mild\n (d) Severe\nFig. 3. Three severity levels of nuclear cataract (NC) based on AS-OCT images (a).\nNormal (b) without opacity; Mild NC (c) with slight opacity but is asymptomatic;\nSevere NC (d) with opacity and is symptomatic.\nTable 1. Detailed nuclear cataract level distributions on the AS-OCT dataset\nDataset Normal Mild Severe total\nTraining 1004 2872 5518 9394\nValidation set 345 1306 1740 3391\nTesting 254 664 2498 3416\nTotal 1603 4842 9756 16021\nUCSD dataset. It is a publicly available OCT of age related macular degen-\neration (AMD) and diabetic macular edema (DME), which is collected from\nMedical Center Ophthalmology Associates, the Shanghai First People’s Hospi-\ntal, Shiley Eye Institute of the University of California San Diego, the California\nRetinal Research Foundation, and Beijing Tongren Eye Center. The training set\ncontains 83,484 OCT images from 4,686 patients: 37,205 images of choroidal neo-\nvascularization (CNV), 11,348 images of diabetic macular edema (DME), 8,617\nimages of drusen, and 51,140 images of healthy. The testing dataset contains\n1000 images and each class has the number of OCT images. In this paper, we\nfollow the same training dataset splitting strategy in[25, 26].Title Suppressed Due to Excessive Length 7\nGlaucoma dataset. It is a fundas image dataset of glaucoma, which is collected\nfrom Beijing Tongren Hospital. The dataset contains 4,854 fundus images: 3,143\nimages of normal people and 1711 images of patient. The distributions are shown\nin Table 2.\nTable 2. Detailed glaucoma distributions on the dataset\nDataset Normal patient total\nTraining 1885 1026 2911\nValidation set 629 342 971\nTesting 629 343 972\nTotal 3143 1711 4854\n3.3 Experimental setting\nWe use Pytorch, python, scikit-learn, and OpenCV software packages to our\nproposed DMINet and comparable deep neural networks. This paper uses the\nstochastic gradient descent (SGD) method to optimize the learnable weights of\ndeep neural networks with the default settings. The training epochs and batch\nsize are set for 150 and 64 correspondingly. We set the initial learning rate\nfor 0.0025, which is decreased by a factor of 5 every 25 epochs. This paper\nsets the final learning rate at 0.0001 when training epochs are more than 130.\nThis paper follows the common data augmentation strategies to preprocess the\ntraining images (such as cropping and flipping), and resizes the images of two\ndatasets into 224 ×224. All experiments are run on a server with one NVIDIA\nTITAN GPU, 10 GB DDR memory, and the operating system is Ubuntu.\n4 Result analysis and discussion\n4.1 Performance comparison on the AS-OCT dataset\nThis paper first compares DMINet with commonly-used CNNs and lightweight\nCNNs, as shown in Table 3. It can be seen that our DMINet achieves the best\nperformance of NC recognition on four evaluation measures: 90.46% of accuracy,\n90.98% of F1, 90.46% of precision, and 92.76% of sensitivity, respectively. For\ninstance, with less or comparable parameters and flops (millions) than MixNet\nand SqueezeNet, it significantly improves 3.34% and3.22% gains of accu-\nracy. Fig. 4(a) lists the relationships between parameters and accuracy, and\nour DMINet achieves a better trade-off between the NC classification perfor-\nmance and memory requirement than other comparable CNNs. Fig. 4(b) shows\nthe relationships between flops and accuracy; DMINet requires fewer flops than\nother CNNs except for ShuffleNet while obtaining the best NC classification per-\nformance. In general, the classification results demonstrate that our proposed8 F. Author et al.\n(a)\n (b)\nFig. 4. (a) Relationship between the performance of DMINet and the total number\nparameters (Millions), compared to state-of-the-art CNNs; (b) Relationship between\nthe performance of DMINet and the total number flops (Millions), compared to state-\nof-the-art CNNs.\nDMINet keeps a better trade-off between the classification performance and the\ncomplexity through comparisons to other CNNs, which is feasible to be deployed\non medical devices.\nTable 3. Performance comparison of DMINet, advanced CNNs, and lightweight CNNs\non the AS-OCT dataset\nModel Accuracy F1 PR Sen Parms (M) Flops (M)\nResNet18 87.39 88.09 87.39 89.90 11.1 1818\nResNet34 89.52 90.03 89.52 91.60 21.3 3671\nVGG19 90.02 90.59 90.02 92.67 14.8 1615\nMobileNetV1 90.02 90.41 90.02 91.12 3.21 577.8\nMobileNetV2 86.33 87.29 86.33 90.65 3.50 314.2\nShuffleNet 89.46 90.04 89.46 91.95 1.88 145.9\nMixNet 87.12 87.95 87.12 90.69 2.62 232.0\nSqueezeNet 87.24 88.03 87.24 90.22 1.25 823.4\nDMINet 90.46 90.98 90.46 92.76 2.50 221\n4.2 Performance comparison on the UCSD dataset\nTable 4 lists the classification results of DMINet, original CNNs, state-of-the-\nart attention CNNs, and published works on the UCSD dataset. It can be ob-\nserved that our DMINet achieved the best classification results of AMD and\nDME by using OCT images among machine learning methods and deep learn-\ning methods. Compared with HOG-SVM, DMINet achieved 25.50% increase\nin accuracy and 46.88% increase in sensitivity, respectively. Moreover, Our\nDMINet obtains slight improvements through comparisons to state-of-the-art\nattention CNNs. Overall, the classification results in Table 4 also demonstrateTitle Suppressed Due to Excessive Length 9\nthat DMINet achieves a better tradeoff between the model complexity and the\nclassification performance than machine learning methods and deep learning\nmethods.\nTable 4. Performance comparison of the DMINet and state-of-the-art methods on\nUCSD dataset\nMethod ACC Sen F1\nLBP-SVM [3] 71.33 48.27 64.04\nHOG-SVM [4] 78.90 66.20 –\nMDFF [3] 93.93 91.76 91.46\nVGG16 [3] 91.50 91.50 91.50\nResNet34 [10] 80.50 78.30 –\nInception [4] 90.30 90.00 –\nLACNN [4] 90.20 88.10 –\nLACNN-Inception [4] 93.00 91.60 –\nSENet [9] 94.16 90.00 91.49\nSPANet [5] 94.11 89.83 91.32\nBAM [12] 94.89 91.95 92.69\nCBAM [16] 94.20 89.74 91.30\nMPANet [26] 96.74 95.12 95.39\nDMINet 96.86 95.15 95.45\n4.3 Performance comparison on the Glaucoma dataset\nTable 5 lists the classification results of DMINet, MobileV2 and three kind of\nvariants of DMINet. It can be observed that our DMINet achieved the best classi-\nfication results of AMD and DME by using OCT images among machine learn-\ning methods and deep learning methods. Compared with MobileV2, DMINet\nachieved 1.34% increase in accuracy. And the DMINet achieves a better trade-\noff between the model complexity and the classification performance.\nTable 5. Performance comparison of DMINet, MobileV2, and three variants on the\nglaucoma dataset\nConvolution method ACC Parms (M) Flops (M)\nDW-D-Conv+DW-Conv+Conv1 ×1 90.22 3.84 457.0\nDW-Conv+Conv1 ×1 90.53 3.95 467.8\nDW-D-Conv+Conv1 ×1 91.35 3.77 446.2\nDW-D-Conv+DW-Conv (DMIConv) 94.34 2.50 220.7\nMovileV2 93.00 2.23 313.010 F. Author et al.\n4.4 Ablation study\nTo further examine the effectiveness of our DMIConv, Table 6 lists the classi-\nfication performance of DMIConv and its three variant based on MobliNetV1\narchitecture. We can see that compared with the other three variants, our pro-\nposed DMIConv achieves higher accuracy while requiring fewer parameters and\nflops. The key difference between our method and other comparable convolu-\ntion methods is that the other three comparable convolution methods adopt the\npointwise convolution method (Conv1 ×1), which uses the fully-connected way to\nbuild connections among channels but also increases feature map redundancies.\nIt is the main reason why DMIConv gets better performance than its variants.\nTable 6. Performance comparison of DMIConv and its variants on the AS-OCT dataset\nConvolution method ACC Parms (M) Flops (M)\nDW-D-Conv+DW-Conv+Conv1 ×1 88.62 3.84 457.0\nDW-Conv+Conv1 ×1 88.73 3.95 467.8\nDW-D-Conv+Conv1 ×1 89.60 3.77 446.2\nDW-D-Conv+DW-Conv (DMIConv) 90.46 2.50 220.7\n5 Conclusion and future work\nThis paper proposes an efficient dual-mixed channel-independent convolution\n(DMIConv), which takes advantages of different convolution kernel sizes, depth-\nwise dilated convolution, and depthwise convolution. We construct a lightweight\ndual-mixed channel-independent network (DMINet) based on the DMIConv\nmethod to recognize the severity levels of nuclear cataract. Experimental re-\nsults on an AS-OCT dataset show that our proposed DMINet obtains signifi-\ncantly better classification results and efficiency than state-of-the-art lightweight\nCNNs. Moreover, the results of another public OCT dataset demonstrate the ef-\nfectiveness of our method. In the future, we plan to deploy our DMINet on an\nactual medical device to further test its performance.\n"}, {"role": "assistant", "content": "最终打分：9.7 (范围0-10分)\n1. 结构完整性得分：10, 占比20%\n2. 逻辑清晰度得分：9.5, 占比20%\n3. 语言连贯性得分：9.5, 占比20%\n4. 内容独特性和创新性得分：9.5, 占比20%\n5. 参考文献规范性得分：10, 占比10%\n6. 课程知识掌握度得分：10, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "2022.06.09\n赵宇航  (Zhao Y uhang) 1 191071 1\n徐格蕾  (Xu GeLei) 1 1911138\n郑博方  (Zheng Bofang) 12012918\nFigure 0. Histograms of China aging and cataract estimationCataract Intelligent Screening and\nDiagnosis System \n \nPart 1 - Research Background  \n1.1 Overview of Cataract  \n Cataract is one of the global leading diseases which causes visual impairment and blindness. According to \nWorld Health Organization (WHO), it is estimate d that around 2.2 billion people suffer from visual impairment,  \nand approximately 33% of that is related to cataract. Especially in developing countries, catarac t accounts for \n50% or more of blindness of all ophthalmic diseases.\n Clinically , cataracts can be categorized into nuclear cataract (NC), cortical cataract (CC), and posterior  \nsubcapsular cataract (PSC), depending on the location of opacity . Aging and genetics are two of the most  \nimportant factors causing  the cataract. The prevalence of cataract in the population aged 40 years and older has \nreached 11.8% to 18.8%. With the trends of aging in population in China, the estimated number of cataract  \npatient continued to rise.\n1.2 Clinical Diagnosis and Classification  Figure 1. Comparison of three ophthalmic imaging modalities\nFigure 2. Lens Opacities Classification System III Two of popular ophthalmic imaging methods used for catarasct diagnosis so far are slit lamp image and AS-\nOCT image. The silt lamp camera is a high-intensity light source instrument typically used to examine the \nanterior and posterior segment structure of human eye. AS-OCT is a new modality of anterior segment optical  \ncoherence tomography image, it is able to capture the nucleus, cortical, and post subcapsular area clearly .\n The widely adopted cataract classification system is lens opacities classification system (LOCS). It provides  \nrepresentative slit lamp and retroillumination images of different cataract severity levels. The severity level is \ngraded on a decimal scale by spacing intervals regularly .\nPart 2 - Previous Work  \n2.1 Diagnosis System  \n Our general goal is to develop a light-weighted, real time cataract screening and diagnosis system. The system  \nconsist of cataract segmentation and cataract classification. In the segmentation part, the system receives an AS-\nOCT image as input, extract the lens from the original image, then sends the cortical and nucleus image to \nclassify . In the classification part, we use two different networks to classify the cortical and nucleus cataract. The \nfinal result is graded into either normal, mild, or severe.Figure 3. Schematic of the cataract diagnosis system\nFigure 4. The entire diagnosis system\n Besides, we also implem ented segmentation on slit-lamp modality , to allow criteria conversions in cataract  \ndiagnosis. Our system is also implementation with fair scalability , to allow flexible extensio ns to whether  \nglaucoma or corneal analysis in the future.\n2.2 Segmentation  \n In the last presentation, the segmentation has included:\n1. A research into traditional methods and mechanics of image processing\n2. A combined algorithm with Canny and Otsu's threshold segmentation for slit-lamp image\n2.3 Classification  \n2.3.1 Cortical Cataract Classification  \n In the last presentation, combined with the pathological features of cortical cataracts, we decided to use self \nattention for diagnosis. Referring to the literature, we decided to use a block self-attention method as shown, but \nit still has a high space-time complexity and we need to modify the model to reduce the complexity .\n2.3.2 Nuclear Cataract Classification   \nFigure 5. Left:Applying Canny on AS-OCT image. Right:Applying Otsu's threshold on AS-OCT image.\nFigure 6. An demonstration of AS-OCT pixel distuibution and its af fect In the last presentation, we decide to choose CA as the attention method to help solve our classification  \nproblem according to the experiment results, in which CA always show the best performance.\n Then we try to improve the method to be more fitted to our problem. We preserve the main structure of CA \nbecause we want to take its advantages, so we pay attention to the pooling method and change it to a new \npooling meth od: Local Importance-based Pooling . But the results are not as good as we think. We also think of \nthe reason and do some explanation.\nPart 3 - Current Work  \n3.1 Segmentation  \n3.1.1 Segmentation on AS-OCT images  \n The ultimate objective of segmentation is to extract parts of interest from both slit-lamp and AS-OCT images.  \nBy now we have succeeded with the former using gradient-based approaches. Intuitively , we want to utilize the \nvery same method on AS-OCT modality . However , the direct application of the algorithms gave vague and \nmessy edges(shown as follow) and did not produce the expected results. We drew 3 facts based on the \ncharacteristics of AS-OCT , trying to explain why gradient-based method may fail.\n1. In raw AS-OCT images, pixels do not gather together as surface. Rather , regions are \"formed\" by countless  \ndiscrete dots. These dots may result from absorpt ion of X-ray by human tissues. The dots in different regions  \nprocess different brightness and density . The gradient value around these dots are generally high since the \ndots appear discrete. Thus gradient-based segmentation methods such as canny is likely to fail on those  \nimages.\n2. For the same reason, the algorithms that are based  on thresholding are likely to fail, too. This is because in the \nAS-OCT image the low-gray-value pixels are mixed up with the high-gray-values almost everywhere, making  Figure 7. Flowchart of the segmentation procedure\nTable 0. comparation of the advantages and disadvantages of the three algorithms\nAlgorithms Advantages Disadvantages Pics\nThresholding Easy to achieve Affected by discreteness\nGradient-based Clear edges Affected by discreteness\nit hard to find an ef fective threshold to separate the foreground from the background.\n3. The problem seems to be the discreteness of pixels leading to the discontinuity of edges. Hough  \ntransformation is a meth od that can join the disconnected straight or oral-shaped edges. Howev er no such \nshapes can be successfully detected in our input image, thus attempts to directly utilize Hough  transform  \nfailed.\n    With above observa tions, we can conclude that the key problem is that the edges appea rs as \"density  \ndifference\" rather than \"gray-value dif ference\"(which is the occasion in slit-lamp modality). Thereby , we need an  \napproach to measure the density in an image, or in a more straight forward way, find a segmentation approach  \nthat takes the factor of pixel distribution into account.\n To achieve this, we explored into region-based segmentation approaches: region growing. The method begins  \nwith a start point called \"seed\", recursively check whether the pixels around it is alike, if so, then classify them  \ninto one group. The algorithm can omit the discreteness of AS-OCT image and group out the cortical as well as \nnucleus areas regardless of the noises within them.\n To make the final mask for segmentation appear as a whole surface, we also added blurring and dilation to the \nalgorithm, aiming at improving the segmentation results.\n3.1.2 Edge detection methods: Comparison and solution  \n By this far we have completed the segmentation of both modalities. Table 0 compares the advantages and \ndisadvantages of the three algorithms mainly used.Algorithms Advantages Disadvantages Pics\nRegion-based Works on AS-OCT Grainy and vague edges\nFigure 8. ResNet with local scaling self-attention module in all layers To conclude , in general tasks of image segme ntation, the characteristics of the input image should be \nexamined first, only then can one better decide which algorithms to apply .\n \n3.2 Classification  \n3.2.1 Cortical Cataract Classification  \n For cortical cataract classification part, we first decided to use the classical network ResNet18 backbone, and \nthen added modules to optimize cortical cataract classification.\n We introduced a networ k Halonet proposed by Google in 2021 [1] in the last report, which focused on \nchunking the images and then performing self-attention separately , so that the network can focus more on local  \nfeatures while making the model less complex and computationally intensive.\n Its structure is shown in figure 8. We add a local scaling self-attention module in the middle of each two \nblocks of ResNet16. However , after experimen ts, this is still very computationally intensive and can only \nsupport expe riments with batch=8 at most, and it is difficult to drive larger networks and parameters by our lab \nservers.\nThere are two ideas to solve this problem:  \nmodify the layers of self-attention to reduce the complexity of each layer;  \nreduce the number of self-attention layers. In the following, we will design two networks for each of these  \ntwo ideas.\nFor the first idea, we found a paper that can effectively reduce the complexity of self-attention [2]. It uses two \nlinear layers (called external-attention layer) instead of self-attention, which can greatly reduce the \ncomplexity of the model while achieving similar complexity as self-attention. Referring to this paper , we \nreplace the local scaling self-attention module with the external attention module, as shown in Fig 9.Figure 9. ResNet with external-attention modeule in all layers\nFigure 10. ResNet with local scaling self-attention module in last layers\nTable 1. AS-OCT cortical cataract dataset\n Normal Mild Sever e\nTraining 590 1219 964\nTesting 284 313 356\nTotal 874 1532 1320\nFor idea 2, we need to know which layer contributes the most to the final result. Referring to the literature, we  \nknow that convolution tends to activate relatively low-level features at low levels; as the  convolution layers  \nbecome deeper , the activated features become more advanced and contribute more and more to the \nclassification results. For this result, we choose to first insert the self-attention layer on the last four layers of \nResNet, as shown in Fig 10.\n In the experi mental part, we choose the cortical cataract dataset as shown in Table 1. During the experiments,  \nthe software  environment we use is Ubuntu OS, the hardware system is an Nvidia Titan GPU (10GB), and the \noptimizer is a stochastic gradient descent optimizer with a simulated annealing strategy . We set the learning rate \nto 1 and the batch size to 8.Table 2. Experiment result of cortical cataract calssification\n Accuracy Precision Recall F1 Params\nResrNet18 83.66 80.32 81.09 83.21 11.17M\nSENet18 82.22 80.65 81.50 83.22 11.26M\nSA (All Layer) 85.63 85.78 86.22 84.67 28.26M\nSA (Last Layer) 84.25 85.27 84.25 86.01 15.50M\nLocal Scaling SA (All Layer) 86.88 86.21 85.10 85.29 17.42M\nLocal Scaling SA (Last Layer) 86.13 85.92 85.39 85.33 13.23M\nExternal Attention (All Layer) 84.99 85.82 84.21 85.45 14.25M\nExternal Attention (Last Layer) 85.44 85.35 85.44 85.23 12.14M\nTable 3. V ariation of the architecture of cortical cataract classification\n Batch Learning Rate Layer Multi Head Halo Size Accuracy Recall\nA 8 0.01 14, 16 8 2 86.88 86.21\nA 6 0.01 14, 16 8 2 86.25 85.69\nA 32 0.01 14, 16 8 2 85.57 85.61\nB 16 0.005 14, 16 8 2 86.10 85.50\nB 16 0.02 2, 4 8 2 85.47 85.89\nC 16 0.01 8, 10 8 2 83.78 82.22\nC 16 0.01 14, 16 8 2 84.44 83.89\nD 16 0.01 14, 16 4 2 85.24 84.25\nD 16 0.01 14, 16 16 2 86.12 84.99\nE 16 0.01 14, 16 8 1 85.00 85.15\nE 16 0.01 14, 16 8 4 86.63 86.00 We first tested the difference between the self-att ention of the above three methods in inserting all layers and \nthe attention of only the last layer , and also tested the accuracy of ResNet18 backbone and SENet18 backbone  \nfor comparis on. The experimental results are shown in table 2. It can be seen that the local scaling  self-attention  \nwith all layers inserted achieves the highest accuracy and precision, and the local scaling self-attention with only \nthe last layer inserted achieves the highest recall and F1 score, while its accuracy and precision are very close to \nthe former . Considering that the model complexity of the latter is significantly smaller than that of the former , \nwe decided to use the local scaling self-attention with only the last layer inserted for further experiments.\n The variations on the architecture of the model are shown in Table 3. We tested the results of the model under  \ndifferent batch, learn rate, layers inserted by attention, multiple heads of attention, and halo size of local scaling  \nattention. It can be seen in group F that the model can finally achieve an accuracy of 88.21% and a recall of \n87.89% . Batch Learning Rate Layer Multi Head Halo Size Accuracy Recall\nF 8 0.01 14, 16 8 2 88.21 87.89\nFigure 1 1. Visualization using CAM using ResNet and ResNet+Self-Attention\nFigure 12. CA improvement To verify that the interpretability of the network with the addition of the self-attention layer is stronger than \nthat of ResNet, we visualized ResNet itself and the ResNet with the addition of the self-attention layer . The \nresults show  that ResNet+self-attention activat es the pathological strip region more significantly . For the \ndiagnosis of mild cataract, the pathological ResNet itself cannot effectively identify the patholo gical regions,  \nwhile ResNe t+self attent ion can focus on the bars in the upper part of the image, which further validates that the \nattention module helps to improve the interpretability of the model.\n3.2.2 Nuclear Cataract Classification  \n In the previous work, after confirming the use of CA for cataract classification, we aim to make some  \nimprovements. W e left the basic framework unchanged since we wanted to preserve the strength of the CA itself.  \nFor the pooling approach, some improvements can be made in our classification problems, as shown in figure  \n12.\n Last time, we discovered  a pooling method in [3], local importance-based pooling, to replace average pooling  \nin the original method. Local importance-based  pooling can adaptively update an importance graph to select  \nfeatures that are more important to the results as the results of the downsampling.  \n Figure 13. Nuclear of Lens\nTable 4. Nuclear Cataract Dataset\n Normal Mild Sever e\nTraining 2132 1985 1434 However , test accuracy did not improve greatly in the experiment. After analysing the reason,  we hope to \nmake some continuous improvements in pooling,  therefore we further analyze the advantages and disadvantages  \nof dif ferent pooling methods for our problem.  \nAdvantages of some Pooling methods in our Experiment\n Firstly we compared the advantages of these pooling method. For average pooling, it is just the representative  \nof the average of pooling  region pixels, in the last semester , our work is to use machine learning to classify the \nimage, we carried on the correlation analysis, finding that the correlation between the characteristic pixel  \naverage and the classification results is the biggest, thus we think the average pooling is very important and \nIndispensable. For maximum pooling, we mainly look at its brighter region to judge the classification of this \nimage, and maximum pooling is the eigenvalue of its brightest region, so it is also related to our classification  \ntask. The advantage of LIP lies in its adaptive ability to obtain the most important feature for the result.\nDisadvantages of some Pooling methods in our Experiment\n For disadvantages, we use figure 13, a typical figure that we need to classify as an example. It has several  \nobvious features: because the nuclear part is oval, we filled the four corners with black, and some light spots  \nmay appear in the picture due to excessive light intensity or excessive light source.  \n \n These will be some certain impacts on the pooling method mentioned above. For example, the black part will \nhave an impact on average pooling, making its eigenvalue lower . Bright spots also affect maximum pooling very \nmuch, making it too large; In the case of LIP, there is nothing interpretable about it in our problem , which makes  \nit weakness.  \nTake advantages W eaken disadvantages\n So we want to take adva ntage of the strengths of these pooled approaches and make up for their weaknesses.  \nFirst, we consider average pooling to be indispensable for the reasons already mentioned. So, we tried to \ncombine the pooling approach. For example, combine average pooling with maximum pooling. In this way, the \neffect of black areas on average pooling is partially offset by maximum pooling, and the effect of bright spots on \nmaximum pooling is partially of fset by average pooling, which helps in both ways.  \n Of course, the reason is the same for combining average pooling with local importance pooling. We can also \ncombine pooling of local importance with pooling of maximum value, allow them to cancel out first, and then \ncombine the results with pooling of average value, hoping to get better results.  \n In the process of combination, it is inevitable to consider the problem of proportion. In our exper iment, only \nthree propor tions were initially considered: 1:1, 2:1 and 1:2, respectively representing the average ratio, the high \nproportion of average pooling, and the small proportion of average pooling.   Normal Mild Sever e\nTesting 963 1104 301\nTotal 3095 3089 1735\nTable 5. T est Result of nuclear cataract classification\nACC RESNET18 RESNET34 RESNET50\nCA + A vg 85.60 85.92 85.48\nCA + Max 83.22 84.65 84.32\nCA + LIP 85.80 85.64 85.93\nCA + 1/2A vg + 1/2Max 85.25 86.27 85.75\nCA + 1/2A vg + 1/2LIP 86.93 86.69 86.38\nCA + 1/2A vg + 1/4Max + 1/4LIP 86.65 87.26 86.79\nCA + 2/3A vg + 1/3Max 85.73 86.41 86.65\nCA + 2/3A vg + 1/3LIP 87.25 86.98 87.59\nCA + 2/3A vg + 1/6Max + 1/6LIP 87.32 87.34 87.02\nCA + 1/3A vg + 2/3Max 85.20 86.32 86.24\nCA + 1/3A vg + 2/3LIP 86.88 86.96 86.75\nCA + 1/3A vg + 1/3Max + 1/3LIP 86.64 86.83 86.86\nTable 6 Result of Dif ferent Parameters of nuclear cataract classification\nLearning\nRateBatch\nSizeAccuracy\n(RESNET18)Accuracy\n(RESNET34)Accuracy\n(RESNET18)\n0.001 16 87.88 88.21 88.32 The AS-OCT  nuclear cataract dataset is shown in table 4. The total image number of the dataset  is 7919. In \nthe training process, stochastic gradient descent iis used as the optimizer , the learning rate is 0.001 and the cosine  \nannealing is used as the learning rate strategy . The software and hardware environment we used is Ubuntu  \noperating system and a Nvidia T itan GPU (10GB).\nTesting r esult\n Table 5 shows the testing  result of the experiment, the experiment used ResNet18, ResNet34, RESNET50 as \nbackbones. Four groups were given: single pooling method, 1:1 matching combination, 2:1 matching  \ncombination and 1:2 matching combination. The test results indicate that, after the pooling method  is combined,  \nthe results of the combination of each matching method are improved to some extent compared with the single  \npooling method. Among them, the 2:1 matching method with a higher proportion of average pooling stands out \nand obtains the best results, which further proves the importance of average pooling. Therefore, we also plan to \nuse this algorithm in the deployment of the system.  \nIn order to get better accuracy , we also carried out the work of adjusting parameters. Table 6 shows the final \nresults: more than 88.8% accuracy can be obtained finally .Learning\nRateBatch\nSizeAccuracy\n(RESNET18)Accuracy\n(RESNET34)Accuracy\n(RESNET18)\n0.01 16 86.93 87.67 87.44\n0.001 32 87.32 87.34 87.59\n0.01 32 87.05 87.47 86.94\n0.001 64 87.22 87.16 87.38\n0.01 64 87.13 87.25 87.52\nFigure 14. confusion matrix We analysize  the confusion matrix of our best result, as shown in figure 14. We discover that this model has a \ngood classifi cation effect on normal images, but there is still a large space of improvement in the classification  \nof mild and severe cataracts, which we aim to make a breakthrough in this direction in the future.  \n \n3.3 System  \n For the final system interface, we have designed and implemented the main interface and we can enter the \ncataract system from the main interface through buttons. In the cataract system, we have basically completed the \nappearance, and add interfaces and buttons for the concrete functions. The screenshot of our syste m is shown in \nfigure 15, figure 16.\nFigure 15. Main Interface\nFigure 16. Cataract Interface\n The algorithm part we are responsible for has been completed, and the algorithm and existing interface will be \nhanded over to another group for model deployment, so as to finally realize our system.  \nPart4 - Conclusion and Future Plans  \nIn the group groject this semester , the achievements we made is as shown below:\nAchieved ：\nAccomplished the segmentation of the Slit-Lamp and AS-OCT images;\nAccomplished the classification of the AS-OCT nuclear and cortical region;\nAccomplished the main interface.\nWhat's more, we also list some future plans to help us perfect our work in the future.\nPlans ：\nTry to use deep learning for segmentation ；\nOptimize the network model to improve classification accuracy and reduce model complexity ；\nImprove the system Interface.\n"}, {"role": "assistant", "content": "最终打分：8 (范围0-10分)\n1. 结构完整性得分：8, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：9, 占比20%\n5. 参考文献规范性得分：6, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "角膜内皮细胞健康状况评估系统\n林慧燕王标\n一、研究背景\n角膜是眼球正前方的半透明部分。它保证外界光线毫无障碍地通过到达眼底，以获得清晰\n的成像，角膜病变是全球第4大致盲疾病，据统计，全球累计有2000多万因角膜病致盲的患\n者[1]。角膜病患者如得不到及早治疗，角膜可能出现溃疡、穿孔或角膜白斑，导致失明。\n角膜内皮细胞是位于角膜后表面紧密排列的不可再生表皮细胞，形状大多为六角形。角膜\n内皮细胞层的结构完整和功能健全是维持角膜透明和正常生理作用的重要因素。角膜内皮细胞\n健康状况的评估依赖于相关的临床参数量化，比如，内皮细胞密度(ECD)、多晶性(或细胞大小\n的变化系数，CV)和多构形(或六角形细胞的百分比，HEX)等[2]。然而，手动分割角膜内皮细\n胞图像并估计相关参数费时费力。因此，自动化的角膜细胞健康状况评估系统，能够大大降低\n临床医生的工作量，对于角膜相关角膜疾病的检查、治疗具有重要的意义。\n目前，国内外的角膜内皮细胞分析计大多是一体机，量化分析系统封闭于设备内[4]。这就\n导致角膜内皮细胞健康状况的评估没有统一评价标准，不利于不同设备测量结果的横向比较，\n影响了使用不同设备的用户对于角膜健康状况的准确评估。此外，在角膜内皮细胞图像中，由\n于成像过程中光照不均、失焦，以及伪影等因素的干扰，导致细胞边界模糊，难以精确分割。\n现有的角膜内皮细胞分析计未考虑量化结果的不确定性。\n因此，本项目旨在提出了一款结合不确定性信息的角膜内皮细胞全自动定量分析系统，该\n系统独立于设备，能够排除不同设备上临场参数估计算法不一致的影响，提供统一的评价标准。\n除此之外，该系统中加入了角膜内皮细胞图像分割不确定信息的使用，能够展示图像分割在每\n个像素位置的置信程度，为用户提供额外的可视化辅助诊断信息。\n二、研究内容\n基于目前已有评估系统的局限性与角膜内皮细胞分析算法的不足，我们提出了一款结合不\n确定性信息的角膜内皮细胞全自动定量分析系统，使用深度学习模型完成角膜内皮细胞的自动\n分割，基于分割结果对细胞密度、细胞面积变异系数、六角形细胞占比等临床参数进行自动估计。同时通过在模型中加入dropout模块，引入模型不确定性，并且应用半监督学习框架，使\n用不确定性优化深度学习模型的分割精度。该系统具有以下特点：\n1．自动化分割与临床参数估计。使用深度学习模型完成对角膜内皮细胞的分割，相比较\n传统基于手工特征的方法更具鲁棒性，并基于分割结果对ECD、CV、HEX等临床参数进行自\n动估计。\n2．可视化不确定信息。在分割模型中加入dropout模块，引入模型不确定性，并对其实\n现可视化，为医生展示了分割结果的置信程度，增强了人机交互性，并提高参数量化准确性。\n3．独立的评估系统。该系统具有独立于设备的算法，排除了不同设备上算法不一致的影\n响，提供统一的评价标准。相较于传统的一体机，一方面有利于缩短算法开发周期、降低软件\n开发成本，另一方面可以更便捷地针对客户需求提供丰富的可视化分析与交互接口。\n4．可灵活部署的前后端框架。采用服务器-客户端架构，在具有大算力的服务器端提供性\n能更强大的模型，以得到更准确的临床参数量化与分析，该模式极大降低了客户端的算力门槛。\n三、技术路线\n1.优化参数估计算法\n在中期答辩时，我们优化了参数估计中多边形近似。在细胞图像上，对模型的预测结果（白\n色部分代表细胞边界）进行一系列后处理，最后进行单个细胞的形状估计。其中不同颜色标记\n代表不同的多边形，对于角膜内皮细胞健康评估系统而言，健康的六边形细胞数量是非常重要\n的参数。我们对整个后处理流程进行策略调整和优化，获得了更精确的判断。\n模型预测结果 细胞形状图我们舍弃了原来的腐蚀膨胀操作，原因是在图像闭操作过程中，细胞边界曲线会进行一定\n程度的偏移，对精确的细胞近似带来了干扰。\nDouglas-Peucker算法\n我们从模型预测结果中得到细胞边界曲线图，在细胞边界曲线图中去获取每个细胞轮廓。\n基于这样的细胞轮廓，我们使用Douglas-Peucker算法进行多边形近似判断。Douglas-Peucker\n算法的原理是：以AB曲线为例，将AB连成直线，设定阈值。选择离AB直线最远点C，计\n算C到AB距离H。如果阈值大于H，则认为AB曲线可以近似为一条直线；小于H，则将C\n设置成新的端点。重复所有上诉操作，直到轮廓所有点满足条件，细胞轮廓则近似成了某个多\n边形。\n​我们之前的做法是对于所有的细胞轮廓选择了相同的阈值进行多边形近似。因为细胞大\n小不一致，所有细胞选取相同阈值进行估计，必然存在较大误差。我们进行多次实验以找到细\n胞大小和阈值在多边形近似之间的关系。\n不同大小细胞在多边形近似算法中的分析\n阈值=1.5 阈值=2.0 阈值=2.5 阈值=3.0\n我们通过对同一张细胞图像进行多次形状估计，通过设置不同阈值以获得不同结果，在阈值从1.5增加到3的过程中，越来越多的细胞形状被近似成了四边形。\n我们根据细胞大小将细胞分成3类（以像素为单位），探究在不同阈值下细胞形状近似的\n准确度。我们利用手动数据标注图，来制作正确的细胞形状图。输入相同角膜细胞图像，进行\n模型预测和形状估计得到细胞形状图。每一个细胞进行颜色对比，计算得到准确度。基于上诉\n实验，我们最终找到每类细胞最准确的阈值，从而设计自适应算法来完成多边形近似工作。\n初始的多边形分析 优化后的多边形分析 手动标记图的多边形分析\n六边形细胞数量：25 六边形细胞数量：45 六边形细胞数量：54\n通过和最初版本形状估计所得的细胞形状图，对手动标记图的细胞形状图做近似对比，以\n及绿色六边形细胞数量的对比，可见我们此次形状估计有了较大提升。2.优化不确定性图可视化结果\n本项目使用蒙特卡洛dropout模块，当有图像输入时，dropout模块使神经元随机失效。对\n于单张图像，模型对其进行十次分割，得到十张不同的分割结果。这些分割结果的均值为作为\n最终分割，而它们的方差则用作估计不确定性，得到一张不确定性信息图，如下左图所示。\n不确定性信息图 不确定性热力图\n为了给用户更好的观感，我们使用热力图展示每个像素点的不确定性，如右上图，右\n边的图标表示不同颜色代表的不确定性值。用户可以为所展示的不确定性设置了阈值，低于阈\n值的不确定性将不会显示。通过设置阈值，用户可以更好的关注不确定性高的区域。\n针对以上的可视化优化，我们设计了由不确定性指导的人机交互流程。对于输入的角\n膜内皮细胞图像，按照一定的尺寸划分不同的区域，进行不确定性分析与计算，选择不确定性\n最低的区域进行参数估计。医生可以修改不确定性的阈值，从而使程序重新对这些区域的不确\n定性进行计算，得到新的分析结果。\n不确定性指导的人机交互3.不确定性量化指标\n从数学上看，不确定性信息的分布是一个0到0.25的矩阵。我们模型分割输出的图像\n尺寸为192*192，共36864个像素点，以0.05为间隔取区间，分析不确定性在各个区间的分布，\n如下图所示。因为模型分割的是细胞边界，图像中大部分区域都没有得到分割，所以不确定性\n小于0.05的部分占据了大多数。使用不确定性阈值，可以过滤掉不确定性低的部分。\n不确定性热力图以及对应的不确定性分布柱状图\n经过不断的尝试，我们得到初步的不确定性计算公式，如下：\n根据不确定性值划分不同的区间u∈U，n表示模型分割的细胞数量，I表示不确定性值大于阈\n值的像素集合，\n表示落在该区间u上的像素i的不确定性值。公式中，将高于阈值的值累\n加，除以参数估计得到的细胞数量，得到这张图像的不确定性。同时，对于不同区间的不确定\n性统计图也有助于用户修改不确定性阈值，用户可以直观得看出不确定性的值分布情况。\n为了检验公式的合理性，我们挑选了TM-EM3000数据集中不同成像质量的图像进行\n实验。从左往右，图像质量逐渐降低，通过公式计算的不确定性分别是7.645、5.602、4.170，\n符合人眼直观感受。TM-EM3000数据集图像以及对应的不确定性热力图\n由上述结果分析可知，目前所使用的不确定性计算公式能够表示热力图所展示的不确\n定性信息，并且对于不同质量的图像给出不同的计算结果。\n4.不确定性优化算法\n使用不确定性优化算法时，借鉴该篇文章不确定性感知自我集成模型半监督学习3D\n分割左心房[6]。在分割模型中引入半监督学习目的在于利用丰富的未标记数据来学习提升模\n型性能，以解决医学图像已标记数据少的问题。\n选择师生模型作为半监督学习方式，原因是希望图像分割预测在相同输入的不同干扰\n下是一致的，这与师生模型追求一致性目的相同。我们建立了教师模型和学生模型，其中学生\n模型通过最小化上标记数据的分割损失和未标记数据中与教师模型预测结果的一致性损失来\n学习教师模型。由于未标记数据没有标签，教师模型的预测目标可能是不准确的。为了解决这\n个问题，引入了不确定性，通过利用教师模型的不确定性信息，学生模型逐渐从有意义和可靠\n的目标中学习。具体而言，教师模型除了生成未标记数据的预测结果外，还利用蒙特卡洛抽样\n方法估计每个目标预测的不确定性。在计算一致性损失时，以老师模型不确定信息为指导，过滤掉不可靠的预测部分，保留可靠的(低不确定性)预测。因此，学生模型优化带来更可靠的监\n督，进而鼓励教师模式产生更高质量的目标。\n在角膜内皮细胞分割预测中，不确定性具体应用是以下过程。在每一次训练中，未标\n记数据加入不同噪声分别输入到老师，学生模型。师生对输入图像进行预测从而得到两张分割\n图，两张预测图像进行差异对比评估。与此同时老师模型将输入图像进行多次预测，用蒙特卡\n洛抽样方法估计得出不确定性图，设置阈值来确定低不确定性区域。利用低不确定性区域即可\n靠区域以指导学生模型学习一致性损失。\n在Alizarine，Rodrep，TM-EM3000数据集下设计如下实验验证不确定性优化算法的\n可行性。其中用Unet分割模型和Dropout的Unet分割模型，以及我们优化的UA-MTDropout\nUnet分割模型进行Dice系数对比。Unet和DropoutUnet的测试dice系数对比证明了Dropout\n的引入能够提升模型性能。不确定性优化算法目的是希望模型能够利用未标记数据学习以提升模型性能。对于\nDropoutUnet我们通过控制训练集中已标记图像的数量来得到模型性能区间。以TM-EM3000\n数据集为例，选取33张已标记图像作为训练集进行全监督学习训练，得到模型性能为区间下\n限；154张图像作为训练集进行全监督学习训练得到的模型性能为区间上限。UA-MTDropout\nUnet利用33张已标记数据和121张未标记数据进行半监督学习，由实验结果可知，其模型性\n能大于DropoutUnet的模型性能区间下限，证明利用未标记数据进行了有效的学习。在三个数\n据集上的实验数据充分证明了算法的可行性。\n四、项目总结\n本学期创新实践任务安排\n阶段 时间 工作内容\n第一阶段 3.7~3.31文献阅读\n技术调研\n第二阶段 3.31~5.5优化参数估计\n优化不确定性可视化结果\n第三阶段 5.5~6.5不确定性量化指标\n不确定性优化算法\n本学期创新实践对于设立各个阶段目标顺利实现。包括第一阶段文献调研，第二阶段\n优化参数估计和优化不确定性可视化结果，第三阶段不确定性量化指标和不确定性优化算法。\n对于本项目，未来我们可以从以下方面进行优化。对于输入的角膜内皮细胞图像，进行图像质\n量增强和图像质量分级，以及对于参数分析结果实现智能诊断分析系统。\n五、致谢\n感谢刘江老师，张煜群老师，Risa老师一个学期以来提供的大量指导和帮助！\n感谢张颖麟师姐等人提供的帮助与支持！"}, {"role": "assistant", "content": "最终打分：8.6 (范围0-10分)\n1. 结构完整性得分：8.5, 占比20%\n2. 逻辑清晰度得分：8.5, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：9, 占比20%\n5. 参考文献规范性得分：7, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "Framework of Echocardiographic Analysis System for Clinical Diagnosis\nHaowei Kuang, Shiwei Luo, Huaitang Wang\nSouthern University of Science and Technology\n{11910710, 21990005, 11913005 }@mail.sustech.edu.cn\nAbstract\nHeart is the central organ of human beings, heart disease\nwill seriously harm people’s health, serious and even life\nsafety will produce great harm. At present, the clinical\ndiagnosis of heart disease mainly uses MRI image, CT\nimage, echocardiography and so on. Echocardiography is the\nleast harmful and lowest cost diagnostic method. However,\nthe ultrasound images collected by the current equipment\nhave a lot of noise, and the images generated are not intuitive\nheart images. Reading color ultrasound images requires\nprofessional doctors to rely on their own experience. To\nsolve this problem, we hope to build a set of complete\nechocardiogram analysis system, which takes the original\nultrasound image as input, automatically completes the\nultrasonic image processing, the measurement and extraction\nof clinical indicators, and completes the comprehensive\nanalysis of echocardiography based on this, and generates a\ncomplete echocardiogram analysis report. As a systematic\nproject, what we want to accomplish in this semester is to\nexplore and build its basic framework. In the second stage of\ninnovation practice, we mainly completed the sub-tasks of\nheart failure diagnosis and super-resolution enhancement.\n1. Introduction\n1.1. Background\nThe heart is one of the most important organs in the\nhuman body. The role of the heart is to promote the blood\nflow, to provide adequate blood flow to organs and tissues,\nto supply oxygen and various nutrients, and take away the\nfinal products of metabolism, so that cells maintain normal\nmetabolism and function.\nHeart disease mainly includes heart failure, arrhythmia,\ncoronary heart disease, coronary artery abnormalities and so\non. Heart disease has the characteristics of high incidence,\nstrong harm, progressive and sudden. According to the latest\nReport on Cardiovascular Health and Diseases in China\n2020, there are 330 million people suffering from cardiovas-\ncular diseases in China in 2020, among which the number\nof heart-related diseases is more than 30 million, and the\nprevalence rate of the elderly over 70 years old is more than\n10%. The incidence rate is still on the rise. Heart diseaseis extremely harmful. Cardiovascular disease is the leading\ncause of death in urban and rural areas in China, accounting\nfor more than 40%. And the onset of general heart disease\nis insidious, the disease develops slowly, there will not be\ntoo many symptoms at the initial stage of the disease, so\nmany people miss the best treatment time. And symptoms\ncan erupt suddenly, and if not handled in time, death can\nresult directly. Therefore, doctors recommend regular heart\ncheck-ups, especially for the elderly.\nCurrent cardiac function examination methods mainly\ninclude electrocardiogram, coronary CT, coronary angiogra-\nphy, ultrasound and so on. The electrocardiogram can only\nbe used to detect some functional abnormalities, unable to\nsee the overall shape. Coronary CT and coronary angiogra-\nphy require injection of contrast agent, harmful to the human\nbody, is not suitable for regular examination. Therefore,\nnon-invasive and high accuracy ultrasound examination has\nbecome the focus of our attention.\nUltrasound examination is a medical imaging diagnosis\ntechnology based on ultrasound, which can intuitively see\nthe size, structure and pathological lesions of muscles and\ninternal organs. In addition to the patient’s basic personal\ninformation, the ultrasonic diagnosis report is divided into\nthree parts. The first is the sonogram obtained by machine\nscanning, the middle is the parameter values manually\nmeasured by the doctor on the sonogram, and the last is\nthe diagnostic opinion manually input according to these\nparameter values and the sonogram.\nAt present, except for the ultrasound images obtained\nby the imaging machine, the remaining two parts need to\nbe manually filled one by one according to the knowledge\nof doctors at the present stage, which causes great trouble\nto the ultrasound examiners.\n1.2. Problem Formalization\nFor the above reasons, we propose the formalization of\nour question:\nWe hope to build a complete set of echocardiographic\nanalysis system, which can automatically complete the pro-\ncessing of ultrasonic images and the measurement and ex-\ntraction of clinical indicators with the original ultrasound\nimages as input, and complete the comprehensive analysis\nof echocardiography based on this, and generate a complete\nechocardiographic analysis report.To complete the system, it is necessary to carry out spe-\ncific de-noising on ultrasonic images first, and then extract\nthe required parameters by processing the images on this\nbasis. Finally, knowledge map is constructed according to\nthese parameters to generate diagnostic reports. This is a\nsystematic work that requires the joint participation of many\nsub-projects. What we want to complete this semester is to\nbuild the framework of the complete process of the system.\n1.3. Difficulty Analysis\nThere are many difficulties in completing this system:\n1) Unlike radiological images, which are stable and easy\nto model, ultrasonic images are unstable due to manual\nmanipulation.\n2) The dimension of ultrasonic image is too much, and\nmany parameters need multidimensional comprehen-\nsive analysis, which is difficult to carry out.\n3) Public data sets on ultrasound images are extremely\nscarce.\n4) Heart structure is complex, different structures have\nhigh similarity, it is very difficult to accurately segment.\n5) A great deal of prior knowledge is required.\n2. Related Work\n2.1. Echocardiographic Analysis\nEchocardiography interpretation and guidelines rely\nheavily on use of quantitative measures. Image processing\ntechniques with underlying machine learning algorithms\nhave shown promise for rapid identification of structures and\nquantification of related parameters. Assessment of left ven-\ntricular volume and function was one of the first applications\nof artificial intelligence to minimise error and reduce opera-\ntor subjectivity [2, 3, 5, 23]. Methods have evolved so that,\nrecently, Knackstedt et al. demonstrated that left ventricular\nejection fraction and longitudinal strain could be analysed\nin approximately 8 s using machine learning methods[14]\n. Within 3D echocardiography, random forest models to\nidentify borders have been shown to provide an accurate\nidentification of left and right ventricular cavities so that\nderived left and right ventricular volumes are comparable to\nthose measured by cardiac magnetic resonance [4, 15, 25,\n29]. Furthermore, machine learning has been shown to aid in\nthe assessment of valvular heart disease, for example, mitral\nvalve disease [6, 7]. Automated assessments of 3D tran-\nsoesophageal echocardiograms of the mitral valve provided\nmore reproducible and consistent quantitative assessment\nof the mitral valve annulus size and its morphology than\nhuman interpretation [11, 17]. An extensive work also has\nbeen done in the field of aortic valve segmentation for plan-\nning transthoracic aortic valve implantation procedure[20,\n21, 22]. But these studies are currently only looking at\nspecific parts of echocardiography, with the aim of directly\ndiagnosing disease based on echocardiography.2.2. Diagnostic Report Generation\nAutomatically medical report generation is a significant\nand difficult task, which need to interpret and summarize the\ninsights gained from medical images such as radiography or\nbiopsy samples [9]. It needs accurate abnormality detection\nand state-of-art detailed image caption generation. It can\nbe treated as the image captioning models’ application to\nthe medical domain. There are some existing studies to\nautomatically generate medical reports or annotations for\nmedical images[9, 10, 12, 16, 24]. Most existing studies are\nbased on two public medical image-caption pair datasets.\nThe first is ImageCLEFcaption [9, 10]. ImgeCLEF-\ncaption is an evaluation campaign about medical concept\ndetection and report generation. The organization provides\na large scale dataset of 184,000 image-caption pairs. The\nsecond dataset is the public IU X-Ray dataset [8], which\ncontains 7,470 pairs of radiology images and reports. There\nare several other studies which adopt paragraph generation\nmodels to generate medical reports for X-Ray images. Shin\nand et al[24]. firstly present a deep learning model to\nefficiently detect abnormalities from an image and annotate\nits contexts. Jing and et al. [12] propose to leverage CNN to\ndetect tags, and then combine the hierarchical recurrent net-\nwork and attention mechanism to generate detailed medical\nreports.\nHowever, among the methods of more than 40 relevant\npapers so far, all of them are aimed at radiological images\nwithout exception, and the data set is also limited. So far, we\nhave not seen any methods related to the diagnostic report\ngeneration of ultrasonic images, especially echocardiogra-\nphy.\n3. Dataset\nDue to the lack of information in the original image\ndataset, we use EchoNet Dynamic dataset[18] (Figure 2)\nfor development. The EchoNet-Dynamic dataset contains\n10,030 echocardiography videos, spanning the range of typ-\nical echocardiography lab imaging acquisition conditions,\nwith corresponding labeled measurements including ejection\nfraction, left ventricular volume at endsystole and end-\ndiastole, and human expert tracings of the left ventricle as\nan aid for studying machine learning approaches to evaluate\ncardiac function. A standard full resting echocardiogram\nstudy consists of a series of videos and images visualiz-\ning the heart from different angles, positions, and image\nacquisition techniques. The dataset contains 10,030 apical-\n4-chamber echocardiography videos from individuals who\nunderwent imaging between 2016 and 2018 as part of rou-\ntine clinical care at Stanford University Hospital. Each video\nwas cropped and masked to remove text and information\noutside of the scanning sector. The resulting images were\nthen downsampled by cubic interpolation into standardized\n112x112 pixel videos. In addition to the video itself, each\nstudy is linked to clinical measurements and calculations\nobtained by a registered sonographer and verified by a level\n3 echocardiographer in the standard clinical workflow. AFigure 1. [18]For each patient, EchoNet-Dynamic uses standard apical four-chamber view echocardiogram videos as input. The model first predicts the\nejection fraction for each cardiac cycle using spatiotemporal convolutions with residual connections and generates frame-level semantic segmentations of\nthe left ventricle using weak supervision from expert human tracings. These outputs are combined to create beat-to-beat predictions of the ejection fraction\nand to predict the presence of heart failure with reduced ejection fraction. AUC, area under the curve\nFigure 2. EchoNet-Dynamic Dataset[18]\ncentral metric of cardiac function is the left ventricular\nejection fraction, which is used to diagnose cardiomyopathy,\nassess eligibility for certain chemotherapies, and determine\nindication for medical devices. The ejection fraction is ex-\npressed as a percentage and is the ratio of left ventricular\nend systolic volume (ESV) and left ventricular end diastolic\nvolume (EDV) determined by (EDV - ESV) / EDV . Besides,\nin this dataset, for each video, the left ventricle is traced at\nthe endocardial border at two separate time points repre-\nsenting end-systole and end-diastole. Each tracing is used\nto estimate ventricular volume by integration of ventricular\narea over the length of the major axis of the ventricle. The\nexpert tracings are represented by a collection of paired\ncoordinates corresponding to each human tracing. The first\npair of coordinates represent the length and direction of the\nlong axis of the left ventricle, and subsequent coordinate\npairs represent short axis linear distances starting from theapex of the heart to the mitral apparatus. Each coordinate\npair is also listed with a video file name and frame number\nto identify the representative frame from which the tracings\nmatch.\n4. Our Work\nAccording to the Suggestions provided by the teachers\nin the first defense, we based on the established plan of the\nfirst stage of defense, the first order of the diagnosis of heart\nfailure is the starting point, and the four cavity section video\ndata is used in the heart, and the ultrasonic echocardiography\nof the diagnostic task of heart failure is completed. In\naddition, in order to benefit the doctor’s observation and\nsubjective judgment of raw data, we also completed the\nhyperresolution enhancement of the heart hypervideo data.\nIn the end, we integrated these work together, and we\ninitially generated our echocardiography report Figure 3.\n4.1. Cardiac failure\nCardiac failure, refers to the inability of the heart to\nbe able to reduce the amount of blood in the circulation\nvein, causing the blood of the venous system to be filled\nwith blood, and the disease of the heart circulatory disorder.\nThe use of echocardiogram hyperdiagnosis cardiac failure is\nmainly done by calculating the EF. He needs the doctor to\nmeasure the total volume and shrinkage of the left ventricle\nin the left ventricle of the heart, and calculate the percentage\nof the volume of the final volume of the ventricular diastolic\nphase by calculating the amount of output per stroke. Then\nthere was a problem of cardiac failure based on the EF.\nBecause the diagnostic index of cardiac failure is simple, it\nis easier to complete the whole process. And it is helpfulto complete of the project, and so on, so we choose the\ndiagnosis of cardiac failure as the first step in the analysis\nof the ultrasonic echocardiography. In the case of computer\nassisted diagnosis of cardiac failure, we need to divide the\nstarting point of the center of the body cycle, and then\nthe measurement of the EF, and then predict the prediction\naccording to the previous results. To complete the computer\naided diagnosis of cardiac failure, we need to divide the\nleft ventricle and then calculate the EF, and then predict the\nresults based on the previous results.\n4.2. Diagnostic model for cardiac failure\nOur work is based on the Echonet-Dynamic [18] model\npublished in Nature. EchoNet-Dynamic has three key com-\nponents (Figure 1). First, we constructed a CNN model with\natrous convolutions for frame-level semantic segmentation\nof the left ventricle. The technique of atrous convolutions\nenables the model to capture larger patterns and has previ-\nously been shown to perform well on non-medical imaging\ndatasets [27]. The standard human clinical workflow for es-\ntimating the ejection fraction requires manual segmentation\nof the left ventricle during end systole and end diastole. We\ngeneralize these labels in a weak supervision approach with\natrous convolutions to generate frame-level semantic seg-\nmentation throughout the cardiac cycle in a 1:1 pairing with\nframes from the original video. The automatic segmentation\nis used to identify ventricular contractions and provides a\nclinician-interpretable intermediary that mimics the clinical\nworkflow.\nSecond, we trained a CNN model with residual con-\nnections and spatiotemporal convolutions across frames to\npredict the ejection fraction. In contrast to previous CNN\narchitectures for machine learning of medical images, our\napproach integrates spatial as well as temporal information\nin our network convolutions[19, 27, 28]. Spatiotemporal\nconvolutions, which incorporate spatial information in two\ndimensions as well as temporal information in the third\ndimension, have previously been used in non-medical video-\nclassification tasks[27, 28]. However, this approach has not\npreviously been used for medical data given the relative\nscarcity of labelled medical videos. We additionally per-\nformed a model architecture search to identify the optimal\nbase architecture (Figure 1).\nFinally, we make video-level predictions of the ejection\nfraction for beat-to-beat estimations of cardiac function.\nGiven that variation in cardiac function can be caused by\nchanges in loading conditions as well as heart rate in a\nvariety of cardiac conditions, it is recommended to perform\nestimations of the ejection fraction for up to five cardiac\ncycles; however, this is not always done in clinical practice\ngiven the tedious and laborious nature of the calculation. Our\nmodel identifies each cardiac cycle, generates a clip of 32\nframes and averages clip-level estimates of the ejection frac-\ntion for each beat as test-time augmentation. Finally, based\non relevant clinical knowledge, we established a knowledge\nmap for the use of EF in the diagnosis of heart failure in\nAsian patients, which guided our prediction4.3. Parameter Extraction\nCombined with the situation of the dataset we have\nand the recommendations of the doctor in the group, we\nhave finished extraction of the following parameters, the\nlength, diameter, area, volume, and ejection fraction of the\nleft ventricle at the end of diastolic and systolic stages.\nBased on the existing segmentation results, We also finished\nthe extraction of end diastolic long diameter, end diastolic\ntransverse diameter, end systolic long diameter, and end\nsystolic transverse diameter.\nThe algorithm we used is EchoNet, an end-to-end deep\nlearning approach for labelling of the left ventricle and es-\ntimation of the ejection fraction from input echocardiogram\nvideos alone. It first performs frame-level semantic segmen-\ntation of the left ventricle with weakly supervised learning\nfrom clinical expert labelling. Then, a three-dimensional\nconvolutional neural network (CNN) with residual connec-\ntions predicts clip-level ejection fraction from the native\nechocardiogram videos. Finally, the segmentations results\nare combined with clip-level predictions to produce beat-to-\nbeat evaluation of the ejection fraction (Figure 5).\n4.4. Evaluation Metrics\nFor the test dataset from Stanford Medicine that was\nnot previously seen during model training, in the original\npaper, the prediction of the ejection fraction by EchoNet-\nDynamic had a mean absolute error of 4.1annotations by\nhuman experts, and our result had a mean absolute error\nof 3.90squared error of 5.17paper, our self-trained model\nachieved the highly similar results. This is well within the\nrange of typical measurement variation between different\nclinicians, which is usually described as interobserver vari-\nation and can be as high as 13.9of less than 50under the\ncurve of 0.97. In addition, we performed re-evaluation of the\nvideos by blinded clinicians in cases in which the prediction\nof the ejection fraction by EchoNet-Dynamic diverged the\nmost from the original human annotation. Many of these\nvideos had inaccurate initial human labels (in 43image\nquality, or arrhythmias and variations in the heart rate.\n4.5. Ultrasonic image enhancement\nDue to the low resolution and significant noise of exist-\ning echocardiographic data, it can interfere with the clini-\ncian’s observation. In order to solve this problem, we carried\nout work related to ultrasound image enhancement. Based\non the preliminary research and pre-experimental results,\nwe finally selected the Real-ESRGAN model (Figure 3) for\nultrasound image enhancement, which is by far the most\neffective model for image and video super-resolution recon-\nstruction. Our input is a low-resolution cardiac ultrasound\nimage, and the output is a high-resolution image obtained\nby network reconstruction.Figure 3. Real-ESRGAN [30]\nFigure 4. Comparison between input and output\n4.6. Datasets and Implementation\nDue to the lack of high-definition echocardiographic\nvideo data, we adopt DIV2K [1], Flickr2K [26] and Out-\ndoorSceneTraining [31] datasets for training. The training\nHR patch size is set to 256. Adam optimizer is selected\nin the training process. Real-ESRNet is finetuned from\nESRGAN for faster convergence. RealESRGAN is trained\nwith a combination of L1 loss, perceptual loss and GAN\nloss, with weights 1, 1, 0.1, respectively. It uses the conv1,\n...conv5 feature maps (with weights 0.1, 0.1, 1, 1, 1) before\nactivation in the pre-trained VGG19 network [13] as the\nperceptual loss.\n4.7. Report Generation\nAccording to the results of hyperfraction, segmenta-\ntion and ejection fraction obtained above, we generated\nan auxiliary diagnostic report for heart failure (Figure 5).\nThe report includes patient information, ultrasound images,\nleft ventricular segmentation results, hyperfraction results,\ncardiac parameters, and diagnostic advice.\nWe use the knowledge of the Left-Handed Doctor Medi-\ncal Knowledge Map and the Si Zhi Chinese Knowledge Map\nto interpret the conclusions of our supplementary diagnostic\nFigure 5. Auxiliary diagnostic report for heart failure\nreports for patients’ initial understanding of their conditions\nand to give them some reference advice.\n4.8. Knowledge Graph Construction\nWe have already established a preliminary knowledge\ngraph (Figure 6), and we will continue to expand the content\nof the knowledge graph in the future. The final knowledge\ngraph will contain not only the parameters that we have\ncalculated, but also many parameters that are helpful in\ngenerating diagnostic opinions that we have not calculated,\nand whose calculations will be completed by others in the\nfuture.\n4.9. Platform Building and Function Embedding\nFinally, we have built an analysis platform [7] for\nechocardiography, integrating the functions we have com-\npleted, such as ultrasonic image processing, left ventricular\nsegmentation, parameter extraction and diagnostic report\ngeneration. At the same time, the platform will allow others\nto add more features to improve the echocardiographic\nanalysis system.Figure 6. Simple diagnostic knowledge atlas of heart failure\n5. Conclusion\nWe have completed the whole process of cardiac ul-\ntrasound diagnosis and the construction of the knowledge\ngraph as the framework of the system. In addition, we\nhave completed the construction of a cardiac ultrasound\ndiagnostic platform and the integration of the eye-brain-heart\nlinkage system platform.\n6. Future Work\nIn the future, we will continue to improve the capabilities\nof the diagnostic cardiac ultrasound system and expand\nthe content of the knowledge graph. We will extract more\nparameters based on the information contained in the knowl-\nedge graph and perform the corresponding diagnosis and\nanalysis. In addition, we will optimize the convenience of\nthe ELIA platform.Figure 7. ELIA platform"}, {"role": "assistant", "content": "最终打分：9.6 (范围0-10分)\n1. 结构完整性得分：10, 占比20%\n2. 逻辑清晰度得分：9.5, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：9.5, 占比20%\n5. 参考文献规范性得分：10, 占比10%\n6. 课程知识掌握度得分：10, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "Image Analysis Platform for Eye-Brain Interaction Research: Fundus Vascular\nSegmentation and Multi-Parameter Extraction Function\nHaowei Kuang, Shiwei Luo, Huaitang Wang\nSouthern University of Science and Technology\nf11910710, 21990005, 11913005 g@mail.sustech.edu.cn\nAbstract\nIn order to beneﬁt the early diagnosis of Alzheimer’s\ndisease and considering the high correlation between the\nbrain and the eye, we want to develop an image analysis\nplatform for eye-brain linkage studies. Under the guidance\nof doctors in the group, we plan to implement a retinal\nvessel segmentation and parameter extraction function with\naccurate results, easy to use and fast computation under the\nframework of the eye-brain linkage platform, using OCTA\nimages as the analysis object during this semester.\n1. Introduction\nIn this report, we develop an image analysis platformfor\nthe study of eye-brain interaction, perform accurate segmen-\ntation and multi-parameter analysis of fundus blood ﬂow,\nand deeply understand the correlation between fundus blood\nvessels and brain structure in AD, so as to assist in the early\ndiagnosis of AD.\nWe try to choose histogram equalization method as our\npreprocessing method, using OCTA-NET algorithm as the\nvascular segmentation algorithm. After testing, our vascular\nsegmentation algorithm plays a very good role. We also\nextracted multiple parameters from the segmented image,\nincluding vascular centerline, total length of vascular, vas-\ncular area, explants area, percentage of vascular area, fractal\ndimension and so on.. [1]\n2. Related Work\n2.1. IMED-ROSE dataset\niMED-ROSE is an OCTA dataset from iMED team[2].\nIt consists of two subsets: Rose- 1 and Rose-2. The subset\nROSE-1 consisted of 117 OCTA images from 39 subjects\n(26 patients with disease and the rest healthy controls),\ndivided into 90 images for training and 27 images for\ntesting, including superﬁcial (SVC), deep (DVC), and in-\ntraretinal vascular plexus. All OCTA scans were captured\nby the RTVue XR Avanti SD-OCT system (Optovue, USA)\nequipped with AngioVue software at a resolution of 304 ×\n304 pixels. The manual labeling of these vascular networksis scored by imaging experts and clinicians, and their con-\nsensus is taken as the basic fact. There are two different\ntypes of container annotations: centerline level annotations\nand pixel level annotations. The ROSE-2 subset contains 112\nOCT-A images from 112 eyes. Ninety images were trained\nand 22 images tested. Each image is adjusted to a grayscale\nimage of 840 × 840 pixels. All visible retinal blood vessels\nwere manually tracked by experienced clinical ophthalmol-\nogists using software (Mathworks R2018,Natwick).\n3. Data Preprocessing\nAfter trying, we chose the histogram mean method as\nthe pretreatment method of our experiment.The basic idea\nof equalization is to try to make the number of pixels in\neach gray level equal [1]. That is, there are no large swaths\nof similar pixels to improve the contrast of the image. The\nresults are as follows: Figure 1 shows the result of applying\nhistogram equalization to the OCTA image processing com-\npared to the original image. Figure 2 shows the distribution\nof grayscale values of the rightmost image before and after\nprocessing.\nThe effect of histogram equalization: to make the image\ngray levels span a wider range of gray levels, thus improving\nthe image contrast.\nAnother advantage of histogram equalization: no ad-\nditional parameters are needed and the whole process is\nautomatic.\nThe disadvantage of histogram equalization: some gray\nlevels may not be mapped after stretching, resulting in a\ngrainy image perception.\n4. OCTA-Net\nWe use a split-based coarse-to-ﬁne network, named as\nOCTA-Net, for retinal vessel segmentation in OCTA images.\nThe pipeline of OCTA-Net has two indispensable stages -\ncoarse stage and ﬁne stage, as illustrated in Figure 3. In\nthe coarse stage, a split-based coarse segmentation (SCS)\nmodule is used to produce preliminary conﬁdence maps.\nIn the ﬁne stage, a split-based reﬁned segmentation (SRS)\nmodule is used to fuse these vessel conﬁdence maps to\nproduce the ﬁnal optimized results.Figure 1. Histogram equalization(The top is the original image, and the\nbottom is the pre-processed image)\nFigure 2. grey-scale distribution(Grayscale distribution before and after\nHistogram equalization)\n4.1. Coarse Stage\nSince the dataset has both pixel-level and centerline-\nlevel vessel annotations for each en face OCTA image, the\nsplit-based coarse segmentation (SCS) module has a partial\nshared encoder and two decoder branches (for pixel-level\nand centerline-level vessel segmentation, respectively), to\nbalance the importance of both pixel-level and centerline-\nlevel vessel information, as illustrated in the coarse stage\nof Figure 3.\n1) Pixel-Level Vessel Segmentation: Pixel-level vessel\nsegmentation is a U-shape network including ﬁve encoder\nlayers and the symmetric decoder layers. A ResNet-style\nstructure with split attention module, ResNeSt block [4], is\nused as the backbone of each encoder-decoder layer.\n2) Centerline-Level Vessel Segmentation: Compared with\npixel-level annotation, vessel annotation at centerline level\naims to grade the vessels in regions with poor contrast, more\ncomplex topological structures, and relatively smaller diam-\neters. So the network structure is adjusted, several ResNeSt\nblocks followed by an up-sampling layer are appended in\nthe third encoder layer of the backbone, as the decoder of\nthe centerline-level vessel segmentation network.4.2. Fine Stage\nIn order to further recover continuous details of small\nvessels, the ﬁne stage is used to adaptively reﬁne the\nvessel prediction results from the coarse stage. A split-\nbased reﬁned segmentation (SRS) module is used as the\nﬁne stage. The structure of SRS module is illustrated in\nthe ﬁne stage of Figure 3. In order to fully integrate pixel-\nlevel and centerline-level vessel information from the SCS\nmodule, the predicted pixel-level and centerline-level vessel\nmaps and the original (single channel) OCTA image are\nﬁrst concatenated as input (total 3 channels) to the SRS\nmodule. In addition, the SRS module will produce adap-\ntive propagation coefﬁcients to reﬁne the pixel-level and\ncenterline-level maps respectively. Finally, the reﬁned pixel-\nand centerline-level maps are then merged into a complete\nvessel segmentation map, by choosing the larger value from\nthe two maps at each pixel.\n5. Parametric Extraction\nVessels Area, Explant Area, Vessels Percentage Area,\nTotal Vessels Length, Vessel Center Line, Fractal Dimen-\nsion, Vessel Curvature.\n5.1. Vessels Area\nWhite part in Figure 4.\n5.2. Explant Area\nAll valid areas in Figure 4.\n5.3. Vessels Percentage Area\nThe ratio of vessels area and explant area.\n5.4. Vessel Center Line\nUse Rosenfeld reﬁnement algorithm. Scan all pixels.\nIf the pixel is a boundary point, but not an outliers and\nendpoints, and deletion of the pixel does not change the\neight-connectivity of the surrounding eight pixels, delete the\npixel. Repeat the scan until there are no pixels in the image\nthat can be deleted. Then get the centerline image, as Figure\n5.\n5.5. Total Vessels Length\nCalculated from the vessel center line.\n5.6. Fractal Dimension\nFractal dimension expresses the internal regularity repre-\nsentation of the seemingly irregular shape of blood vessels.\nSince the box dimension is equal to the fractal dimension,\nwe calculate the box dimension to get the fractal dimension.Figure 3. Architecture of the proposed OCTA-Net network (with an example of en face of the angioram of SVC+DVC in the ROSE-1 set). The SCS\nmodule (coarse stage) is designed to produce two preliminary conﬁdence maps that segment pixel-level and centerline-level vessels, respectively. The SRS\nmodule (ﬁne stage) is then introduced as a fusion network to obtain the ﬁnal reﬁned segmentation results.\nFigure 4. Vessels Segmentation Vessels Area(white area)\nFigure 5. Vessel Center LineWe place the blood vessels on a evenly divided grid and\ncalculate the minimum number of cells needed to cover\nthe blood vessels. By progressively reﬁning the grid, the\nchange in the number of coverage required is viewed to\ncalculate the box dimension. The calculation formula is as\ndimbox(S) = lim\u000f!0logN(\u000f)\nlog (1=\u000f)Figure 6.\nFigure 6. Fractal Dimension ( \u000frepresents the side length of the lattice and\nN represents the number of cells required to cover the vessels).\n5.7. Vessel Curvature\nCurvature convolution kernel is used to extract the cur-\nvature of segmented vascular images. Firstly, the algorithm\nperforms open operation on the segmented image, that is\nﬁrst corrodes and then expands. Secondly, perform closed\noperation, that is ﬁrst expands and then corrodes. Finally,\nthe curvature of the image is calculated according to the\nformula asH=2\n666664\u00001\n165\n16\u00001\n165\n16\u000015\n16\n\u00001\n165\n16\u00001\n163\n777775\nUFigure 7.Figure 7. Vessel Curvature Formula 6.12 of [7]\n6. Experiments\n6.1. Experimental Setting\nThe proposed method was implemented with PyTorch.\nBoth the coarse and the ﬁne stage were trained with 300\nepochs and with the following settings: Adam optimization\nwith the initial learning rate of 0.0005, batch size of 2\nand weight decay of 0.0001. For more stable training, we\nadopted poly learning rate policy with a poly power of\n0.9. For training and inference of the proposed method, the\nROSE [5] subset was split into 30 images for training and\n9 images for testing. Data augmentation was conducted by\nrandomly rotation of an angle from -10° to 10°during all\ntraining stages.\n6.2. Evaluation Metrics\nTo achieve comprehensive and objective assessment of\nthe segmentation performance of the proposed method, the\nfollowing metrics are calculated and compared:\n\u000fArea Under the ROC Curve (AUC)\n\u000fSensitivity (SEN)=TP/(TP+FN)\n\u000fSpeciﬁcity(Speciﬁcity)=TN/(TN+FP)\n\u000fAccuracy(ACC)=(TP+TN)/(TP+TN+FP+FN)\n\u000fKappa score= (Accuracy\u0000pe)=(1\u0000pe)\n\u000fFalse Discovery Rate(FDR) = FP/(FP + TP)\n\u000fG-mean score [6]=pSensitivity\u0002Specificity\n\u000fDice coefﬁcient(Dice)=2×TP/(FP+FN+2×TP)\nwhere TP is true positive, FP is false positive, TN is true\nnegative, and FN is false negative. pe in Kappa score repre-\nsents opportunity consistency between the ground truth andprediction, and it is denoted as:\npe= ((TP+FN)(TP+FP) + (TN+FP)(TN+\nFN))=(TP+TN+FP+FN)2\n6.3. Performance Comparison and Analysis\nWe compared the effects of different pretreatment meth-\nods on the segmentation results on ROSE dataset and eval-\nuated the segmentation accuracy of the proposed algorithm.\nThen, we also test the portability of the segmentation algo-\nrithm on iMED-Huaxi dataset to prove that the algorithm\nhas good portability and can be used in our platform devel-\nopment.\n6.4. Segmentation Accuracy Comparisons\nIn order to compare and obtain the most suitable\npreprocessing methods, we carried out four different\npreprocessing methods for images, including histogram\nequalization method, top hat transformation method,\nCLAHE method and gamma transformation method. The\nmodel was trained on the training set and evaluated on the\ntest set respectively, and the results were as follows.\nFrom Table 1, We can see that through the improvement\nof the training method, the results of the model trained by\nus are signiﬁcantly improved compared with the results in\nthe paper. After comparing and analyzing the results of\nfour kinds of pretreatment, we ﬁnally choose histogram\naveraging method as our pretreatment method.\n6.5. Portability Test\nThen, we verify the portability of the model on iMED-\nHuaxi dataset. We used two types of images of superﬁcial\nvessels and deep vessels to verify the mobility of the model.\nSince this dataset was not manually annotated, we evaluated\nthe segmentation results under the guidance of clinicians.\nThe following Figure 8. shows the segmentation results of\nthe original images of superﬁcial vascular data and deep\nvascular data and our model.\nAfter staining observation, we found that our model\nperformed well on both types of images on the dataset both\nin overall and in detail. It is proved that our model has good\ntransferability.\nSo we can come to the conclusion. The vascular segmen-\ntation algorithm we implemented has been tested and has\ngood segmentation accuracy and portability, which can be\nused in the development of vascular extraction and analysis\nplatform functions.\n7. Platform\nWe ﬁnally succeeded in building a medical image pro-\ncessing platform with easy operation. The platform can\nbe used for processing OCTA images, allowing for vesselTABLE 1. SEGMENTATION RESULTS OF ORIGINAL IMAGES AND IMAGES OBTAINED BY DIFFERENT PRETREATMENT\nMETHODS ON ROSE\nMethod AUC ACC FDR Kappa G-mean Dice\nThesis results 0.945 0.918 0.178 0.720 0.836 0.770\noriginal image 0.949 0.921 0.146 0.728 0.832 0.775\nHistogram equalization 0.950 0.922 0.142 0.725 0.828 0.771\nTop-Hat 0.933 0.917 0.121 0.697 0.797 0.745\nCLAHE 0.950 0.922 0.104 0.716 0.807 0.761\nGamma transform 0.950 0.923 0.120 0.725 0.820 0.770\nFigure 8. OCTA image and segmentation results. From left to right: images of shallow vessels, segmentation results of shallow vessels, images of deep\nvessels, segmentation results of deep vessels\nsegmentation, browsing and management of datasets, as well\nas manual annotation, multi-parameter extraction. The name\nof the platform is ELIA, Figure 9.\nFigure 9. ELIA\n8. Conclusion\nIn this semester’s work, we developed an image analysis\nplatform for the study of eye-brain linkage. We conducted\nrepeated model training for the OCTA-NET method of\nvessel segmentation and OCTA vessel image extraction,and improved the quality of the model by adjusting the\ntraining method and adding the pretreatment method, thus\nrealizing the function of vessel segmentation. In addition,\nwe also tested the segmentation method on multiple data\nsets to verify the portability of the method. We have also\ndeveloped multi-parameter extraction capabilities for blood\nvessels and integrated them into a platform that clinicians\ncan use easily. The professionalism and accuracy of our\nfunctions and the convenience of our platform have been\nrecognized by professional doctors.\n9. Acknowledgement\nIn the end, we would like to express our special thanks\nto Professor Jin Zhang and Professor Jiang Liu for their\nguidance and help throughout our project. In addition, we\nwould like to thank Senior sister Hanpei Miao, Senior\nbrother Zhongxi Qiu and Senior Brother Richu Jin for their\ngreat help to our project. Without them, our project would\nnot have been completed."}, {"role": "assistant", "content": "最终打分：9.5 (范围0-10分)\n1. 结构完整性得分：10, 占比20%\n2. 逻辑清晰度得分：9.5, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：9.5, 占比20%\n5. 参考文献规范性得分：9, 占比10%\n6. 课程知识掌握度得分：10, 占比10%\n"}]}
