{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "卷积神经网络在皮肤病相关的研究进展综\n述\n报告人：王奕童、刘仝\n一、CNN模型介绍\n卷积神经网络（ConvolutionalNeuralNetworks,CNN）的研究最早可以追溯到福岛邦彦在1980\n年提出的neocognitron模型。而第一个卷积神经网络是由AlexanderWaibel等提出的时间延迟\n网络（TimeDelayNeuralNetwork,TDNN），用与进行语音识别。\n经典的CNN模型主要包含卷积层，降采样层（池化层）和全连接层三部分。卷积层主要是对输\n入数据进行特征提取。降采样层会将卷积层输出的特征图进行特征选择和信息过滤。全连接层\n等价于传统神经网络的隐藏层，对前面变换的特征进行处理，可作为输出层。\n二、CNN在皮肤病相关的发展历程\nCNN在目标检测、图像识别及处理等领域已经显示了巨大的优势。也因此，近年来，CNN在皮\n肤病相关的研究越来越多。通过学习已有的皮肤病资料，用于进行皮肤病的分类。\n2018年，李航等人提出一种皮肤镜图像自动评估的新框架，利用深度学习方法，使其在有限的\n训练数据下产生更具区分性的特征。首先在大规模自然图像数据集上预训练一个深度为152层\n的残差神经网络（Res-152），用来提取皮肤病变图像的深度卷积层特征，并对其使用均值池化\n得到特征向量，然后利用支持向量机（SVM）对提取的黑色素瘤特征进行分类。在公开的皮肤\n病变图像ISBI2016挑战数据集中，用所提出的方法对248幅黑色素瘤图像和1031幅非黑色素瘤\n图像进行评估，达到86.28%的准确率及84.18%的AUC值。与现有使用传统手工特征的研究相比，\n或仅与深层神经网络的全连接层提取特征进行分类的方法相比，新方法能够产生区分性能更强的特征表达，可以在有限的训练数据下解决黑色素瘤的类内差异大、黑色素瘤与非黑素瘤之间\n的类间差异小的问题。\n2018年，何雪英等人以VGG19模型为基础架构，训练了一个结构化的深度卷积神经网络，实\n现了色素性皮肤病的自动分类。采用数据增强对数据进行预处理；接着将在ImageNet上预训练\n好的模型，迁移至增强后的数据集进行调优训练，训练过程中通过设置Softmax损失函数的权\n重，增加少数类判别错误的损失，来缓解数据集中存在的类别不平衡问题，提高模型的识别率。\n最终实验结果显示，该方法的识别率和敏感性可分别达到71.34%、70.01%，相比未设置损失函\n数的权重时分别提高了2.84%、11.68%。\n2019年，针对皮肤病中基底细胞癌与脂溢性角化病的临床特征非常相似、不易区分的问题，许\n美凤等人提出了一种面向皮肤病识别的多卷积神经网络模型融合方法。使用迁移学习方法训练\nResNet，Xception，DensNet共3个CNN模型，获得每个模型的最佳识别结果。在此基础上，\n利用传统融合方法、投票法和均方误差作为损失函数来融合多模型的识别结果，从而提高皮肤\n病的识别精度。为了克服皮肤病识别中噪声的影响以提高模型的精度和泛化能力，提出将最大\n相关摘准则（MCC）作为多CNN模型融合的目标函数，并使用梯度上升法学习不同模型对最终\n结果的贡献权重，从而建立基于MCC的多CNN融合模型。最终在基底细胞瘤和脂溢性角化病\n数据集上的实验结果表明：相比于多个单模型的预测结果，提出的使用模型融合的方法取得了\n更高的识别准确率；相比于传统模型融合方法，提出的基于MCC的多CNN融合识别模型可以\n较好地克服噪声并具有较强的泛化能力，识别准确率达到了97.07%，超过了CNN单模型和传统\n的多模型融合方法。\n2019年，JessicaVelasco等人利用MobileNet模型，对7种皮肤病进行迁移学习，并在Android\n应用上创建皮肤病分类系统。采用欠采样法和默认预处理方法对输入数据进行预处理，准确率\n达到84.28%。使用非平衡数据集和默认预处理输入数据的准确率为93.6%。使用数据集的过采样，\n模型达到了91.8%的准确率。最后，采用过采样技术和数据增强技术对输入数据进行预处理，可\n获得94.4%的准确率。\n2019年，ZHEWU等人研究了基于临床图像的面部皮肤病分类的不同CNN算法。利用中国最大\n的皮肤病临床图像数据集，建立了一个包含2656张人脸图像的数据集。包含六种常见皮肤病[脂\n溢性角化病(SK)、光化性角化病(AK)、酒渣鼻(ROS)、红斑狼疮(LE)、基底细胞癌(BCC)和鳞状细\n胞癌(SCC)]。使用五种主流网络算法对数据集中的这些疾病进行分类，并对结果进行比较。然后\n使用相同疾病类型但来自其他身体部位的独立数据集进行研究，在模型上进行迁移学习。对比\n性能，使用迁移学习的模型对几乎所有结构都获得了更高的平均查准率和查全率。在包含388\n张人脸图像的测试数据集中，最佳模型针对LE、BCC和SK分别实现了92.9%、89.2%和84.3%的查全率，平均查全率和查准率分别达到77.0%和70.8%。\n2019年，ShuchiBhadula等人选择了随机森林、朴素贝叶斯、逻辑回归、核支持向量机和CNN\n五种不同的机器学习算法，在皮肤感染数据集上执行，以预测皮肤病的类别。类似的检查依赖\n于混淆矩阵参数和训练精度已执行和描绘利用图。结果发现在所有选取的样本中，CNN对于皮\n肤病正确预期的训练精度是最高的。\n2019年，XinHe等人实现了一种基于多个CNN模型的集成方法，在构建的两个数据集中分别\n获得了79.01%和53.54%的识别率。还通过在数据集中引入边界框，做出了一种基于对象检测的\n方法。最终研究结果表明，对象检测可以帮助提高某些皮肤病分类的准确性。\n2020年，CapsNet（胶囊网络）是在卷积神经网络之后的一种新的神经网络。CapsNet的矢量\n化特征能够较好地表达空间关联性，每一个胶囊独立地服务各自的任务。李励泽等人改进了网\n络模型从而避免过拟合现象发生，基于改进CapsNet针对预处理之后的皮肤图像进行识别，并\n与传统卷积神经网络的模型作对比。最终实验结果表明，使用改进CapsNet对色素性皮肤病进\n行识别可以有较好的识别效果，并且准确率比传统方法高出8%~10%。\n2020年，EvginGoceri等人使用了在图像分类中应用广泛的五种深度网络来比较对数码彩色照\n片中的血管瘤、酒渣鼻和寻常痤疮疾病进行自动分类的性能，分别是VGG16、VGG19、GoogleNet、\nInceptionV3和ResNet101，并从准确性、精度和针对性方面对从这些网络获得的结果进行了比\n较评价，并计算了F1分数和Matthew的相关系数值。最终结果表明，ResNet101模型在上述图\n像分类达到了77.72%的准确率，高于其他模型。\n2020年，MasumShahJunayed等人提出了一种新的基于深度CNN的方法，用收集的数据集对\n五种不同类型的湿疹进行分类。并将数据增强用于转换图像以获得更好的性能，使用正则化技\n术，如归一化和dropout帮助减少过拟合。最终提出的模型达到了96.2%的精度，超过了目前的\n性能水平。\n2021年，针对皮肤镜图像数量不充足以及各类疾病之间影像数据不平衡的问题，邵虹等人提出\n一种融合类加权交叉滴损失函数和分层卷积神经网络的皮肤镜图像分类方法。首先对皮肤镜图\n像进行色彩恒常化处理，消除环境光源噪声；然后构建基于ResNet50的分层卷积神经网络，\n并在迁移学习的基础上构建二分类和多分类卷积神经网络模型，根据皮肤镜图像的数量特点设\n置类加权交叉滴损失函数。实验结果表明,该方法具有较好的分类效果，分类准确率达到了85.94%，\n与未改进的分类模型ResNet50相比，测试准确率提高了5.75%。三、CNN在皮肤病相关的算法及其改进\n1.机器学习算法在皮肤病检测相关的算法与实现\n对皮肤病的识别和检测是当前医疗行业面临的主要挑战之一。由于环境污染和营养不良等\n问题，皮肤病相关患者数量以更快的速度增长，因此早期发现皮肤相关疾病的现实意义日渐显\n现，越来越多的研究学者投入到这一问题的研究中。而机器学习方法凭借其对皮肤疾病的有效\n识别能力，逐渐成为了解决皮肤疾病识别检测的主流算法之一。机器学习算法中有众多的实现\n模型，其中卷积神经网络以其最佳的训练精度，成为了机器学习中最主要使用的算法模型之一。\n本文将以皮肤病相关的五种机器学习算法为例，基于《MachineLearningAlgorithmsbased\nSkinDiseaseDetection》这一篇文章，主要介绍不同机器学习算法在皮肤病识别上的研究与\n应用。\n机器学习是人工智能的一种实现方式，它使计算机程序能够学习而无需相应地做逐一定制。\n在许多领域，机器学习可以有良好的实现与应用，例如，医学诊断、图像处理、预测、分类、\n学习关联、回归等。在这些领域中，机器学习的图像识别和分类技术是目前研究的主要方向之\n一，主要让计算机图像感知程序学习具体的实例，并将其映射到众多实例之一，从而完成图像\n的分类任务。\n从算法模型的角度，这篇文章主要讨论了逻辑回归、核支撑向量机、朴素贝叶斯、随机森\n林以及卷积神经网络这五种皮肤病分类检测相关的机器学习算法，在痤疮、扁平苔藓癣以及SJS\n综合征三种皮肤相关的疾病的检测中的应用。作者在实验中将每个算法在同一数据集上运行十\n次得到他们的训练精度，并将结果进行了讨论比较与可视化分析，从而确定用于皮肤病分析检\n测的最佳机器学习算法。\n为了对预测结果进行评估，这篇文章使用了混淆矩阵来将模型预测的正确与错误情况做汇\n总。该矩阵描述了一个经典模型在一个给定的数据集上的性能。它显示了分类器所犯的错误以\n及正在犯的错误的类型。真阴性（TN）、真阳性（TP）、假阴性（FN）和假阳性（FP）分数都\n存储在每个类的一个矩阵中。作者通过混淆矩阵，可以计算出各种指标如准确率、召回率、\nF-score等等，如下表所示：如表IV和表V所示，我们可以得出CNN能够提供所有模型中的最佳准确率，分别为\n99.05%和96%，能够提供最低的错误率，为0.04。图表分析表明，CNN在五种算法中对皮肤\n病的预测更准确，降低了误诊的可能性。如果在早期阶段就正确地预测了疾病的类别，就可以\n开始更好的治疗。作者认为CNN的多层感知特性，提供了更精确的分类结果，是这种精度背\n后的关键。\n最后，作者对这篇文章的实验做了总结。皮肤病的检测是医疗行业的主要问题之一，如果\n早期诊断得当，可以完全治愈。机器学习算法有可能对皮肤病的早期检测产生影响。它可以帮\n助人们实时调整他们的皮肤。如果应用得当，这些技术可以对于预防皮肤问题的提供有效的建\n议与帮助，从而帮助患者和医生及时治愈皮肤病。如果未来有更多的实时数据，就可以利用人\n工智能的最新进展和人工智能辅助诊断的好处来探索皮肤病的检测模型。\n上述基于五种机器学习模型和三种皮肤病的对比实验，有效论证了卷积神经网络在同类型\n的机器学习算法中的优越性。其模式和算法的提出，丰富了卷积神经网络模型在皮肤病检测上\n的研究，并为今后新型机器学习算法的设计研究提供了可以借鉴参考的思路。\n2.CNN网络模型在皮肤病相关的发展与改进\n目前卷积神经网络的不同算法与网络结构在皮肤病相关的临床图像上是一个重点的研究方\n向。其间有许多文章探讨这一主题。经过筛选，我们选择以《StudiesonDifferentCNN\nAlgorithmsforFaceSkinDiseaseClassificationBasedonClinicalImages》这一篇由ZWu\n等人在2019年发表于IEEEAccess上的文章，对CNN在皮肤病相关的网络模型进行了深入的\n阅读和研究。这篇文章研究的问题是五种不同卷积神经网络算法在六种皮肤病的分布和检测。这六种文\n章中研究的皮肤病，主要是指脂溢性角化病（SK）、光化性角化症（AK）、酒渣鼻（ROS），红\n斑狼疮（LE）、基底细胞癌（BCC）和鳞状细胞癌（SCC）。这些皮肤病易引起患者出现皮肤损\n伤，皮肤鳞片化，色素沉着等不良症状。这些不良症状在损害身体健康的同时，也会增加出现\n心理疾病的患病风险。随着互联网大数据的快速发展和硬件计算水平的提高，深度学习算法如\nCNN在医学图像领域有很大突破，对这六种皮肤病的检测分类也有了一定的发展。但目前对于\n临床图像的研究上不充足以及临床图像大多数基于特定环境，致使图像处理任务变得困难。\n在CNN与皮肤病相关领域目前的研究中，主流的研究方向是将深度学习算法和皮肤病检测\n的相关问题做结合。如使用初始-v3网络对皮肤肿瘤进行分类的任务中，深度学习模型已经达\n到了专业皮肤科医生的水平；对于9类肿瘤，计算机模型的识别准确率达到55.4%，而两名皮\n肤科医生的准确率仅分别达到53.3%和55.0%。因此深度学习模型在皮肤病相关问题有充足的\n研究前景。\n在文章中，作者将五种主流的卷积神经网络算法在皮肤病的相关检测中做了比较实验，并\n重点研究了其在不同疾病中的检测效果。这篇文章使用的数据集是一个基于面部皮肤病图像的\n数据集，来自于已有的Xiangya-Derm数据集。该数据集由543种皮肤病，150223张临床图\n像组成，并配备有相应的病理和病史，是最大用于计算机辅助诊断的皮肤病临床图像数据集。\n但这些数据中存在来源多样，数据失衡等问题。为了解决数据不平衡的问题，作者还使用了随\n机反向裁剪的方法，并且针对不同的疾病，提出了不同权重的成本函数，从而实现了不同网络\n算法在不同疾病中的比较实验。下表为其在这篇文章中使用的数据集说明。作为卷积神经网络算法模型，其学习过程需要确定的网络结构。这篇文章比较研究的五种\n主流网络结构分别是：ResNet50，InceptionV3，DenseNet-121，Xception以及Inception-Resnet\nV2。这些网络结构各有特点，均有其实际中的应用价值。\nResNet增加了网络的浅层和深层之间的连接。这种连接直接将浅层的信息传递到深层。另\n一方面，在反向传播过程中，梯度向浅层的传播大大增加了网络层的数量。\nInceptionV3模型有不同形状，不同大小的内核，从而能够结合来自不同内核的输出结果，\n因此Inception模型可以有效提高网络的宽度和接受输入域的多样性，从而提高对不同物体的识\n别程度。\nDenseNet在每两层之间添加连接；也就是说，每个层的输出特性映射将被用作所有后续\n层的输入。DenseNet利用这些密集的连接和网络重用特性，从而以减少参数的方法提高性能，\n从而使计算效率更高。\nXception模型是Inception模型的更新版本。它利用深度可分离卷积改进了初始模块。这种\n变化解耦了空间相关性和跨通道相关性。在相同的参数下，它可以获得比Inception-v3更好的\n性能。\n在某种程度上，Inception-ResNet是Inception和ResNet的组合。通过在初始网络中添\n加一个残差连接，它可以在保持网络的规模多样性的同时训练更深层次的网络，从而提高性能。\n到目前为止，皮肤病相关的主流卷积神经网络算法模型已经介绍完毕。在完成了主流卷积神经网络模型的设计与构建后，作者开始对不同皮肤病进行横向的比较实验。作者对上述的五\n种网络结构和六种主要皮肤疾病进行了训练和测试，然后进行了两两比较。作者对实验数据还\n增加了混淆矩阵与假阳性曲线，做到了尽可能详细的比较和分析。\n作者总结的实验结论如下：\n1.卷积神经网络具有识别检测面部皮肤疾病的能力\n2.使用更加合理的卷积神经网络结构能够有效提升模型的性能\n3.对于不同身体部位的疾病，应该采用不同的模型和权重\n在完成了实验和对结果的分析后，作者也对卷积神经网络在皮肤病中的应用提出了改进的\n方向。首先是目前CNN的网络结构和检测效果基本令人满意，但总体还有可待提升的空间；其\n次是如果想要在真正的实际生活中应用该技术完成对人们皮肤健康的检查，那还需要开发出专\n门对口的改进模型与产品。\n整体而言，这篇论文的研究方向与研究方法与当前国内外研究进展较为吻合。通过阅读并\n学习此篇文章的研究思路和所使用的深度学习理论、强化学习思想以及具体的网络架构解析、\n注意力机制原理以及各种决策过程和函数的推演，我们能获知当前前沿研究的大致走向与成果。\n四、CNN在皮肤病相关的应用\n在皮肤病的研究领域中，皮肤病的分类问题和诊断问题是主要的两个研究方向。接下来我\n们将针对这两个主流问题，分别介绍CNN在这两个问题中的实际应用情况。\n1.CNN+皮肤病分类\n皮肤科是临床医学的重要分支之一，其在临床环境的分类是该分支的重要课题之一。分类\n过程很大程度上依赖于具体皮肤病变的形态，而由于皮肤病本身类型广泛，人工观察下的外观\n可能高度相似，给皮肤病的有效检测和诊断带来了困难。人工智能中卷积神经网络本身的理论模型经过多年的研究和演进，已经取得了长足的发展，在图像分类识别问题上有广泛的应用，\n因此基于卷积神经网络直接从医学图像中学习皮肤疾病的临床相关特征的深度学习算法成为目\n前主要的研究方向之一。\n虽然目前的研究通过不同数据库和数据集训练得到的数据在特定目标疾病中取得了可接受\n的准确率，但现有的研究由于大多数基于内部封闭的数据，训练效果在真实的临床环境中却相\n对不足，训练出的模型在真实临床环境的应用情况表现较差。\n针对真实临床环境的挑战，新式的多分类网络结构设计是一种可行性较高的解决方案。朱\n晨雨等人基于EfficientNet-b4CNN算法提出的新式网络模型在不同的中间层之间使用多个辅\n助预测模块来积累来自不同特征层次的鉴别信息，从而拥有在真实临床环境下优于普通卷积神\n经网络的表现。他们以EfficientNet-b4为骨架，采用迁移学习方法进行CNN模型构建（图2）。\n随后，进一步将本研究构建的CNN模型与既往文献报道先进的CNN模型以及280名皮肤科\n医师的分类准确性进行比较。\n本研究构建的CNN模型具有较好的分类水平，对14种皮肤疾病分类的总体准确性达\n94.8%，灵敏度为93.4%，特异度为95.0%，受试者工作特征曲线的曲线下面积为0.985（图3）。\nGrad-CAM特征显著图及t-SNE降维可视化结果均显示本研究构建的CNN模型有极佳的可区\n分性及高分辨率。作者在文章中提出的新式卷积神经网络模型的实际应用效果有效说明了文章中的结论：本\n研究建立的CNN模型可准确分类14种临床最常见皮肤疾病并辅助医师进行高效而精准的临床\n诊断。通过基于真实诊断数据的临床试验，卷积神经网络模型在真实临床环境下已经得到了广\n泛有效的实际应用。\n2.CNN+皮肤病诊断治疗\n皮肤科领域的另一个重要研究问题是皮肤病的相关诊断与治疗。皮肤病类型众多，传统的\n皮肤病的诊断是基于综合考虑病变区域的大小、形状、颜色等视觉特征。近年来，深度学习方\n法已应用于许多医疗任务，并取得了显著的成就，许多对皮肤病图像的研究取得了很好的效果，\n因此使用计算机来帮助诊断皮肤病将是一个很有前途的发展方向。\n但CNN在诊断技术的发展也有较大的技术困难。对于训练与实际检测，由于皮肤镜图像代\n价高昂和不便访问等因素，更多情况使用的是实际的临床图像。从实际临床图像中进行皮肤病\n诊断资料主要面临以下挑战：\n（1）临床图像的背景更加复杂，模型更易受到噪声干扰。模型如何在受到临床图像的噪声干\n扰时仍能保持诊断准确率是需要考虑的一个重要课题。\n（2）临床图像覆盖的疾病范围更广。皮肤镜图像主要是为了检测皮肤癌如黑色素瘤和基底细胞癌等而建立的，因此临床图像需要做的诊断类型就更多，有可能受到相似图像却不同\n类别疾病的影响。\n（3）临床图像识别本质上是一个细粒度的深度学习分类任务，因此比一般的分类任务有更高\n的分类要求。\n为了验证CNN等神经网络模型在临床皮肤病诊断任务上是否能有较好的表现，XinHe等人\n在2019年构建了Skin-10和Skin-100两个临床诊断上的数据集，并在两个数据集上评估了他们\n基于目标检测的神经网络的表现效果。\nCNN表现效果的实验验证需要广泛普适的数据集做支撑。在这篇文章中，Skin-10和\nSkin-100的数据集来源于网络。其中Skin-10包含10218张图像，而Skin-100包含19807张图\n像，这二者的大小都比当前存在的大多数临床数据集要大得多。除此以外，作者包含的数据集\n包含各种年龄、性别和病变位置，因此作者的实验能够说明CNN的深度学习模型在皮肤病上的\n诊断治疗效果。\n文章中的实验以Skin-10和Skin-100为数据集，从两类模型中选择了四种有代表性的模型，\n并从两个维度评估了模型在SOTA对象检测中的有效性。通过反复实验和参考先前的研究，数\n据完成了一系列调整如调整大小，随机裁剪，翻转旋转、归一化等等，并使用随机梯度下降优\n化的方法，将学习率设置为0.01，将交叉熵作为损失函数。模型的基础实现也做了小幅调整，\n从而实现模型的收敛，最终得到如下结果：\n我们从上述表格可以看到：一方面，去除图像中的噪声有利于提高CNN模型的分类准确率；\n另一方面，经过数据清理后，来源于NAS设计的模型鲁棒性不如人工设计的模型，也就是说这\n些人工设计的模型能够更加鲁棒，能够提取更多的诊断识别特征。\n基于上述实验结果，作者归纳总结了CNN在目前皮肤病诊断治疗的进展情况与相关结论：\n（1）去除噪声对提升基于卷积神经网络的检测模型有显著帮助；（2）在基于卷积神经网络的实现模型中，目标检测模型会比目标分类模型有更好的表现；\n（3）AutoML（自动机器学习）技术生成的结构鲁棒性会强于人工设计的结构。\n在完成实验后，作者也对未来CNN在诊断资料方向上的发展做了展望。首先，未来CNN\n的网络结构设计可以考虑应用AutoML技术来生成基于临床诊断数据集的特定网络结构；其次，\n尽管上述方法易于实现且卓有成效，但面对难以分类的类型的准确率提升却无能为力，因此提\n高CNN的诊断的准确率仍然是一项比较大的挑战；第三，目标检测时使用局部区域的方法虽然\n可以削弱背景噪声带来的负面影响，但也损失了全局特征信息，因此将全局特征和局部特征相\n结合是未来的一个重要发展方向。\n整体而言，这篇文章系统性地介绍了CNN在皮肤病诊断治疗方面的工作与成果。通过阅读\n学习这篇文章的算法与网络结构，我们能够对如何将CNN深度学习模型更有效地应用于皮肤病\n的诊断有一个全面具体的认识。\n五、CNN在皮肤病相关的研究展望\n1.卷积神经网络的多输入\n卷积神经网络仍然只能接受单一图像输入，对于图像清晰度识别，视频异常判断等应用，\n需要多幅图像综合分析才能得到结果。因此构造多图像输入的深度卷积神经网络具有极高的理\n论和应用价值。\n2.卷积神经网络的泛化能力\n在生产实践中，需要人工为每个数据设置类标号，非常耗费时间。因此研究非监督或者半\n监督卷积神经网络是提高泛化能力，进一步加深卷积神经网络应用的重要途径。另外还可以利\n用核函数等结构最优化方法提高网络全连接层的泛化能力．\n3.卷积神经网络的卷积核关联\n现有卷积神经网络的结构与真实视觉结构仍有很大不同，单个卷积核内部元素在训练时相\n互独立。如果能够将卷积核各像素之间的关系考虑进来，充分发挥卷积核的优势，则隐藏层可\n以捕获更多特征信息，在图像重建和识别上可以取得更好的效果。\n4.卷积神经网络的反馈学习\n现有卷积神经网络的应用更注重最终的识别结果，而没有充分用到不同层次级别的图像特征，\n或者是研究深度卷积神经网络逐层特征之间的关系，因此未来制定高层信息对低层信息的反馈\n可能是进一步提升图像处理质量的关键。"}, {"role": "assistant", "content": "最终打分：8.4 (范围0-10分)\n1. 结构完整性得分：8, 占比20%\n2. 逻辑清晰度得分：8.5, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：7, 占比10%\n6. 课程知识掌握度得分：10, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "#11910501\n11910934CVPR\n#11910501\n11910934CVPR 2023 Submission #11910501 11910934. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\nMOT subsystem in Auto Drive\nAnonymous CVPR submission\nPaper ID 11910501 11910934\nAbstract\nThis survey investigates the MOT, which is moving ob-\nject tracking, subsystem in the architecture of auto driving\nsystem. We first present the overview of an auto driving\nsystem, including perception system and decision system.\nThen, we focus on the MOT subsystem, introducing some\ncommon implementation techniques. We end this introduc-\ntion with some problems we found in these techniques, and\nintroduce collaborative perception to solve them and im-\nprove the MOT subsystem.\n1. Introduction\nSince the mid-1980s, many universities, research cen-\nters, automotive companies, and companies in other indus-\ntries around the world have been researching and develop-\ning self-driving cars (also known as autonomous and driver-\nless cars) [1] [2] [3]. Important examples of self-driving ve-\nhicle research platforms in the last two decades are Navlab’s\nmobile platform, the University of Pavia and the Univer-\nsity of Parma’s car, ARGO, and UBM’s vehicles, VaMoRs\nand VaMP. To stimulate the development of self-driving\ncar technology, the Defense Advanced Research Projects\nAgency (DARPA) has organized three competitions in the\npast decade [6] [4] [16] [15].\nIn the first competition, self-driving cars are required to\ndrive a 142-mile route across a desert trail in a 10-hour pe-\nriod. All participating cars failed within the first few miles.\nIn the second competition, self-driving cars were required\nto drive a 132-mile route through flatlands, dry lake beds\nand mountain passes, including three narrow tunnels and\nmore than 100 or so sharp turns. The competition had 23\nfinalists, and four cars completed the route in the allotted\ntime [14] [13]. In the third competition, self-driving cars\nwere required to drive a 60-mile route in a simulated urban\nenvironment with other self-driving and human-driven cars\nover a six-hour period. The cars had to obey California traf-\nfic rules. There were 11 finalists in this competition and 6\ncars completed the race in the allotted time [12].\nTo measure the level of autonomy of self-driving cars,the Society of Automotive Engineers International (SAE)\nhas published a classification system based on the level of\nhuman driver intervention and attention they require, where\nthe level of autonomy of self-driving cars can range from\nLevel 0 (where the car’s autonomous system issues a warn-\ning and may intervene momentarily, but there is no contin-\nuous control of the car) to Level 5 (where no human inter-\nvention is required in any situation) [5] [7].\nThrough the history of this famous self-driving car\ncompetition, we can see that self-driving techniques have\nevolved quickly in the past several decades. The typical\narchitecture of self-driving cars were also decided and sta-\nbilized during the research [8]. The architecture of au-\ntonomous systems for self-driving vehicles is usually di-\nvided into two main parts: the perception system and the\ndecision system. The perception system is typically di-\nvided into many subsystems responsible for tasks such as\nautonomous vehicle localization, static obstacle mapping,\nroad mapping, moving obstacle detection and tracking, and\ntraffic signalization detection and recognition. The decision\nsystem is also typically divided into many subsystems re-\nsponsible for tasks such as route planning, path planning,\nbehavior selection, motion planning, obstacle avoidance,\nand control [9].\nThe rest of this paper is structured as follows. In Section\n2, we present the typical structure of an autonomous sys-\ntem for self-driving cars, consisting of the decision system\nand their subsystems. In Section 3, we introduce common\ntechniques for implementing MOT subsystems, presenting\nthe problems of the current mainstream implementation ap-\nproaches. In Section 4, we introduce the newly emerged\ncooperative perception technology, aiming to use this new\ntechnology to solve the problems we identified.\n2. Typical architecture of self-driving cars\nIn this section, we present a typical architecture of an au-\ntomated system for self-driving cars, consisting of the deci-\nsion system and their subsystems.\nFig. 1 shows a block diagram of a typical architecture\nof an automated system for a self-driving car, where the\nperception and decision system is shown as a collection of\nCVPR\n#11910501\n11910934CVPR\n#11910501\n11910934CVPR 2023 Submission #11910501 11910934. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\nFigure 1. Overview of the typical architecture of the automation\nsystem of self-driving cars. TSD denotes Traffic Signalization De-\ntection and MOT denotes Moving Objects Tracking\nsubsystems in different colors. The perception system is\nresponsible for the estimation of the state of the car and\nthe creation of an internal (to the autonomous driving sys-\ntem) representation of the environment using data captured\nby on-board sensors such as Light Detection and Ranging\n(LIDAR), Radio Detection and Ranging (RADAR), cam-\neras, Global Positioning System (GPS), Inertial Measure-\nment Unit (IMU), odometer, etc., as well as models about\nthe sensors, road network, the traffic rules, vehicle dynam-\nics, and other a priori information. The decision system is\nin charge of navigating the car from its initial position to a\nuser-defined final destination, taking into account the cur-\nrent state of the car and the internal representation of the\nenvironment, as well as the traffic rules and the passenger’s\nsafety and comfort.\nTo navigate the car across the environment, the decision\nsystem needs to know where the self-driving car is located\nwithin it. The localizer subsystem is in charge of the estima-\ntion of the car’s state (pose, linear velocity, angular veloc-\nity, etc.) from static maps of the environment. Such static\nmaps, or offline maps, are computed autonomously prior to\nthe self-driving operation, usually by using the self-driving\ncar’s own sensors, although manual annotation or editing is\noften required. Self-driving cars can use one or more of-\nfline maps for localization, such as occupancy grid maps,\nmitigation maps, or landmark maps.\nInformation about the rules and regulations (traffic di-rection, maximum speed, lane delineation, etc.) governing\nhow self-driving cars move on roads and highways is also\ncrucial for decision-making systems. This information is\nusually embedded in road maps, where geometric and topo-\nlogical features are used to represent it in such maps.\nThe localizer subsystem receives offline maps, sensor\ndata, and the odometer of the self-driving car as inputs and\ncalculates the state of the car as an output (Figure 1). It\nis worth noting that although GPS can help the position-\ning process, GPS positioning alone is not sufficient in urban\nenvironments due to interference caused by tall trees, build-\nings, tunnels, etc., which makes GPS positioning unreliable.\nThe mapper subsystem receives an offline map and the\nstate of the self-driving car as input and generates an online\nmap as output. This online map is usually a merging of the\ninformation in the offline map and an occupancy grid map\ncalculated online using sensor data and the current state of\nthe car. It is preferable that the online map contains only\na static representation of the environment, as this may as-\nsist in the operation of certain subsystems of the decision\nsystem.\nInformation about the attitude and speed of moving ob-\nstacles is also vital to the decision-making system. This\ninformation enables the system to make decisions to avoid\ncollisions with moving obstacles. It also allows moving ob-\nstacles to be removed from the online map. The moving ob-\nject tracking subsystem, the MOT, receives the offline map\nand the state of the self-driving car and detects and tracks,\ni.e. calculates, the pose and speed of the nearest moving ob-\nstacle (e.g. other vehicles and pedestrians). We survey the\nliterature on moving object detection and tracking methods\nfor self-driving cars in Sec. 3.\nHorizontal (i.e. lane markings) and vertical (i.e. speed\nlimits, traffic lights, etc.) traffic signalisation must be recog-\nnised and obeyed by autonomous vehicles. The traffic sig-\nnalisation detector subsystem, the TSD, is responsible for\ndetecting and recognising traffic signalisation. It receives\ndata from sensors and the state of the car, detects the posi-\ntion of the traffic signals and identifies their level or status.\nThat’s all about the subsystems of perception system, the\ndecision making systems is out of concern of this survey. It\nwill not be explained in this paper.\n3. MOT subsystem\nThe Moving Object Tracker (MOT) subsystem is respon-\nsible for detecting and tracking the posture of moving ob-\nstacles in the environment around the self-driving car. This\nsubsystem is critical in enabling self-driving cars to make\ndecisions on how to act to avoid colliding with potential\nmoving objects. Over time, the position of moving obsta-\ncles is usually estimated by range sensors such as LIDAR\nand radar or data captured by stereo and monocular cam-\neras. Images from monocular cameras help to provide a\nCVPR\n#11910501\n11910934CVPR\n#11910501\n11910934CVPR 2023 Submission #11910501 11910934. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\nwealth of information on appearance that can be used to\nimprove assumptions about moving obstacles.\n3.1. Traditional based MOT\nThe traditional MOT approach follows three main steps:\ndata segmentation, data association and filtering. In the data\nsegmentation step, the sensor data is segmented using clus-\ntering or pattern recognition techniques. In the data asso-\nciation step, segments of the data are linked to targets us-\ning data association techniques. In the filtering phase, for\neach target, the position is estimated by taking the geomet-\nric mean of the data assigned to that target. The position es-\ntimate is usually updated by a Kalman or particle filter. One\ntraditional approach is to use 3D LiDAR sensors to detect\nand track moving vehicles. The 3D LiDAR point cloud is\nsegmented into clusters of points using Euclidean distances.\nObstacles observed in the current scan of the sensor are cor-\nrelated with obstacles observed in previous scans using a\nnearest neighbour algorithm. The state of the obstacle is es-\ntimated using a particle filtering algorithm. Obstacles with\nvelocities above a given threshold are considered as moving\nvehicles. Data correlation is solved by an optimisation al-\ngorithm. A multi-hypothesis tracking algorithm is used to\nreduce association errors. Once filtered, object tracking was\nperformed based on a segmental matching technique using\nfeatures extracted from the images and 3D points.\n3.2. Model based MOT\nThe model-based approach uses a physical model of the\nsensor and a geometric model of the object with a non-\nparametric filter to infer directly from the sensor data. Data\nsegmentation and association steps are not required as the\ngeometric object model associates the data with the target.\nThe hypothesis of using differences in LiDAR data between\nsuccessive scans to detect moving vehicles. Instead of sep-\narating the data segmentation and association steps, new\nsensor data is incorporated by updating the state of each\nvehicle target, which includes the vehicle’s pose and geom-\netry [10].This is achieved by combining a hybrid formula-\ntion of Kalman filtering and Rao-Blackwellized particle fil-\ntering. The geometry becomes a tracked variable, meaning\nthat its previous state is also used to predict the current state.\nA trajectory is a sequence of object shapes generated by an\nobject over time which satisfies the constraints of the mea-\nsurement model and the motion model from frame to frame.\nDue to the high computational complexity of this solution,\nthe Data Driven Markov Chain Monte Carlo (DD-MCMC)\ntechnique is used, which efficiently traverses the solution\nspace to find the best solution.DD-MCMC aims to sample\nthe probability distribution of a set of trajectories, given the\nset of observations over a time interval. In each iteration,\nDD-MCMC draws a new state from the current state ac-\ncording to the proposed distribution. The new candidatestate is accepted with a certain probability. To provide DD-\nMCMC with an initial proposal, dynamic segments that fall\ninto the free or unexplored areas occupying the grid map are\ndetected from laser measurements and a moving obstacle\nhypothesis is generated by fitting a predefined object model\nto the dynamic segments. A Bayesian filter is responsible\nfor jointly estimating the pose of the sensor, the geometry\nof the static local background, and the dynamics and geom-\netry of the object. The geometric information consists of\nboundary points obtained with a 2D LiDAR. Basically, the\nsystem operates by iteratively updating the tracking state\nand associating the new measurements with the current tar-\nget. The hierarchical data association works on two levels.\nAt the first level, the new observations are matched to the\ncurrent dynamic or static target. At the second level, the\nboundary points of the obstacle are updated.\n3.3. Stereo vision based MOT\nThe stereo vision-based approach relies on the colour\nand depth information provided by stereo image pairs to\ndetect and track moving obstacles in the environment. The\nmethod is used for obstacle detection and recognition and\nuses only synchronised video from a front-view stereo cam-\nera. The focus of their work is on obstacle tracking based\non the output of each frame from pedestrian and car de-\ntectors. For obstacle detection, a support vector machine\n(SVM) classifier is typically used. For obstacle tracking, a\n”hypothesis-verification” strategy is typically used to match\na set of trajectories to possible detected obstacles. The set of\ncandidate trajectories is generated by Extended Kalman Fil-\nters (EKFs) and initialised with obstacle detection results.\nFinally, using model selection techniques, only the small-\nest, conflict-free set of trajectories that can explain past and\npresent observations is retained [11].\n3.4. Grid map based MOT\nThe grid map-based approach begins with constructing\na occupied grid map of the dynamic environment (Petro-\nvskaya). The map building step is followed by data seg-\nmentation, data association, and filtering steps to provide an\nobject-level representation of the scene. Nguyen proposed\na grid-based method for detecting and tracking moving tar-\ngets using stereo cameras. Their work focuses on pedes-\ntrian detection and tracking. Reconstruct three-dimensional\npoints from stereoscopic image pairs. On this basis, the in-\nverse sensor model is used to estimate the occupancy proba-\nbility of each cell in the grid map. The hierarchical segmen-\ntation method is used to cluster the grid cells according to\nthe regional distance between them. Finally, the Interactive\nMultiple Model (IMM) method is used to track moving ob-\nstacles [12]. Azim and Aycard use an eight-axis tree-based\n3D local occupancy grid map to divide the environment into\noccupied, free, and unknown voxels. Once a local grid map\nCVPR\n#11910501\n11910934CVPR\n#11910501\n11910934CVPR 2023 Submission #11910501 11910934. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\nis built, motion barriers can be detected based on inconsis-\ntencies between the observed free space and occupied space\nin the local grid map. Dynamic voxels are clustered into\nmoving objects, which in turn are further divided into lay-\ners. Using geometric features extracted from each layer,\nmoving objects are classified into known categories (pedes-\ntrians, bicycles, cars, or buses). Ge utilized 2.5D occu-\npancy grid maps to simulate static backgrounds and detect\nmoving obstacles. Mesh cells store the average height of\nthree-dimensional points where a two-dimensional projec-\ntion falls into the mesh domain. Detect motion assumptions\nfrom differences between the current mesh and background\nmodels.\n3.5. Sensor fusion based MOT\nSensor fusion-based approaches fuse data from various\nsensors, such as lidar, radar, and cameras, to explore their\nrespective characteristics and improve situational aware-\nness. Darms propose a sensor-fusion-based approach to\ndetecting and tracking moving vehicles employed by au-\ntonomous vehicle Boss (Urmson) (Carnegie Mellon Univer-\nsity Carnegie Mellon University’s car, which won first place\nin the 2007 DARPA Urban Challenge). The MOT subsys-\ntem is divided into two layers. The sensor layer extracts\nfeatures from the sensor data that can be used to describe\nthe movement barrier hypothesis based on a point model\nor box model. The sensor layer also attempts to link fea-\ntures with current prediction assumptions from the fusion\nlayer. Features that cannot be associated with existing as-\nsumptions are used to generate new proposals. Generates an\nobservation for each feature associated with a given hypoth-\nesis and encapsulates all the information needed to update\nthe hypothetical state estimate. Based on the recommenda-\ntions and observations made by the sensor layer, the fusion\nlayer selects the best tracking model for each hypothesis and\nuses Kalman filtering to estimate (or update the estimate)\nthe hypothesis state. Cho describe a new MOT subsystem\nused in a new experimental autonomous vehicle at Carnegie\nMellon University. The previous MOT subsystem proposed\nby Darms was extended to utilize camera data to identify\nclasses of moving objects (e.g., cars, pedestrians, and bi-\ncycles) and augment measurements from automotive-grade\nactive sensors (e.g., lidar and radar). The scan lines used\nby Mertz can be obtained directly from 2D lidar, projection\nof 3D lidar on a 2D plane, or from the fusion of multiple\nsensors (lidar, radar, and camera). Scan lines are converted\nto world coordinates and split. Extract line and corner fea-\ntures for each segment. Segments are updated with exist-\ning barriers and targets using the Kalman filter. Na merged\nthe trajectories of moving obstacles generated by multiple\nsensors such as radar, 2D lidar, and 3D lidar. The 2D li-\ndar data is projected onto a 2D plane and moving obstacles\nare tracked using a joint probabilistic data correlation filter(JPDAF). The 3D lidar data is projected onto the image and\nsegmented into moving obstacles using an area growth al-\ngorithm. Finally, iterative closest point (ICP) matching or\nimage-based data correlation is used to estimate or update\ntrajectory poses. Xu describe the use of situational aware-\nness to track moving obstacles to maintain distance using a\nnew experimental autonomous vehicle from Carnegie Mel-\nlon University. In the context of a given behavior, the road\nnetwork generates ROI. Candidate targets are found within\nthe ROI and projected into road coordinates. The distance\nkeeping goal is achieved by associating all candidate targets\nfrom different sensors (lidar, radar, and camera). Xue fused\nlidar and camera data to improve the accuracy of pedestrian\ndetection. They use a high degree of prior knowledge of\npedestrians to reduce false detection. They estimated the\nheight of pedestrians based on the pinhole camera equation\n(a combination of camera and lidar measurements).\n3.6. Deep learning based MOT\nThe deep learning-based approach uses deep neural net-\nworks to detect the location and geometry of moving ob-\nstacles and track their future state based on current cam-\nera data. Huval et al. propose a neural network-based\napproach to motion vehicle detection that uses the Over-\nfeat convolutional neural network (CNN) (Sermanet ) and\nmonocular input images with a focus on real-time. Overfeat\nCNN’s goal is to predict the position and distance (depth)\nof a car in the same driving direction just by looking at\nthe car’s rearview mirror. Mutz et al. solved the problem\nof motion obstacle tracking for a closely related applica-\ntion ”Follow the Leader” that is primarily suitable for fleets\nof autonomous vehicles. This tracking method is based\non generic target tracking (GOTURN) using regression net-\nworks (Held et al. ). GOTURN IS A PRE-TRAINED DEEP\nNEURAL NETWORK THAT TRACKS GENERAL OB-\nJECTS WITHOUT FURTHER TRAINING OR OBJECT-\nSPECIFIC FINE-TUNING. Initially, GOTURN receives an\nimage and a manually demarcated leading vehicle bounding\nbox as input. Suppose the object of interest is in the center\nof the bounding box. Subsequently, for each new image,\nGOTURN gives an estimate of the position and geometry\n(height and width) of the bounding box as output. The lo-\ncation of the lead vehicle is estimated with lidar points that\nfall within the bounding box and are considered vehicles.\n4. Collaborative Perception\nPerception is one of the key modules of autonomous\ndriving systems, but the limited capabilities of bicycles cre-\nate a bottleneck for improving perception performance. In\norder to break through the limitations of individual percep-\ntion, collaborative perception is proposed, enabling vehicles\nto share information and perceive the environment beyond\nthe line of sight and beyond the field of vision. This paper\nCVPR\n#11910501\n11910934CVPR\n#11910501\n11910934CVPR 2023 Submission #11910501 11910934. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\nreviews promising work on collaborative sensing technolo-\ngies, including basic concepts, collaborative models, and\nkey elements and applications. Finally, the challenges and\nproblems of openness in this research area are discussed\nand further directions are given. Two important problems\nwith single perception are long-range occlusion and sparse\ndata. The solution to these problems is that vehicles in\nthe same area share a common perception message (CPM)\nwith each other, and the co-perception environment is called\nco-perception or cooperative perception Thanks to the con-\nstruction of communication infrastructure and the develop-\nment of communication technologies such as V2X, vehi-\ncles can exchange information in a reliable way and thus\nenable collaboration. Recent work has shown that coopera-\ntive sensing between vehicles can improve the accuracy of\ncontext perception as well as the robustness and safety of\ntransportation systems.\nIn addition, autonomous vehicles are often equipped\nwith high-fidelity sensors for reliable perception, resulting\nin high costs. Collaborative sensing can alleviate the de-\nmanding requirements of individual vehicles for sensing de-\nvices.\n4.1. Synergistic Classification\nCollaborative sensing shares information with nearby\nvehicles and infrastructure, enabling autonomous vehicles\nto overcome certain perception limitations, such as occlu-\nsion and short field of view. However, achieving real-time\nand robust co-sensing requires addressing some of the chal-\nlenges posed by communication capacity and noise. Re-\ncently, there has been some work on the strategy of collab-\norative perception, including what collaboration is, when to\ncollaborate, how to collaborate, alignment of shared infor-\nmation, etc.\nSimilar to fusion, there are 4 categories of synergistic\nclassification:\n1. Early collaboration\nEarly collaboration takes place in the input space, shar-\ning raw sensory data between the vehicle and infras-\ntructure. It aggregates raw measurements from all ve-\nhicles and infrastructure to get a holistic view. There-\nfore, each vehicle can perform the following process-\ning and complete the perception based on the overall\nperspective, which can fundamentally solve the occlu-\nsion and long-distance problems that occur in single\nperception.\nHowever, sharing raw sensory data requires a lot of\ncommunication and tends to congest communication\nnetworks due to excessive data load, which in most\ncases hinders its practical application.\n2. Post-synergyLater collaboration is carried out in the output space,\nwhich promotes the fusion of the perceptual results of\neach intelligent output to achieve refinement.\nAlthough late synergy has bandwidth economy, it is\nvery sensitive to the positioning error of the intelligent\nbody, and suffers from high estimation error and noise\ndue to incomplete local observation.\n3. Intermediate synergy\nIntermediate collaboration takes place in intermediate\nfeature space. It is able to transfer intermediate fea-\ntures generated by individual Homo sapiens predic-\ntive models. After fusing these features, each sapient\nbody decodes the fused features and produces a per-\nceptual result. Conceptually, representative informa-\ntion can be compressed into these features, which can\nsave communication bandwidth compared to early col-\nlaboration, and improve perception compared to later\ncollaboration.\nIn practice, the design of this collaborative strategy is\nalgorithmically challenging in two aspects: i) how to\nselect the most efficient and compact features from the\noriginal measurements for transmission; and ii) how to\nmaximize the integration of the characteristics of other\nintellects to enhance the perception of each body.\n4. Hybrid synergy\nAs mentioned above, each synergy mode has its ad-\nvantages and disadvantages. As a result, some efforts\nemploy hybrid collaboration, combining two or more\ncollaboration models to optimize collaboration strate-\ngies.\n4.2. Factors\nThe main factors of co-sensing include:\n1. Synergy diagram\nGraphs are a powerful tool for collaborative percep-\ntion modeling because they have good interpret ability\nfor modeling non-Euclidean data structures. In some\nwork, the vehicles participating in collaborative sens-\ning form a complete collaboration graph, where each\nvehicle is a node, and the synergy relationship between\nthe two vehicles is the edge between these two nodes.\n2. Pose alignment\nSince collaborative sensing requires the fusion of data\nfrom vehicles and infrastructure at different locations\nand at different times, achieving precise data alignment\nis critical to successful collaboration.\n3. Information fusion Info fusion is a core component of\na multi-body system, and its goal is to fuse the most\n#11910501\n11910934CVPR\n#11910501\n11910934CVPR 2023 Submission #11910501 11910934. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\ninformative parts of other bodies in an efficient man-\nner.\n4. Resource allocation based on reinforcement learning\nLimited communication bandwidth in real-world envi-\nronments requires the best use of available communi-\ncation resources, which makes resource allocation and\nspectrum sharing important. In the vehicle communi-\ncation environment, rapidly changing channel condi-\ntions and increasing service requirements make the op-\ntimization of distribution problems very complex and\ndifficult to solve using traditional optimization meth-\nods. Some work utilizes multi-intelligence reinforce-\nment learning (MARL) to solve optimization prob-\nlems.\n4.3. Challenging Questions\n4.3.1 Communication robustness\nEffective co-ordination relies on reliable communication\nbetween intellects and bodies. However, communication\nis not perfect in practice: i) as the number of vehicles in\nthe network increases, the available communication band-\nwidth per vehicle is limited; ii) difficulty for vehicles to re-\nceive real-time information from other vehicles due to un-\navoidable communication delays; iii) communications may\nsometimes be interrupted, resulting in a disruption of com-\nmunications; iv) V2X communications are under attack and\ncannot always provide reliable services. Despite the con-\ntinuous development of communication technology and the\ncontinuous improvement of the quality of communication\nservices, the above problems will continue to exist for a\nlong time. However, most existing work assumes that infor-\nmation can be shared in a real-time and lossless manner, so\nconsidering these communication constraints and designing\nrobust collaborative sensing systems is important for further\nwork.\n4.3.2 Heterogeneity and trans modality\nMost co-system perception efforts focus on lidar point\ncloud-based perception. However, there are more types\nof data available for perception, such as images and\nmillimeter-wave radar point clouds. This is a potential way\nto leverage multi modal sensor data for more effective col-\nlaboration. In addition, in some scenarios, there are dif-\nferent levels of autonomous vehicles that provide different\nquality information. Therefore, how to perform collabo-\nration in heterogeneous vehicle networks is a problem for\nfurther practical applications of collaborative sensing. Un-\nfortunately, little work has focused on heterogeneous and\ncross-modal collaborative awareness, which has also be-\ncome an open challenge.4.3.3 Large-scale datasets\nLarge-scale datasets and the development of deep learning\nmethods have improved perceptual performance. However,\nexisting datasets in the field of co-sensing research are ei-\nther small in size or not publicly available.\nThe lack of public large-scale datasets hinders the fur-\nther development of collaborative sensing. In addition, most\ndatasets are based on simulations. While simulation is an\neconomical and safe way to validate algorithms, real-world\ndatasets are needed to put collaborative sensing into prac-\ntice.\n"}, {"role": "assistant", "content": "最终打分：9 (范围0-10分)\n1. 结构完整性得分：9, 占比20%\n2. 逻辑清晰度得分：9, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：8.5, 占比20%\n5. 参考文献规范性得分：10, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "1辅助临床诊断的超声心动图分析系统框架\n小组成员：张颖栋徐岩张子研殷超越陈康睿\n指导教师：刘江叶海礼\n1.问题概述\n心脏，是人体最重要的器官之一，维持着人体的生命活动。心脏疾病具有发病率高，危\n害性强，具有渐进性和突发性的特点，对人的生命安全有极大的危害。因此，医生建议\n定期检查心脏。超声检查，是一种无创、价格低廉、诊断快速的诊断方式，非常适合心\n脏疾病的初筛。但是，超声检查需要超声医生的专业知识支持，并且高度依赖主观观察，\n一旦超声医生诊断有误，会极大地影响主治医师的诊断。因此，受限于超声医生的缺乏，\n超声检查难以大规模的发展。\n为了降低心脏疾病的危害，促进心脏超声检查的发展，减轻超声医生的检查压力，我们\n希望构建一套完整的超声心动图分析系统，以超声原始图像作为输入，自动完成超声图\n像的处理、临床指标的测量提取、并以此为基础完成超声心动图的综合分析，生成完整\n的超声心动图辅助诊断报告。同时为了方便患者对自身病情的了解，我们基于辅助诊断\n报告，利用知识图谱生成了超声心动图辅助解读报告。\n先前刘江老师组内的创新实践小组已经完成了针对射血分数相关的疾病诊断与分析。我\n们小组的目标是进一步扩大心动图系统的疾病诊断范围，检测左心室壁厚的增加对患者\n产生的影响。通过深度学习来量化心室肥厚以及增加预测心室壁肥厚增加原因方面的准\n确性，减少误诊。识别由于肥厚性心肌病(hypertrophiccardiomyopathy)和心脏淀粉样变2(cardiacamyloidosis)造成的壁厚。\n2.工作概述\n由于我们组的成员先前对于超声心动图方向均没有了解，所以在前期，我们进行了大量\n的文献调研，以及与叶海礼老师和匡浩玮同学进行了多次交流。最终我们决定针对左心\n室壁厚方向进行进一步的研究。并在最后将成果整合到iMED眼脑心联动系统平台中。\n我们以心脏心尖四室切面视频数据为切入点，完成了室间隔厚度（IVS），左心室内径\n（LVID)，左心室后壁厚度（LVPW）的诊断参数准备；之后，依托这些参数，利用知\n识图谱完成了左心室肥大的临床辅助诊断任务，然后结合为了辅助医生主观判断的超声\n图像超分辨率增强结果，生成了辅助诊断报告。最后，为了辅助患者初步了解自己的病\n情，我们又依托知识图谱构建了辅助解读报告。\n3.主要工作\n3.1.知识图谱构建\n图1是我们调研后完成的知识图谱结构，从IVS,LVPW,LVID三个参数出发，显示可以\n进行诊断的疾病有哪些。3\n图1:知识图谱构建\n3.2.数据集介绍\n我们在项目中使用的数据集是StanfordHealthCare的胸骨旁长轴视频。\n该训练集的样本数为9600，验证集的样本数为1200，测试集的样本数为1200。另外还\n有7767份SHC超声心动图研究样本来自具有明确疾病特征的患者，包括心脏淀粉样变、\nHCM和严重主动脉瓣狭窄。\n3.3.算法介绍\n为了解决目前在评估心室肥大和疾病诊断方面的局限性，该论文开发了一种端到端的深\n度学习方法，用于标记心室尺寸、量化心室壁厚度，并预测心室肥大的原因。4该论文使用一个改良的DeepLabv3架构在胸骨旁长轴图像上使用最小化加权均方误差\n损失进行训练，用于识别测量心室尺寸的关键点。并且使用了学习率为0.001的Adam\n优化器，模型被训练了50个epoch，根据验证损失提前停止。该论文评估了不同的视频\n长度、分辨率和时间分辨率，作为优化模型性能的超参数。\n对于基于视频的疾病分类，该论文使用了一个18层的ResNet3D架构来对视频进行分类。\n鉴于有可能出现同时诊断为主动脉瓣狭窄和淀粉样变心肌病的患者，并有多种原因导致\n左室肥厚，该架构训练了平行的二元分类深度学习模型来预测淀粉样变心肌病、肥厚型\n心肌病、主动脉瓣狭窄的概率，并独立地预测。\n图2：算法工作\n3.4.诊断报告\n我们将之前建立的一个个小的知识图谱整合，构建了完整的辅助诊断报告生成知识图谱，\n通过输入患者的个人信息和视频信息，生成了我们的超声辅助诊断报告。（图3）\n可以发现，我们主要便是根据LVPW,LVID,IVS三个参数进行的诊断，下方的数值为正\n常范围，这三个参数也就是我们诊断疾病的标准。5\n图3：超声辅助诊断报告\n在我们构建知识图谱的工作中发现，网上的信息鱼龙混杂。许多患者在拿到初步的辅助\n诊断报告后，可能不能及时的找医生解读而是自行在网上搜索。因此，我们又利用左手\n医生医学知识图谱和思知中文知识图谱的知识，对我们辅助诊断报告的结论进行解释，\n供患者初步了解自己的病情，并告知患者不要惊慌，及时就医。\n图4：超声辅助解读报告63.5.ELIA交互系统\n后续我们将上述工作整合到了iMED已有的ELIA系统中，图5、图6为该系统运行相\n关功能时的截图展示。\n图5：ELIA系统展示1\n图6：ELIA系统展示2\n"}, {"role": "assistant", "content": "最终打分：8.4 (范围0-10分)\n1. 结构完整性得分：8.5, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：8.5, 占比20%\n5. 参考文献规范性得分：8, 占比10%\n6. 课程知识掌握度得分：10, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "图1 ：大象机器人 - 人工智能套装大 象 机 器 人 —— 分 拣 码 垛 积 木\n一 、 项 目 背 景 & 目 的\n二 、 实 验 环 境 搭 建\n1. Ubuntu + ROS\n2. Anaconda + Python + 大 象 机 器 人 开 源 代 码 库\n三 、 目 标 检 测 算 法\n1. 综 述\n2. 传 统 目 标 检 测 算 法\n3. 深 度 学 习 算 法\n4. RGB 颜 色 识 别 算 法\n四 、 机 械 臂 控 制\n1. 位 置 计 算 & 码 垛 抓 取\n2. 码 垛 收 集\n3. 机 械 臂 复 原\n五 、 总 结 与 提 升\n六 、 小 组 成 员\n大象机器人 —— 分拣码垛积木  \n一、项目背景 & 目的  \n随着计算机、电子、机械等学科的不断发展，机器人越来越多地被应用于各个领域，如医疗、智能制\n造、应急救援等领域。其中，六轴机器人由于具有可编程、拟人化、通用性、机电一体化、高自由度等\n特性，在各个领域都广受欢迎。本项目旨在使用大象机器人的 myCobot 协作机械臂，进行积木的识别和\n抓取功能，并借此了解 UR 构形的六轴协作机器人相关知识及操作，包括坐标系概念，机器人运动学，运\n动轨迹规划，计算机视觉等。\n该项目的硬件环境使用的是 大象机器人 的 人工智能套装 ，其中机械臂型号为 myCobot 280-M5 ，自重仅\n850g ，有效载荷 250g ，有效工作半径 280mm ，可搭配多种末端执行器适配多种应用场景，也可支持多\n平台软件的二次开发。 myCobot 是 大象机器人 和M5stack 联合出品的一款， 世界最小最轻的六轴协作\n机器人 ，是一个拓展生产力和想象力边界的有力工具。对体积小巧但功能强大的六轴机器人的探索，可以为 科研教育、智能家居，商业探索 等各种场景积累宝\n贵的经验，并给予创造性的启发。\n本项目期望实现 1. 通过摄像头对场景中的目标码垛识别； 2. 控制六轴机械臂移动至相应位置； 3. 使用空气\n吸泵对目标码垛进行抓取； 4. 控制机械臂移动码垛至目标收集点； 5. 释放码垛， 6. 回归初始位置并可以进\n行下一轮目标抓取操作；这六个主要操作。\n二、实验环境搭建  \n实验环境搭建方面我们进行了诸多尝试，初期我们尝试使用 Ubuntu ，但由于该版本的 myCobot 机械臂\n不支持 Ubuntu 20.04 的版本，后续我们又改用 Anaconda+Python 的环境进行开发。\n1. Ubuntu + ROS  \nUbuntu 是一个以桌面应用为主的 Linux 操作系统。由于 Linux 在包管理方面相较于 windows 有很大优\n势，我们在项目初期计划安装 Ubuntu 并搭建 ROS 开发环境来进行项目的开发。我们使用 Vmware 进行了\nUbuntu20.04 的安装，并安装相应版本的机器人操作系统  ROS Noetic 、 MoveIt 以及  git 版本管理器。\n但由于 myCobot280-M5 型号的机械臂不支持 Ubuntu20.04 的使用，效率起见，我们使用更加友好的\nAnaconda + Python 作为我们的实验环境。\n2. Anaconda + Python + 大象机器人开源代码库  \nAnaconda 是一个可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。\nAnaconda 包含了 conda 、 Python 在内的超过 180 个科学包及其依赖项。我们的实验环境基于 python3.7\n搭建，主要使用 openCV 开源库 和 大象机器人开源代码库 进行开发。\n三、目标检测算法  \n1. 综述  \n目标检测任务是找出图像或视频中人们感兴趣的物体，并同时检测出它们的位置和大小。不同于图像分\n类任务，目标检测不仅要解决分类问题，还要解决定位问题，属于 Multi-Task 问题。目标检测的发展脉\n络可以划分为两个周期：传统目标检测算法时期 (1998 年 -2014 年 ) 和基于深度学习的目标检测算法时期\n(2014 年 - 至今 ) 。而基于深度学习的目标检测算法又发展成了两条技术路线： Anchor based 方法 ( 一阶\n段，二阶段 ) 和Anchor free 方法。\n2. 传统目标检测算法  \n传统目标检测算法主要基于手工提取特征，思路大多不复杂，但也有一些较为明显的缺点，如识别效果\n不够好，准确率不高；计算量较大，运算速度慢；可能产生多个正确的识别结果等。一般传统检测算法\n流程可概括如下：\n选取感兴趣区域，选取可能包含物体的区域\n对可能包含物体的区域进行特征提取\n对提取的特征进行检测分类\n其代表算法有 Viola Jones Detector ，HOG Detector ，DPM Detector 等。\n3. 深度学习算法  \n2012 年卷积神经网络 (Convolutional Neural Networks, CNNs) 的兴起将目标检测领域推向了新的台阶。\n基于 CNNs 的目标检测算法主要有两条技术发展路线： anchor-based 和 anchor-free 方法，而 anchor-\nbased 方法则包括一阶段和二阶段检测算法 ( 二阶段目标检测算法一般比一阶段精度要高，但一阶段检测\n算法速度会更快 ) 。 \n图2 ：识别蓝色和红色码垛\n图3 ：码垛识别、分类、收集总流程Anchor-Based 中的代表算法有 RCNN ，SPPNet ，Fast RCNN ，Faster RCNN ，Cascade RCNN ，\nYOLO v1 ，SSD ，RetinaNet ，YOLO V5 等\nAnchor-Free 的代表算法有 CornerNet ，CenterNet ，FSAF ，FCOS ，SAPD 等\n4. RGB 颜色识别算法  \n在我们的机器人识别码垛任务中，由于任务场地固定，背景单一，没有必要使用过于复杂的深度学习模\n型。基于这些考虑，我们计划 Python+OpenCV 进行基于颜色的目标识别。\n首先通过机器人自带的摄像头获取目标的 RGB 图片，利用 opencv 对目标颜色进行识别和提取，并计算目\n标颜色的区块大小，过滤掉轮廓围成面积较小的物体，提取图像的轮廓，并计算中心点，最后将中心点\n至机械臂操作端进行抓取操作。\n四、机械臂控制  \n图 4 ：智能机械臂控制下的码垛抓取与收集1. 位置计算 & 码垛抓取  \n通过俯视摄像头以及 RGB 颜色识别算法，我们向机械臂控制模块返回了目标码垛所在的平面坐标系中的\n中心点位置。由于我们所用的六轴机器人固定基地啊与检测范围相对固定，我们可以据此计算出使得六\n轴机器人的吸泵端接触目标码垛所需的六个自由度上的具体数值 —— 旋转 (S 轴 ) ，下臂 (L 轴 ) 、上臂 (U 轴 ) 、\n手腕旋转 (R 轴 ) 、手腕摆动 (B 轴 ) 和手腕回转 (T 轴 ) 以及相应的移动速度，移动机械臂至指定位置并启动吸\n泵，完成目标码垛的抓取操作。\n2. 码垛收集  \n在 RGB 颜色识别算法部分，我们在传给机械臂控制模块时，同时传递了码垛的位置和颜色信息。机械臂\n会根据获得的颜色信息，以及坐标系转换计算，将拾取到的码垛移动至对应颜色的收集小框上方，释放\n吸泵，使得目标码垛被准确收集。\n3. 机械臂复原  \n在完成码垛释放后，我们会控制机械臂回到初始位置，同时再次使用俯视摄像头拍摄新状态下的码垛状\n态，重复步骤 1-3 ，最终完成所有目标颜色码垛的识别，拾取，与分类操作。\n \n五、总结与提升  \nmyCobot 机器人操作简单且能与人协同安全工作，我们通过不断地尝试与探索完成了 myCobot 机械臂分\n拣目标码垛实验，并了解了基本的 UR 构形的六轴协作机器人相关知识及操作，包括机器人坐标系概念及\n转换，机器人运动学，运动轨迹规划，计算机视觉等知识。\n在未来提升方面，在实验过程中，我们发现由于硬件设备的空气吸泵吸力较大，不能很好地完成积木块\n的释放操作，后续可以对吸泵控制进行进一步的优化。\n六、小组成员  \n吴一凡，金冬阳，朱世博，蓝晨溪"}, {"role": "assistant", "content": "最终打分：7.7 (范围0-10分)\n1. 结构完整性得分：8, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：8.5, 占比20%\n5. 参考文献规范性得分：3, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "The Feasibility of AI Models in Assisting\nArchaeological Research\nMinglang Huang\nComputer Science and Technology\nSouthern University of Science and Technology\nShenzhen, China\n12011828@mail.sustech.edu.cnTong Zhang\nComputer Science and Technology\nSouthern University of Science and Technology\nShenzhen, China\n11911831@mail.sustech.edu.cn\nAbstract —With the rapid development of Machine Learning,\nthe research field about using AI models to assisting archaeology\nresearch become more and more vibrant. In ancient jade iden-\ntification, more and more AI models are being used to promote\nthe development of relevant research work.\nIndex Terms —Artificial Intelligence, Machine Learning, Jade,\nArchaeology\nI. I NTRODUCTION\nIn recent years, artificial intelligence has been developed\nand popularized in many fields. It has also been rapidly pop-\nularized and applied in archaeology, promoting the efficiency\nof archaeological work in some relevant fields, such as using\npattern recognition cultural relics restoration, using computer\nvision technology in archaeological exploration, inferencing\nancient characters with deep learning and so on. In this journal,\nwe will focus on trying to use several AI models to identify\ndifferent kinds of jade according to their unique feature like\nspectral, mineral, crystal characteristics.\nII. A PPLICABILITY OF JADE IDENTIFICATION WITH AI\nA. Background Survey\nSince the 1980s, with some Breakthroughs in Chinese\narchaeology research, a great amount of jade which was exca-\nvated need to be accurately classified and detected with some\njade’s details. With the requirement of relevant knowledge and\nexperience, this kind of jobs can be very overwhelming and\nchallenging for archaeology researchers.\nAlso in these day, many provinces in China have witnessed\nan upsurge in the manufacture of fake antiques, and the\nmanufacture of fake ancient jade is one of these phenomenon.\nBy using the current modern production technology, the im-\nitation of ancient jade can be extremely high, even people\ncan not tell them accurately. Antique jade made in different\nregions can be imitated in various ways. The manufacture of\nmodern antique jade is closely related to the development of\narchaeological discoveries and ancient jade identification, and\nmany archaeological research achievements have been used\nfor reference.\nOver the last decade, artificial intelligence (AI) has become\nwidespread across science and technology. With the develop-\nment of Machine Learning, nowadays it can be convenient\nand efficiently to apply several AI models not only to helparchaeology researchers by working jade identification, but\nalso to help people identify different kinds of jade without\nbeing cheated.\nB. Application Cases\nWhen it comes to the topic of cultural relics identification,\nwe can found there exists many AI models applications\nin these relevant fields, such as pottery identification, rock\ngeological identification and jade identification. To extend\nour ideas on this topic, we do some research on relevant\ncultural relics identification’s topic. And We will discuss some\ncommon cultural relics identification methods by using some\ncases we research as below.\n•Automatic Recognition of Pottery with DNN\nThe ArchAIDE project raised by italian archaeology\nresearchers mainly use two different deep neural networks\nto dedicate image recognition and shape recognition. This\nproject basically use pipeline framework to implement the\napplication, using two kinds of classifiers: Appaerence-\nbased recognition classifier and Shaped-based recognition\nclassifier to handle the input image information. Once\nthese images can be identified into correct type, the result\ncan be linked to the photographed sherd and stored in\ndatabases which can be accessed online. For studying this\ncase, we find a new idea to identify jade: classification\nwith DNN.\nFig. 1. ArchAIDE’s Double workflow for appearance-based and shape-based\nrecognition from an input image to results.\n•Intelligent Mineral Identification with Spectral Analysis\nConsidering ancient jade is a special kind of minerals,\nhence we focus on researching some study about mineralanalysis. In the field of Mineral Identification, Ameri-\ncan scientist Lopez-Reyes established three mathemati-\ncal models of multivariate analysis based on principal\ncomponent analysis (PCA), partial least squares (PLS)\nand artificial neural networks (ANNs), and propose a\nsimilarity learning method based on Siamese network,\nwhich classification accuracy reaches 0.94 in Raman\nMinerals Test. Its main idea is to analyze the spectral\ndata of minerals under different light and the size of\nmineral suspended particles, which help us realize how to\naccurately identify the types of minerals including jade.\nC. Limitation of Jade Identification with AI models\n•Huge Gap between Ancient Jade Recognition and AI\nThe meaning of related minerals in ancient jade recogni-\ntion can be qualitatively and clearly described, while ma-\nchine learning is based on data, and the recognition pro-\ncess and standards are often difficult to explain directly.\nHow to overcome the differences between these two\naspects has become the biggest obstacle for researchers\nof different majors to conduct cross research.\n•Lack of Jade Identification Data Sets\nThere exists a shortage of ancient jade related data\nsets. Benchmark data sets play an important role in the\nresearch and innovation of ancient jade identification.\nBenchmark datasets based on high-quality and big data\ncan build better discriminant models and support strict\nperformance comparisons.\n•Early Stage In Archaeology Application with AI\nMachine learning method is still in the early stage of\nexploration in ancient jade recognition, especially in\nmineral intelligent recognition, and lacks mature methods\nthat are widely recognized, highly accurate and fast. The\nexisting methods are usually used for scattered datasets\nof different objects and methods. The author sets his own\nstandards to evaluate the performance in the absence of\nstandard answers, which makes it difficult to accurately\ncompare the detection performance, advantages and dis-\nadvantages of each method, thus hindering the application\nand change of the latest detection methods;\n•High Development Cost and Uncertain Quality\nOn the one hand, AI Archaeology Application’s de-\nvelopment cost can be high, as well as the AI model\ndevelopment’s quality can be uncertain, which means the\nhigh development cost of the ancient jade identification\nproject with the low efficiency, leading to difficulty to\ncarry out large-scale application. On the other hand, due\nto the specificity of cultural relics, it is difficult for\nsuch solutions to generate a universal solution, which\nmay require developers to frequently adjust the relevant\nparameters of the model to reach the standard of the\nactual situation.\nIII. M ACHINE LEARNING IN JADE IDENTIFICATION\nTo test what we have learnt in a more directed approach, we\nimplement several AI models in Jade identification to explainthe theoretical ideas in practical example.\nWe choose jade as the research object for the following\nreasons. Firstly, jade has been a long-lasting symbol for\nancient China. There is a famous jade story dates back to\nthe Warring States Period, when the Qin king offered 15\ntowns to the state of Zhao in exchange of the rare treasure\n’He Shi Bi’. We could easily tell how important it is for\nancient people as a connection bond and socializing symbol.\nThe inheritance of jade has been passed down to this day.\nJade often represents as a family heirloom to witness the\nrise and fall of a family, the family members’ spirit and\nemotion, cohesion of the family strength. The inheritance\nof jade often carries the ancestors’ expectations for future\ngenerations, hoping that future generations can become a\nperson with faith, pursuit and gentle as jade stands for. While\nthe specific way of using jade and its social significance vary\ngreatly in different cultures, social circumstance and historical\nstages. The awareness, recognition, use and respectfulness of\nancestors for jade are directly related to the physical properties\nof jade and the social value as well as cultural connotation\nendowed by the physical properties. Nephrite’s unique high\ntoughness may be used to embody and construct the character\nand social relationship with life vigor, lasting, eternal and\nunbreakable. The high toughness determines that jade can be\nused for a long life, so a piece of jade can be passed down\nfrom generation to generation, while its physical properties\nremain basically unchanged. This is also an important reason\nfor nephrite to be studied in archaeology. The toughness of\nnephrite is determined by the properties of the fiber bundles\nthemselves and the intersection between fiber bundles. The\nsmaller single crystals that compose fiber bundles, the greater\ndegree of dislocation between the crystals, and hence greater\nstrength of fiber bundles. To put it in a nut shell, the smaller the\nfiber bundle, the higher the degree of interweaving, the higher\nthe toughness of nephrite which leads to a higher density\nand better stability. It is the particularity of fiber structure of\nnephrite that determines the special high toughness and density\nof nephrite, and then the special warm luster of nephrite, which\ntogether constitute the special texture of nephrite.\nThe structural composition of nephrite is tremolite-\nactinolite, that is, mineral type with chemical formula\nCa2(Mg,Fe)5Si8O22(OH)2. In order to analyze the structural\ncomponents of jade on the basis of protecting the cultural relics\nthemselves, we adopt the non-destructive mineralogy analysis\nmethod of jade, that is, to scan the jade in an all-round way\nthrough the spectral scanner, and obtain a set of spectral data\nof the jade.\nDifferent bands contain different chemical element infor-\nmation. We can identify whether an artifact is nephrite or not,\nand whether it has been incinerated or not by judging the\ncharacteristics of different bands.\nThe data used for processing is a group of spectral data files\nin.mat format. There are two dimensions in the file: the jade\nnumber corresponding to each spectral data, and the spectral\ndata obtained at the frequency of 1300nm-2500nm at the\nsampling rate of 2nm, which is represented by the reflectancesize at the corresponding frequency.\nIn addition, we manually marked the jade data through the\njade characteristic labels prepared by the archaeologists pro-\nvided by the Humanities and Social Sciences Center. Among\nthem, the mineral composition is analyzed by near infrared\nspectroscopy: Np-i-nephrite Type I, NP-II-nephrite Type II, Np\nIII nephrite Type III, AT-leaf serpentine, CT-chlorite, IT-illite,\nDC-Dicarkite, QZ-chalcedony, cryptocrystalline quartz, UN-\nunknown mineral or rock. The other characteristics, degree\nof sensitivity, and texture classification of objects include: A\n- objects whose mineral composition is Np I; The mineral\ncomposition of B, B1, B2 - is similar to that of A, is Np I,\nbut contains trace vermiculite, B - has no or weak exposure\nor secondary changes (not including type A type I jade pipe),\nB1 - moderate or strong exposure or secondary changes (not\nincluding type A type I jade pipe), B2-A type I jade tube,\nno or weak qinling or secondary changes; C-A type I jade\npipe, mineral composition is Np II; D - objects with mineral\ncomposition of Np III; S - Minerals or rocks not jade.\nWe used Extra Tree Classifier, Support Vector Machine\n(SVM), Gaussian Naive Bayes and K Neighbors to distinguish\nthe spectral data features of jade and non-jade.\nA. Extra Tree Classifier\nExtra Tree Classifier is a tree-based ensemble machine\nlearning algorithm that is used for classification tasks. It is\nan extension of the decision tree algorithm and is often used\nas a substitute for random forests, which are another type\nof tree-based ensemble algorithm. It works by building a\nset of decision trees on a subset of the training data and\nthen combining the predictions of these trees to make a final\nprediction. Each tree in the ensemble is trained on a different\nsubset of the training data, and the predictions of all the trees\nare combined using a majority vote or an averaging method.\nAnd it is known for its fast training speed and high accuracy,\nespecially for datasets with a large number of features. It is\nalso resistant to overfitting, which means that it is able to\ngeneralize well to unseen data. However, it is not as robust to\nnoisy or imbalanced data as some other algorithms.\nB. Support Vector Machines (SVMs)\nSupport Vector Machines (SVMs) are a type of supervised\nmachine learning algorithm that is used for classification and\nregression tasks. They work by finding a hyperplane in a high-\ndimensional space that maximally separates different classes.\nThey are known for high accuracy and ability to handle high-\ndimensional data. They are particularly useful for datasets with\na large number of features or when the relationships between\nfeatures are complex. SVMs are also resistant to overfitting,\nwhich means that they are able to generalize well to unseen\ndata. However, SVMs can be sensitive to the choice of kernel\nfunction and require careful tuning of hyperparameters to\nachieve good performance. They can also be computationally\nexpensive to train, especially for large datasets.C. K-Nearest Neighbors (KNN)\nK-Nearest Neighbors (KNN) is a type of instance-based\nlearning algorithm that is used for classification and regression\ntasks. It works by storing all the training data and then\npredicting the label or value for a new sample by finding\nthe K nearest neighbors in the training data and using their\nlabels or values to make a prediction. It is a simple and\nintuitive algorithm that is easy to understand and implement.\nIt requires no training and can be used for both classification\nand regression tasks. However, KNN can be computationally\nexpensive to use, especially for large datasets, and it is\nsensitive to the choice of K and the distance metric used to\nmeasure similarity between samples.\nD. Naive Bayes\nNaive Bayes is a type of probabilistic machine learning\nalgorithm that is used for classification tasks. It is based on\nthe Bayes theorem, which states that the probability of an\nevent occurring is equal to the prior probability of the event\nmultiplied by the likelihood of the event given some evidence.\nIn the context of classification, Naive Bayes works by using\nthe probabilities of different classes and the probabilities of\ndifferent features given each class to predict the class for a new\nsample. It is called ”naive” because it makes the assumption\nthat all the features in the data are independent, which is often\nnot the case in real-world data. Despite this assumption, Naive\nBayes is often surprisingly effective and is widely used in a\nvariety of applications. It is simple to implement and requires\nrelatively little training data. However, it can be sensitive to\nthe presence of correlated features and may not perform as\nwell as other algorithms for more complex datasets.\nE. Experiments\nThe following is the confusion matrix of the results of\nmineral composition classification:\nFig. 2. Extra Tree ClassifierFig. 3. SVM\nFig. 4. K Neighbors\nFig. 5. Gaussian Naive Bayes\nAccording to the confusion matrix and score of the su-\npervised learning classification results of the above minerals,\nSVM can give better classification results in this case, while\nthe Extra Tree Classifier is unstable, with the dominant re-\nsult exceeding 0.88 and sometimes lower than 0.85, and K\nNeighbors slightly lower than the first two. However, Gaussian\nNB has poor classification effect, this is due to the limits\nwhen using Gaussian NB, it requires the datasets’ features\nare independent as we explained above. However, the datawe obtained from Jade has 600 features, even though we use\nfeature extraction to reduce it to 50 features, they are still not\nindependent. We can tell from the figure below, for a feature -\nOH, it is represented by 1380nm-1420nm. Hence, even though\nwe reduced features in jade datasets, the features are still not\nindependent which leads to bad performance of Gaussian NB.\nFig. 6. Frequency and Chemical Bonds\nACKNOWLEDGMENT\nThanks to the introduction of Artificial Intelligence taught\nat CS103, we have the chance to be involved in such interest-\ning multi-disciplinary research fields. Special thanks to prof.\nJimmy Liu, prof. Zhichun Jing and all the class tutors who\ninstruct us together in completing the survey as well as answer\nour technical and domain knowledge problems.\n"}, {"role": "assistant", "content": "最终打分：8.6 (范围0-10分)\n1. 结构完整性得分：8, 占比20%\n2. 逻辑清晰度得分：8.5, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：9, 占比20%\n5. 参考文献规范性得分：8, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "AReview onAutonomous Driving:\n Multi-sensor Object Detection and Semantic Segmentation\n Runzhe Jiang\n SUSTECH\n  11912528@mail.sustech.edu.cn\n Zhuohang Zhuang\n SUSTECH\n 11912511@mail.sustech.edu.cn\n  Abstract\n Autonomous driving is a smart car technology that uses\n computer systems to drive without a driver. In a new era of\n booming new technologies led by artificial intelligence\n and big data, the trend of intelligent development in the\n automotive sector is no exception. In order to obtain more\n robust and accurate perceptual results, autonomous\n driving usually needs to be equipped with different sensors\n that complement each other in different operating\n conditions. Typical sensor modes include: camera, radar,\n Lidar, high precision map, etc. Multi-modal sensor fusion,\n which means information complementarity, stability and\n safety, has long been an important part of autonomous\n driving perception. However, the insufficient use of\n information, the noise of the original data and the\n misalignment between the various sensors (such as time\n stamps out of sync), and other factors have resulted in the\n limited fusion performance. This survey comprehensively\n investigates the advantages and disadvantages of existing\n sensors, multi-modal autonomous driving perception\n algorithms, and focuses on object detection and semantic\n segmentation. In this survey we summarize more than 20\n literatures and analyze the existing problems in the current\n field and provide reference for future research directions.\n 1. Introduction\n Google began testing its Google autonomous driving car\n in 2010. The goal is to drive vehicles without any human\n intervention. Figure 1 shows its core architecture. So far\n 600,000 km has been tested and a legal test plate has been\n obtained[2]. Other companies such as Tesla, Volvo, BMW\n are also making deep research into autonomous driving\n technology, with the recent positioning of advanced\n assisted driving on highways. Meanwhile, China's Baidu,\n Didi, Huawei and other companies have also conducted a\n lot of research on autonomous driving.\n Automobile industry is a special industry for it involves\n the safety of passengers. Any accident in this area is\n unacceptable, so there are almost harsh requirements for\n safety and reliability. Therefore, the accuracy and\n robustness of sensors and algorithms are highly required in\n the study of autonomous driving. On the other hand,\n Figure 1\n autonomous driving vehicles are products for ordinary\n consumers, so costs need to be controlled. In the past, it has\n been difficult to resolve the contradiction between\n high-precision sensors that make algorithmic results more\n accurate and expensive (such as Lidar)[3]. Today, the high\n accuracy brought by deep learning technology promotes\n the development of autonomous vehicle systems in\n multiple core areas such as object detection, semantic\n segmentation , decision making and sensor application.\n Deep learning technologies, typical of which are\n convolutional neural network (CNN)[7], are widely used\n in various kinds of image processing, and are highly\n applicable to the field of autonomous driving.\n The realization of deep learning puts forward higher\n requirements for sensor technology, which requires\n multi-sensor fusion technology. In the design of software\n and hardware architecture of autonomous vehicles,\n sensors, as the source of data information, are of great\n importance. However, combined with the application and\n implementation of deep learning, no matter what kind of\n sensor has its own advantages and disadvantages. For\n example, the range information obtained by Lidar is very\n accurate, but there are some problems such as lack of\n texture, little feature information and much noise, which is\n not conducive to the application of deep learning. And the\n features of the camera are exactly the opposite of the radar.\n The fusion of sensors like Lidar and camera is of great\n significance for the accurate perception and cognition of\n autonomous driving[2].\nFigure 2 (A complex urban scenario for autonomous\n driving. The driverless car uses multi-modal signals\n for perception, such as RGB camera images, LiDAR\n points, Radar points, and map information. It needs to\n perceive all relevant traffic participants and objects\n accurately, robustly,and in real-time. For clarity, only\n the bounding boxes and classification scores for some\n objects are drawn in the image. The RGB image is\n adapted from [4].)\n 2. Background\n 2.1. Visual and Thermal Cameras\n Images taken by visual and thermal cameras provide\n detailed textural information about the vehicle's\n surroundings. The followers of visual solutions believe\n that humans can become qualified drivers through visual\n information and brain processing. Camera + deep learning\n neural network + computer hardware can also achieve\n similar effects. Visual cameras are very sensitive to light\n and weather conditions. Thermal imagers, on the other\n hand, are sensitive to day-night changes because they\n detect infrared radiation, which is related to an object's\n heat[10]. By means of infrared optical system that can pass\n infrared radiation, the infrared radiation of the scene in the\n field of view is focused on the device that can convert\n infrared radiation energy into physical quantities that can\n be easily measured-- infrared detector[10]. The infrared\n detector then converts the radiation signals of varying\n strength into corresponding electrical signals. After\n amplification and video processing, video images can be\n formed for human eyes to observe. However, neither type\n of camera provides depth information directly..\n 2.2. LiDARs\n LiDARs: which is also named Laser Radar. It is a radar\n system which transmits laser beam to detect the\n characteristic quantity of target such as speed and position.\n It works byfiring a laser beam , which is a detection signal,\n at a target and then comparing the transmitted signal to the\n signal that is reflected back from the target. When the\n beam hits an object, it bounces back and is sensed and\n recorded by a sensor. Then, the system will calculate the\n distance of the beam to the object and back. After a series\n of processing, we can obtain the relevant information of\n the target, such as its orientation, speed, attitude, distance\n and shape parameters. And we can then create a 3D map of\n the area around the vehicle to enable safe navigation of the\n vehicle. Laser radar accuracy can reach mm level, such as a\n pedestrian reflection point can reach tens of thousands of\n points\n or even more, thus giving clear 3D\n three-dimensional graphics. Moreover, the laser radar has\n a long detection range and its angular resolution is several\n grades higher than millimeter wave radar, easily reaching\n 0.1°[11]. That is to say, if the power is enough, it can\n distinguish two small targets with a distance of 3KM for a\n 5M gap[6]. Flash LiDAR was developed to produce\n detailed object information similar to camera images. Of\n course, the FMCW Lidar can provide speed information.\n Figure 3\n 2.3. Radars\n Radars(radio detection and ranging) transmits radio\n waves that bounce off obstacles, measures the signal's\n travel time, and estimates the object's radial velocity\n through the Doppler effect. They are highly adaptable to all\n kinds of light and weather conditions, but it is very\n challenging to classify objects by radar due to its low\n resolution. Radar is often used in adaptive cruise control\n (ACC) and traffic jam assistance systems[11]. In 1999,\n Distronic (DTR) radar control system was installed on the\n Mercedes-Benz 220 Series S-Class sedan, providing basic\n ACC function by controlling the distance between the car\n and the car in front of it at 40km to 160km per hour[8]. At\n present, the radar of major international manufacturers has\n developed to the 5th generation, basically maintaining the\nupdate speed of 2 to 3 years. At the same time, domestic\n millimeter wave radar manufacturers have also had about 5\n years of development, the head company's mass\n production products have been close to the international\n advanced level. Although the accuracy and detection range\n have been greatly improved, but overall its development\n stage has not yet achieved essential breakthrough.\n 2.4. Ultrasonics\n Ultrasonic radar uses ultrasonic waves to detect obstacles\n ahead. The ultrasonic generator emits ultrasonic sound\n waves in a certain direction. When the sound waves are\n propagated in the air, they meet obstacles and return to the\n original path. The ultrasonic receiver receives the echo and\n stops the timing. The distance between the sensor and the\n obstacle can be calculated according to the speed and time\n difference of the ultrasonic wave in the air. Ultrasonic\n radar is an important sensor for parking function due to its\n low price (a few yuan to 10 yuan) and short detection\n distance. In addition, the ultrasonic radar (APA) on the\n side of the car can also act on the lateral driving auxiliary\n alarm function[6]. Ultrasonic radar theoretically works on\n all objects that can reflect ultrasonic waves, including\n solids and liquids, but its ability to detect them is\n compromised if the target is an angled plane that can\n reflect sound waves to other angles, or an object such as a\n sponge or foam that can absorb ultrasonic waves.\n Therefore, they are usually used in close object detection\n and low speed scenarios.\n 2.5. GNSS and HD Maps\n GNSS is the abbreviation of \"Global Navigation\n Satellite System\". It refers to all satellite navigation\n systems in general, including global, regional and\n enhanced, such as the US GPS, Russia's Glonass, Europe's\n Galileo, China's Beidou navigation Satellite System[4].\n Also it related enhanced systems, such as WAAS (Wide\n Area Augmentation System) in the United States, EGNOS\n (European Static Navigation Overlapping System) in\n Europe and MSAS (Multifunctional Transportation\n Satellite Augmentation System)[4] in Japan, and other\n navigation satellite systems under construction or to be\n built in the future. The international GNSS system is a\n complex composite system with multiple systems, layers\n and modes. In other words, it is a large system composed\n of multiple satellite navigation and positioning systems\n and their enhanced systems. As a result, GNSS positioning\n accuracy is much higher than civilian GPS, but the error is\n still several meters. To achieve safe autonomous driving,\n ensure a high precision GNSS is the key. Originally\n introduced as a navigation tool for driving assistance in\n cars, GNSS is now also used in conjunction with\n high-definition maps for autonomous vehicle path\n planning and personal vehicle location. At the present\n stage, GNSS positioning alone is still some distance from\n fully meeting the position accuracy requirements of\n automatic driving. According to analysis, autonomous\n driving on expressways requires a minimum error of 1.5\n meters, while autonomous driving in urban areas requires a\n centimeter-level accuracy. In addition, it is easy to lose\n signal in navigation when entering tunnels or built-up\n metropolitan areas, which is why it is difficult to use GNSS\n alone to locate.\n Figure 4\n 2.6. Inertial Measurement Unit and Odometer\n The Inertial Measurement Unit (IMU) and odometer\n provide internal information about the vehicle. It can be\n used as an effective supplement when sensor data is\n missing. The IMU measures three-dimensional linear\n acceleration and three-dimensional angular velocity, and\n from this information, the vehicle's attitude (pitch and roll\n Angle), heading, speed and position change can be\n calculated. IMUs can be used to fill in the gaps between\n GNSS signal updates and even dead reckoning if the\n GNSS and other sensors in the system fail. The key\n advantage of the IMU is that it works well in all weather\n and geographical conditions. As an independent data\n source, it can be used for short-term navigation and to\n verify information from other sensors, and is not\n invalidated by weather, lens dirt, radar and lidar signal\n reflections, or the urban canyon effect. As an independent\n sensor, IMU is regarded as the sensor that supplements and\n confirms the data of other sensors, namely the \"last sensor\",\n which is used to ensure the safety of the vehicle and stop\n the vehicle in a controlled manner when other sensors are\n damaged or disabled. Therefore, we call IMU the anchor of\n the autonomous driving system[2].\n 2.7. Object Detection\n Object detection is an important task in computer vision,\nwhich develops computational models for a specific class\n of visual objects (such as humans, animals or cars). If we\n think of object detection today as a technological aesthetic\n under deep learning, then go back 20 years and we will\n witness the \"wisdom of the age of cold weapons\". Most of\n the early target detection algorithms were built based on\n manual features. Due to the lack of effective image\n representation at the time, people had no choice but to\n design complex features and utilize various acceleration\n techniques to pk limited computing resources. There are\n three main algorithms: Viola Jones detector, HOG\n detection and DPM algorithm[1].\n Figure 5\n Currently, most of the frameworks used in object\n detection are based on convolutional neural networks\n (CNN)[14], which can effectively extract image features.\n Because of different task requirements, object detection is\n divided into two categories: object detection algorithm of\n two stages and object detection algorithm of one stage. The\n target detection in the Two stages is to first generate a\n series of candidate boxes as samples by the heuristic\n candidate region generation algorithm, and then classify\n samples by convolutional neural network. In one stage, the\n problem of target frame location is directly transformed\n into a regression problem without generating candidate\n boxes. Compared with the two, each has its advantages and\n disadvantages. The target detection algorithm of the two\n stages has higher detection accuracy and positioning\n accuracy, which is suitable for tasks requiring\n high-precision identification. However, the target\n detection algorithm of one stage is faster and suitable for\n real-time recognition tasks[7]. The target detection\n algorithm of the Two stages is mainly represented by the\n RCNN series. The main idea is to generate area proposal\n and separate the target classification detection, so as to\n obtain more accurate detection effect. There are also\n SPP-Net algorithm, Fast R-CNN algorithm and Feature\n Pyramid Networks[13].\n Although Faster R-CNN, the peak of the two stages, has\n made great progress in computing speed, it still cannot\n meet the requirements of real-time detection. Therefore,\n researchers are thinking about how to achieve real-time\n target detection. So a method based on regression has been\n Figure 6 (The Faster R-CNN object detection network. It\n consists of three parts: a pre-processing network to\n extract highlevel image features, a Region Proposal\n Network (RPN) that produces region proposals, and a\n Faster-RCNN head which fine-tunes each region\n proposal.)[1]\n proposed to directly return the location and type of the\n target object from the picture. The target detection\n algorithm of one stage can realize the sharing feature of a\n complete single training, and the speed is greatly improved\n under the premise of ensuring a certain accuracy. Two\n typical algorithms are YOLO and SSD[12].\n Target detection can provide automatic driving with\n traffic sign recognition, pedestrian and vehicle recognition,\n traffic signal recognition and other basic functions.\n Currently, vehicles capable of autonomous driving\n generally rely on ultrasonic sensors in the front and rear\n sides of the vehicle, millimeter-wave radar in the center of\n the front, and forward-looking cameras under the rearview\n mirrors to achieve autonomous driving. Front side and rear\n side ultrasonic sensors are used to detect close distance\n obstacles, help automatic parking, etc. Millimeter wave\n radar is used to detect distant obstacles. The\n forward-looking camera can complete the detection of lane\n lines and obstacles on the road surface.\n 2.8. Semantic Segmentation\n In recent years, with the rapid development of deep\n learning technology, great breakthroughs have been made\n in many tasks that are difficult to be solved by traditional\n methods in the field of computer vision. Especially in the\n field of image semantic segmentation, deep learning\n technology plays a particularly prominent role. Image\n semantic segmentation is a fundamental and challenging\n task in computer vision. Its goal is to assign corresponding\n semantic labels to each pixel in the image. The result is to\n divide a given image into several visually meaningful or\n interesting areas, which is conducive to the subsequent\n image analysis and visual understanding.\n Image segmentation task requires each pixel of the\n original image to predict its category pixel by pixel. Since\n it is necessary to predict the category of the object pixel by\n pixel, this poses great challenges to the deep learning\n model. For example, some objects are small in size and\n difficult to identify, and some objects are mostly blocked,\nof the datasets that only include camera images for other\n traditional tasks in computer vision field, training neural\n networks in this case requires large quantities of\n multi-modal datasets with accurate labels. However, to\n collect such multi-modal data will not only face difficulties\n in both hardware and software technology, but also require\n a great deal of time and money. The following are several\n aspects of datasets that should be taken into consideration.\n Each of them can have a dramatic influence on the\n performance, thus indicating the potential method to\n improve the performance.\n Please use footnotesiii sparingly. Indeed, try to avoid\n footnotes altogether and include necessary peripheral\n observations in the text (within parentheses, if you prefer,\n as in this sentence). If you wish to use a footnote, place it at\n the bottom of the column on the page on which it is\n referenced. Use Times 8-point type, single-spaced.\n Figure 7\n resulting in reduced recognition.\n Although semantic segmentation was first introduced to\n process camera images, many methods have been\n proposed to segment lidar points. A number of data sets for\n semantic segmentation have been released such as\n Cityscape, KITTI, Toronto City, Mapillary Vistas and\n ApolloScape[11]. These data sets advance deep learning\n research on semantic segmentation in autonomous driving.\n Semantic segmentation can also be classified as two\n stages and one stage. In two stage pipelines, domain\n proposals are first generated and then fine-tuned, mainly\n for instance level segmentation (e.g. R-CNN, SDS,\n Mask-RCNN[1]). For semantic segmentation, a more\n common approach is the single-stage pipeline based on the\n full convolutional network originally proposed by Long et\n al. In this work, the full join layer in the CNN classifier\n used to predict classification scores is replaced by a\n convolutional layer to produce a rough output map. These\n maps are then up-sampled to dense pixel labels by reverse\n convolution (i.e. deconvolution). Kendall et al. extended\n FCNbyintroducing encod-decoder CNN architecture[13].\n The purpose of the encoder is to generate a layered image\n representation through the CNN backbone (removing the\n fully connected layer). Instead, the decoder restores these\n low-dimensional features to their original resolution\n through a set of up-sampling and convolution layers. The\n recovered feature map is finally used for pixel label\n prediction.\n 3. Datasets\n As the methods of multi-sensor object detection and\n semantic segmentation are mostly based on supervised\n learning, the quality of datasets becomes quite significant\n to good perception ability in autonomous driving. Instead\n 3.1. Sensing modalities\n In terms of sensing modalities, the mainstream methods\n mainly fuse RGB images taken by vision camera either\n with thermal images or with 3D point clouds. Therefore,\n three most common types of data in current datasets are\n RGBimages,thermal images and LiDAR pointclouds. For\n example, KITTI[14][15], a widely used dataset for vision\n recognition system provides a benchmark developed by\n their autonomous driving platform including RGB camera\n images recorded by four high solution stereo cameras and\n LiDAR point clouds recorded by a laser scanner. An\n GPS/IMU inertial navigation system is also involved to\n provide the path and position information of the car[15].\n Ha et al.[16] not only made contributions to semantic\n segmentation by producing a new CNN architecture\n named Multi-spectral Fusion Networks (MFNet), but also\n creating a groundbreaking public RGB-thermal image\n dataset for training in relative methods. KAIST\n multi-spectral dataset provided both LiDAR cloud points\n and thermal images with their self-developed multi-sensor\n platform[17]. For all-day vision of autonomous driving,\n creating a dataset involving several sensing modalities is\n quite challenging because many sensors need to be aligned.\n To solve this problem, they proposed a calibration\n technique so that their autonomous driving platform can\n support the use of a co-aligned group of sensors including\n a stereo vision camera, an RGB-thermal camera, a 3D\n LiDAR and GPS/IMU sensor groups, thus providing\n several combinations of modalities.\n Figure 8 below demonstrates clearly different sensing\n modalities available in KAIST multi-spectral dataset:\n calibrated RGB-stereo image pairs (the 1st column), 3D\n LiDAR data (the 3rd column), the thermal images\n co-aligned with the left-view RGB image using the\n beam-splitter(the 4th column)[18]. What’s more, the\n chaotic bottom image of the 2nd column has shown us the\nweakness of RGB-stereo images in providing information\n at night when light condition is poor. To fulfill\n unsupervised depth estimation for all-day vision, Kim et\n al.[18] proposed the multi-task framework named the\n Multi-spectral Transfer Network (MTN) to generate depth\n images from a single thermal image that are not sensitive\n to the light condition. This indicate the significance of\n multi-modal fusion that it can leverage complementary\n advantages of various sensing modalities to improve the\n performance of the deep neural networks in object\n detection and semantic segmentation.\n Figure 8[18]Different sensing modalities\n in KAIST datasets\n 3.2. Environmental conditions\n As an autonomous driving system is likely to confront\n all types of complex environmental conditions and is\n expected to deal with them correctly every time to ensure\n the absolute road security, an ideal multi-modal dataset\n used for training should cover various environmental\n conditions as widely as possible, including different\n weather conditions such as rainstorm, snowstorm, thick\n fog, different time periods of the day such as early morning,\n noon and late night. To further improve the accuracy and\n stability of object detection, the datasets should also be\n recorded in different locations to enhance its diversity.\n Nevertheless, many datasets are less efficient for their lack\n in diversity of environmental conditions. Although the\n well known dataset in autonomous driving, KITTI[14][15],\n are widely recognized in this field, it has great limitations\n as the whole dataset is recorded in a single city, Karlsruhe,\n Germany. Some other datasets have take the diversity of\n environmental conditions into consideration. For instance,\n nearly 20 million images of the Oxford RobotCar Dataset\n are collected by the Oxford RobotCar platform with its 6\n visual cameras, LiDAR, GPS and inertial sensors when it\n travel along the same route in Oxford twice a week from\n May2014 to December 2015[20]. Figure 9[20] shows 9 of\n over 100 images taken by the robot car at the same location\n of the route, illustrating great differences between the\n appearances over a series of conditions. The group of the\n images vary from overall lighting condition to seasonal\n variations, all these changes produce a profusion of\n different combination of environmental conditions to this\n dataset for training.\n Figure 9[20] Some of the images taken by the\n Oxford RobotCar platform at the same location\n under different environmental conditions\n 3.3. Dataset size\n Due to the complexity of the multi-modal datasets and\n the difficulties in creating a new one, datasets for\n multi-sensor object detection and semantic segmentation\n are relatively smaller and fewer. The widely used\n KITTI[14][15] only has nearly 7000 frames for training,\n while KAIST provides nearly 100K frames and 100K\n objects in the datasets[17]. Some other datasets include\n continuous video image frames providing similar\n information that are less useful for training. We can see\n that advanced researches in this field are still in great need\n of large-scale and high-quality datasets.\n 3.4. Labels\n Labels are given in most datasets used for training in\n object detection and semantic segmentation tasks.\n Depending on the purposes that the datasets are\n constructed for, in different datasets, even the same object\n can be labeled in different ways. For example, KAIST\n multi-spectral pedestrian dataset focuses on recognizing\n people, thus only including labels for distinguishable\n single person and non-distinguishable individuals and\n cyclists[17]. KITTI provides different labels for\n pedestrians and the sitting people, it can also distinguish\n different vehicles like cars, trucks and vans[14][15]. One\n potential problem is that in most of the datasets, vehicles\n and people are dominant in labeled objects, which might\n weaken the ability of the trained deep neural networks to\n recognize other objects like road signs or trees.\n4. Methods\n points in neighborhood with grouping at different scales to\n make the method robust and accurate in detail.\n 4.1. Processing point clouds\n When fusing LiDAR point clouds with RGB images in\n multi-sensor object detection and semantic segmentation\n tasks, one of the key points is that besides the visual\n information provided by RGB images, 3D object detection\n also needs accurate depth information provided by LiDAR\n point clouds to estimate the distance between the vehicle\n and the object. So there are several researches\n concentrating on a variety of methods to properly represent\n and process the point clouds, so that the deep neural\n networks can learn the features as accurate as possible.\n 4.1.1 3D voxelization\n One method is to divide the 3D space into little 3D\n voxels, where the concept of a voxel in the 3D space can be\n likened to a 2D pixel of a 2D image, so the 3D point cloud\n can be assigned to the voxels respectively. The advantage\n of this method is that voxelization can retain rich spatial\n information of the 3D object. But there are many\n remaining empty voxels because the point clouds are\n usually sparse, making the corresponding 3D-CNN and\n clustering methods quite time-consuming when processing\n such data, which is an unacceptable case in real-time\n autonomous driving. To handle sparse LiDAR point clouds,\n Zhou et al.[21] proposed VoxelNet, a universal 3D\n detection end-to-end trainable deep neural network. They\n introduced a Voxel Feature Encoding (VFE) layer, as\n shown in the Figure 10, to process the point cloud\n efficiently. By conducting experiments on widely used\n KITTI dataset, the VoxelNet has shown outstanding\n performance among the state-of-the-art methods based on\n 3D LiDAR point clouds with an inference time of only\n 225ms.\n Figure 10[21]Voxel feature encoding(VFE) layer\n 4.1.2 Direct learning over points\n Instead of voxelization, some related work based on 3D\n LiDAR directly conducts deep learning on point sets.\n PointNet++ is able to learn individual features from each\n point in the LiDAR point clouds and use max pooling to\n aggregate features of a set of points[22]. It leverages the\n 4.1.3 Projection to 2D feature maps\n Another way to process the 3D point cloud is to utilize\n the idea of processing 2D images. That is, we can project\n the LiDAR point cloud to various 2D feature maps, and\n then leverage 2D convolutional layers to process them.\n Figure 4 displays the RGB image and several types of 2D\n feature maps, such as spherical map, camera-plane map\n and Bird’s Eye View (BEV) map, of a street scene[1]. The\n spherical map can represent the point cloud in a ideally\n dense way by projecting the points on a sphere. However,\n without preprocessing, such spherical map is difficult to\n align with RGB camera images for multi-modal fusing.\n Though the camera-plane map can directly be used for\n fusing, empty pixels exist in the sparse map, thus requiring\n up-sampling methods to fill these pixels and turn it into a\n dense map for further processing. The BEV map is another\n choice that keep the distribution of the objects on the\n ground plane, including their lengths, widths and distances\n between each other.\n Figure 11[1]Different 2D LiDAR representations\n 4.2. Processing camera images\n Most current methods of multi-sensor object detection\n and semantic segmentation take RGB images from vision\n camera or infrared images from thermal camera as one of\n the sensing modalities. They include rich visual\n information demonstrating the surroundings. Thermal\n camera is particularly good at capturing images when\n lighting condition is limited. However, one problem is that\n for an object in an camera image, the shape can be\n distorted and the size can vary on the camera plane, thus\n affecting object detection. To solve this problem, Roddick\n et al.[23] develop an Orthographic Feature Transform\n (OFT) algorithm to project the camera images to a BEV\n representation that is commonly used for processing a\n point cloud, which largely preserves the features of the\n objects.\n4.3. Fusing point clouds and camera images\n Challenges[J].\n IEEE Transactions on Intelligent\n Transportation Systems, 2020, PP(99):1-20.\n Most of related works fuse LiDAR point clouds and\n camera images on the network level. That is, they project\n the point clouds to 2D feature maps and use the\n convolutional neural networks to process them as we\n described in Part III- 1- 3). These methods fuse two\n modalities by fusing the features obtained from the\n networks. Some methods conduct clustering directly on 3D\n point clouds to generate labels for region proposals.\n Several works also leverage the idea of projection to align\n features across different sensing modalities. For example,\n projecting RGB images onto BEV plane when the point\n cloud is represented as a BEV map.\n 5. Challenges\n 5.1. Data diversity\n Ideal datasets with great diversity in environmental\n conditions and object labels with several sensing\n modalities are needed to improve the accuracy and\n robustness of multi-sensor object detection and semantic\n segmentation. So far, available datasets are still small in\n scale and few in number.\n 5.2. Data quality\n Even the data labeled manually might include some\n errors in labels. These errors might lower the accuracy of\n object detection when the labeled data are used for training.\n What’s more, different sensors that are not calibrated\n accurately can have temporal and spatial deviations, which\n might produce critical errors in datasets. Further researches\n are needed for sensor calibration to improve the quality of\n the datasets and the efficiency of calibration.\n 5.3. Sensor redundancy\n Current works mainly focus on fusing different sensing\n modalities, but few conduct research on fusing data\n recorded by several sensors of the same type. Such sensor\n redundancy can not only collect more information from the\n surroundings, but also improve safety of autonomous\n driving.\n 5.4. Network architecture\n Most of the works use convolutional neural networks\n for real-time perception without considering previous\n states. In the future, new network architectures that can\n process time series might be introduced to multi-sensor\n object detection and semantic segmentation task."}, {"role": "assistant", "content": "最终打分：7.9 (范围0-10分)\n1. 结构完整性得分：8, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：7.5, 占比20%\n5. 参考文献规范性得分：8, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "智能考古项目报告  \n南方科技大学 ·2022 年 · 人工智能导论 (CS103)· 课程项目\n作者：章志轩  12010526 、陈志雄  12010302 、宫正  12012803 、肖佳辰  12112012 、薛丁元  \n11910213\n课程老师：刘江\n指导老师：荆志淳、章晓庆\n@ 2022.12.18\n \n一、背景  \n中国作为世界上用玉历史最为悠久的国家之一，在人们的物质生活和精神层面都占据着重要的地\n位。中国传统意义上的玉是软玉 , 又名钙角闪石（透闪石 - 阳起石； ) 。古\n人将玉视为君子的化身，是天地的精华或万物的主宰，认为长期佩戴能获得神明的赐福，保佑平安。因\n此在千年中华文化发展中形成了一种独特的中华民族文化 —— 玉文化。\n                  图1 ：古玉器\n玉文化在中华民族文化中占据显著地位，玉的概念和定义直接影响着我们如何看待和认识古玉的基\n本特性和其所反映的社会、文化、技术、祭祀、礼仪及宗教信仰等。对古玉的分析将有助于我们理解先\n民社会形式、文化内涵，揭露古人的思想文化与传承至今的民族精神。比如在众多的矿物岩石（不乏各\n种各样的宝石和美石）中，中国先民选择软玉来体现重要的文化价值和社会关系，而且这样的选择，持\n续数千年而不变。一种说法是软玉具有显微交织纤维结构，这种结构赋予软玉高韧性，高致密度，进而\n决定了软玉特有的温润光泽和质感，被古人视为 “ 玉德 ” 。《说文解字》总结玉有五德仁，义，智，勇，\n洁，为君子之石。\n                 图2: 玉的显微结构\n二、项目目标  \n本项目将分析玉石光谱数据，实现人工智能对玉石矿物成分和质地进行分类。\n三、实验设计  \n1. 实验数据  \n(1) 光谱数据  \n实验数据是由荆志淳教授提供的安阳花园庄 M54 墓穴出土的玉器光谱资料，共 439 个实验对象，光\n谱数数采取波长 1300~2500nm 波段，每 2nm 采取一条数据。同时荆教授还提供了一份去噪数据 ( 去除背\n景值 contiuum-removed) ，共 231 个实验对象，光谱采样与之前相同。\n(2) 人工分类数据  \n荆教授同时提供了一份人工区分的古玉数据，指标为：矿物成分、质地分类、颜色、光泽、半透明\n度、次生变化和埋葬受沁共 7 个维度。其中我们只关心 矿物成分 和 质地分类 两类指标。\n矿物成分 ： Np I - 软玉类型  I ， Np II - 软玉类型  II ， Np III 软玉类型  III ， At - 叶蛇纹石， Ct - 绿泥\n石， It - 伊利石， Dc - 迪开石， Qz - 玉髓、隐晶质石英， UN - 未知矿物或岩石；\n质地分类 ：A - 矿物组成为 Np I 的器物； B 、  B1 、  B2 - 矿物组成类同于 A, 为 Np I, 但含有微量蛭石，\nB - 未或微弱受沁或次生变化（不包 A 型  I 式玉管）， B1 - 中等或强烈受沁或次生变化（不包 A 型  I 式玉\n管）， B2 - A 型  I 式玉管 , 未或微弱受沁或次生变化  ； C - A 型  I 式玉管，矿物组成为 Np II ； D - 矿物组成\n为Np III 之器物； S - 非玉之矿物或岩石。\n2. 实验流程  \n(1) 数据预处理  \n首先去除非法数据，然后分别用两种方法独热编码标签，一种是将任务看作多分类任务，对每一种\n矿物成分都进行独热编码，如图所示：\n                 图3: 各矿物类型独热码\n另一种是将任务看作二分类任务，将矿物成分分为 Np 类和非 Np 类进行编码。最后对数据分为训练\n集和测试集并进行标准化处理。(2) pca 数据压缩：  \n将原始数据按列组成 n 行 m 列矩阵 X\n将X 的每一行（代表一个属性字段）进行零均值化（去平均值），即减去这一行的均值\n求出协方差矩阵  C= 1/mX*XT\n求出协方差矩阵的特征值及对应的特征向量\n将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P （保留最大的 k 各\n特征向量）\nY=PX 即为降维到 K 维后的数据\n代码流程大致如下：\n去除平均值\n计算协方差矩阵\n计算协方差矩阵的特征值和特征向量\n将特征值从大到小排序\n保留最上面的 N 个特征向量\n将数据转换到上述 N 个特征向量构建的新空间中\n(3) 模型 / 算法选定  \n机器学习上我们选择 SVM 和随机森林两个算法进行实验，深度学习上我们选择 MLP 进行实验，原因\n如下：\n随机森林：随机森林可以生成多个决策树进行训练，最后结果取各个树的众数。它实现简单，\n计算量小，可以处理更多特征的数据。随机森林和 SVM 都是专对物品分类的模型。  \n- \nSVM ： SVM 则是在空间中找到将样本分开的最好的一个超平面，所以它只能对一个标签进行\n0/1 分类，并且样本数量多的时候效率不高，但是它在小样本下表现更好，也无需依赖整个数\n据，无局部极小值问题。但是在实际使用中，因为 Np 类型占比过高，导致对此单一分类很难\n看出区别。权衡之下，我们用了随机森林对全标签进行分类，而 SVM 只做了矿物成分的 Np 与\n非Np 的预测。\nMLP ：多层感知机是最经典的神经网络。使用中间隐含层、激活函数和反向传播进行对样本的\n学习，从而实现预测。它的好处是更加普适化，实现简单。我们也用它做了全标签的分类，并\n与随机森林做了简单比较。\n(4) 设计实验  \n因为需要预测矿物成分和质地分类两个标签，我们将其视为两个任务，分别进行分类预测，再将结\n果合并成最终结果。然后使用以上三种模型 / 算法进行实验并对比结果。\n(5) 训练  \n随机森林和 SVM 的训练过程都是直接调用 sklearn 库中的函数使用默认参数进行训练， MLP 的训练过\n程则是先定义模型，模型为五层每层 100 个神经元，然后使用 fit 函数进行训练。\n3. 实验结果  \n结果的好坏由模型的预测准确率决定。以下是实验结果：                  图4: 实验结果\n                图5: 矿物成分分类散点图\n1. 左为原始数据的数据分布，右为降噪处理后的数据分布\n2. 从上到下，第一行是 Np 和非 Np （ Np 为蓝）；第二行是 Np 类合并的 7 类矿物（ Np 类为\n蓝）；第三行是全部 9 种矿物（ Np I 为蓝）\n3. 散点图基于   库（一个纯 JavaScript 的图表库）绘制，由于数据量并不是很\n大，我们直接将数据筛选分类后拷贝进   文件中绘制图表。 Echarts 官方网\n址：  https://echarts.apache.org/zh/index.html ，可访问其网址完成对应配置或在线直\n接使用。\n图表绘制代码见上传文件  scatter-simple.html4. 分析  \n通过矿物成分分类散点图，我们可以清晰的看出同类矿物有集中分布的趋势，且区别明显。但对于\n同属于 Np 大类的（ Np I 、 II 、 III ），我们不能够很好的区分，在原始数据还可以看出 Np I( 蓝 ) 和 Np II( 绿 )\n的区别，但在右边图中已经混合在一起。对于 Np 类的区分主要出于其它维度的考虑 ( 事实上它们在外观\n包括颜色、光泽等有较大的区别 ) ，所以在矿物成分维度的区分不明显是正常的，它们的区别在于一些杂\n质( 不属于软玉的部分 ) 有所不同，这带给了玉器不同的物理特征进而被区分。由于这部分不是本项目的重\n点，所以本文不再过多探讨。\n对于模型的分析，首先是 SVM 模型的实验失败。尽管它的预测准确率很高，但我们分析数据，发现\nNp 类别的实验对象远多于非 Np 类别，模型只需要一直预测 1 ，就会得到极高的准确率，这显然是不合理\n的。事实上模型对 1 的预测全对了对 0 的预测只有 0.92 左右。由此， SVM 的实验结果并没有特别多的参考\n意义。\n对比随机森林和 MLP ，可以发现 MLP 的预测准确率高于随机森林，在矿物成分和质地分类上具有优\n势。其原因可能是我们使用的数据集太小，或者非 Np 类的数据量太小，导致随机森林不能产生很好的分\n类。尽管如此，我们还是可以发现随机森林占优的地方。比如当我们将矿物成分和质地分类同时考虑\n时，每条训练数据的维度都会增大，这会提高训练难度。自然的类似于 MLP ，预测结果就发生显著降低\n(0.81 → 0.6) ，但随机森林基本保持着精度不变，和直接综合的结果等同。\n接着是数据集的影响，本实验分了两类数据：原始数据和去噪数据。实验结果显示当使用去噪数据\n时，预测精度会下降。其最直接的原因是数据量的减小 (439 → 231) ，其仅为原始数据的一半。理论上当\n光谱数据去除背景值后，将减低测量误差 ( 背景值不可能完全去除，误差也不可能为 0) ，有利于提高分析\n结果的稳定性和准确度。为此我们重新设计了一个实验：在原始数据中随机挑选 231 条，重新进行训练\n并预测。得到的结果为：\n               图6: 相同数据量下的实验结果\n可以看出，随着数据量的减小，预测准确率确实有所降低，原始数据与去噪数据的预测结果基本等\n同。但若是视所有 Np 类为同一类，那么原始数据的结果依旧高于去噪数据。因此前面关于数据量的推测\n并不完全准确，去噪数据存在其它因素导致精度下降。抛去去噪过程的影响 ( 因为去噪工作并不是我们完\n成的 ) ，我们重点分析了去噪数据的数据分布，发现去噪数据的 Np 与非 Np 的数据量差距更加巨大。对于\n非Np 类，随着负样本增多，精度上升，召回率上升；对于 Np 类，负样本减少，精度下降，召回率下\n降。由于数据集以 Np 为主，当 Np 的预测精度下降，整体的预测结果也就下降。这也就造成了去噪数据\n预测结果反而不如原始数据。\n所以，最终结果为使用原始数据的精度最高，因为数据量大且正负样本相对更均衡。且若是仅关注\nNp 大类，精度可以接近 100% ，是十分理想的预测结果。\n四、改进方案 & 总结  \n使用 MLP 模型能够得到由于随机森林的预测精度。对于矿物成分的预测，最高的预测精度为 100% ，\n质地分类的预测精度为 0.91 ，整体预测精度为 0.96 。我们选择模型时有考虑到模型的实现难度、训练难\n度以及是否适合矿物分类三个要素。这给了我们很好的实验空间，代码复用率高。当然这里肯定存在更\n加适合的网络模型，比如可以将 MLP 替换成 CNN ，或是其它适合的网络模型。其次是在数据集的选择\n上，本次我们使用了现成的数据集，这固然减少了我们的工作量，但也难以得到我们理想的数据样本。就如本次数据集的正负样本差距过大，造成后续实验的出错，未能达到理想预期。为此，在数据收集以\n及预处理上下功夫将显著提高我们的实验结果。\n本次项目我们只实现了玉石的矿物成分和质地的分类。后续我们将应用本次实验的结果结合考古领\n域专家对玉和玉料来源进行分析。这将有望得出外来玉器判别进而得出古代社会文化交往的证据等可观\n的研究结果。再进一步可以研究玉器其它特征属性 ( 颜色、光泽等 ) ，并尝试提取特征波段以发现古人识玉\n的秘密。最后感谢荆志淳教授和章晓庆博士提供的帮助，在你们的引导下，我们都从本次项目学到了很\n多知识与经验。"}, {"role": "assistant", "content": "最终打分：8 (范围0-10分)\n1. 结构完整性得分：8.5, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：9, 占比20%\n5. 参考文献规范性得分：3, 占比10%\n6. 课程知识掌握度得分：10, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "LIN YUHANG, PENG SHUBO, DECEMBER 2022 1\nSurvey of Intelligent Archaeology\nLin Yuhang, Peng Shubo\nAbstract —Recently more and more AI techniques are used\nin archaeology area, one of which is machine learning (ML).\nApplication of ML can be divided into two main categories: object\nclassification and site searching. Classification can not only be\nused to classify fragments, like pottery or papyrus, but also help\nto merge messy fragments into complete patterns. Site searching\nis a new topic raise recently thanks to the popularity of satellites,\nwhose generated systematized data offers premises for ML. But\nML still has some shortage, most of which limited by nature of\narchaeology. In future, ML maybe more used in identification of\narchaeological sites, or illegal treatment about art products.\nIndex Terms —Archaeology, machine learning, classification,\nsite searching.\nI. I NTRODUCTION\nARTIFICIAL Intelligence (AI for short) is a new study\narea rising in recent centuries, and can be divided into\nseveral directions, like Machine Learning (ML) or Neural\nNetwork (NN). Five to ten years ago, they were concepts\nunknown to archaeologists. But now, AI is widely used,\nthere are even sessions dedicated to AI at archaeological\nconferences.[10]\nML is programming allowing algorithm learning from data\nand adjust its parameters and then make predictions on new\ndata. Objects must be quantify into digital data first and can be\nany type, like sonar data under water[9] or aerial laser scanning\ndata [11]. ML uses mathematical techniques to analyze a set\nof already-classified objects and generate ”classifiers” for each\ncategory. In theory, objects in every category is identified from\nother categories in mathematics. In short, ML use math to\nclassify quantifiable objects into different groups.[5]\nMl application can be divided into two main types: classify\narchaeological objects, and identify archaeological sites, both\nof which will be explained more detailed in II.\nDeep learning(DL) a branch of ML, the biggest difference\nis that, ML needs human to appoint characteristics of object to\nbe trained, while DL do not need pre-appointed characteristics.\nWhat DL processing is a large number of data, and DL can\nlearn and choose significant characteristics by itself. DL will\nfirst determine which characteristics are related to its goal,\nthen attempt to obtain accessible features on layer level, finally\ndetermine which accessible features match its goal best.\nII. A PPLICATION\nDL has been successfully used in many applications. Among\nthe DL methods, recurrent neural networks (RNNs) are good at\ndealing with sequential data as they take into account temporal\ninformation. RNNs have been applied in speech recognition to\nmap acoustic sequences to phonetic sequences. RNNs have\nalso been used in natural language processing to translate\ntext from one language to another. Another famous methodin DL is convolutional neural networks (CNNs). CNNs take\ninto account spatial correlation among data points and hence\nperform well in image-based data. CNNs have been used in\nimage classification, face recognition, scene labelling, and so\non. DL methods are also used in the field of remote sensing.\nFor example, Hu and Yuan used CNNs to extract digital terrain\nmodels (DTMs) and filter out non-ground points from airborne\nlaser scanning (ALS) data, which was claimed that this method\nperforms better than previous filtering methods[12].\nIn the papers we’ve read, we’ve found that all of them,\nwithout exception, use CNNs to study a specific thing. This is\nbecause the things they studied were all based on image data,\nin which CNNs performs better than RNNs.\nWe divide the papers that use CNNs into two types: one is\nto classify archaeological objects like papyrus, potteries, and\nso on, the other is to detect objects in archaeological sites.\nA. classify archaeological objects\n•In the paper[1], a method is proposed for matching and\nassembling pairs of ancient papyrus fragments containing\nmostly unknown scriptures. This task, which is assem-\nbling fragments in a puzzle-like manner into a composite\npicture, plays an important role in the archaeology, be-\ncause it can help historic artifacts to reconstruct archae-\nological objects for research. The proposed method is to\nuse image processing and machine learning techniques\nto identify matching fragments, and then support the\nquick and automated classification of matching pairs of\npapyrus fragments as well as the geometric alignment of\nthe pairs against each other. The algorithm was trained\non a batch of fragments which was excavated from the\nDead Sea caves and is dated circa the 1st century BCE.\nThe algorithm shows excellent results on a validation\nset which is of a similar origin and conditions. Then\nthe algorithm was used to against a real-life set of\nfragments for with no prior knowledge or labeling of\nmatches. This test batch is considered extremely chal-\nlenging due to its poor condition and the small size\nof its fragments. Evidently, numerous researchers have\ntried seeking matches within this batch with very little\nsuccess. The algorithm performance on this batch was\nsub-optimal, returning a relatively large ratio of false\npositives. However, the results showed that this algorithm\neliminated 98% of the possible matches thus reducing\nthe amount of work needed for manual inspection, which\nmeans this algorithm was quite useful. Indeed, experts\nthat reviewed the results have identified some positive\nmatches as potentially true and referred them for further\ninvestigation. A glimpse of this project is shown in Figure\n1.LIN YUHANG, PENG SHUBO, DECEMBER 2022 2\nFig. 1: An example of two adjacent artificially torn fragments\nwith a set of candidate squares to be used in the matching\nphase.\n•In the paper[13], the project developed at Dumbarton\nOaks—a research institute and library, museum, and\nhistoric garden affiliated with Harvard University and\nlocated in Washington, DC— focused on a collection\nof 10,000 images of Syrian monuments in the institu-\ntion’s Image Collections and Fieldwork Archives (ICFA).\nDrawing on that project, as well as the broader landscape\nof AI-based categorisation efforts in the fields of art\nand architecture, authors’ insights on the potential of\nAI to facilitate and enhance archival image access and\nrecording will be shaerd. Many of the Syrian sites in\nthe Dumbarton Oaks collection have been inaccessible\nto researchers and the public for over a decade and/or\nhave been damaged or destroyed. For Dumbarton Oaks,\nthe primary goal was to explore whether AI can improve\nthe speed and efficiency of sharing collections and allow\nfor more sophisticated curation by subject experts who,\nthanks to automation, would be relieved of the burden\nof rote processing. The methods and techniques explored\nincluded multi-label classification, multi-task classifica-\ntion, unsupervised image clustering, and explainability.\nA glimpse of this project is shown in Figure 2.\n•The main contribution in this paper[10] is the completion\nof the project called ArchAIDE. This project realised an\nAI-based application to recognise archaeological pottery.\nPottery is of paramount importance for understanding ar-\nchaeological contexts. However, recognition of ceramics\nis still a manual, time-consuming activity , reliant on\nanalogue catalogues. The project developed two comple-\nmentary machine-learning tools to propose identifications\nbased on images captured on-site, for optimising and\neconomising this process, while retaining key decision\npoints necessary to create trusted results. One method\nrelies on the shape of a potsherd; the other is based on\ndecorative features. For the shape-based recognition, a\nnovel deep-learning architecture was employed, integrat-\ning shape information from points along the inner and\nouter profile of a sherd. The decoration classifier is based\nFig. 2: Explainability heatmaps for predictions of the classes\n“architecture” (left) and “fac ¸ades” (right) by the Phase 1\nclassifier.\non relatively standard architectures used in image recog-\nnition. In both cases, training the algorithms meant facing\nchallenges related to real-world archaeological data: the\nscarcity of labelled data; extreme imbalance between\ninstances of different categories; and the need to take note\nof minute differentiating features. Finally , the creation of\na desktop and mobile application that integrates the AI\nclassifiers provides an easy-to-use interface for pottery\nclassification and storing pottery data.\nA glimpse of this project is shown in Figure 3.\nIn particular, since this project ArchAIDE is the most\nclassic and mature one among all paper, the more detailed\nand overall explanation will be given in III.\nFig. 3: The ArchAIDE portal is available at the Archaeology\nData Service of the University of York.\n•The paper[15] indicate that deep learning with CNNs is\na highly accessible and effective method for classifying\nceramic fabrics based on images of petrographic thin\nsections and that it can likely be applied on a larger scale.\nClassification of ceramic fabrics has long held a majorLIN YUHANG, PENG SHUBO, DECEMBER 2022 3\nrole in archaeological pursuits. It helps answer research\nquestions related to ceramic technology, provenance, and\nexchange and provides an overall deeper understanding\nof the ceramic material at hand. One of the most effective\nmeans of classification is through petrographic thin sec-\ntion analysis. However, ceramic petrography is a difficult\nand often tedious task that requires direct observation and\nsorting by domain experts. In this paper, a deep learning\nmodel is built to automatically recognize and classify\nceramic fabrics, which expedites the process of classi-\nfication and lessens the requirements on experts. The\nsamples consist of images of petrographic thin sections\nunder cross-polarized light originating from the Cocal-\nperiod (AD 1000-1525) archaeological site of Guadalupe\non the northeast coast of Honduras. Two CNNs, VGG19\nand ResNet50, are compared against each other using\ntwo approaches to partitioning training, validation, and\ntesting data. The technique employs a standard transfer\nlearning process whereby the bottom layers of the CNNs\nare pre-trained on the ImageNet dataset and frozen, while\na single pooling layer and three dense layers are added\nto ’tune’ the model to the thin section dataset. After\nselecting fabric groups with at least three example sherds\neach, the technique can classify thin section images into\none of five fabric groups with over 93% accuracy in each\nof four tests. A glimpse of this project is shown in Figure\n4.\nFig. 4: Examples of the five ceramic fabric types analyzed.\n•In the study[17], an alternate approach to archaeological\ntypology which uses DL to classify digital images of\ndecorated pottery sherds into an existing typological\nframework is presented. This study focuses on a spe-\ncific kind of ancient painted pottery from the American\nSouthwest, Tusayan White Ware (TWW), but it is be-\nlieved that it has broader implications for a wide range\nof geographical settings and artifact types. The results\nshow that when properly trained, a deep learning model\ncan assign types to digital images of decorated sherds\nwith an accuracy comparable to, and sometimes higher\nthan, four expert-level contemporary archaeologists. The\ntechnique also offers novel tools for visualizing both\nthe importance of diagnostic design elements and overalldesign relationships between groups of pottery sherds.\nThis method can objectively match a specific unclassified\nsherd image to its most similar counterparts through a\nsearch of thousands of digital photos. This discovery has\nimportant archaeological implications for analyzing time\nrelationships, monitoring stylistic trends, reconstructing\nfragmentary artifacts, identifying ancient artisans, and\nstudying the evolution and spread of ancient technologies\nand styles. It also shows how deep learning models can\npotentially supplement or supplant traditional typologies\nin favor of more direct groupings and comparisons of\nartifacts. A glimpse of this project is shown in Figure 5.\nFig. 5: Grad-CAM heat maps for TWW sherds, showing\nareas of high (red) and low (blue) model activation for a\nKayenta sherd (top) and Flagstaff sherd (bottom). CNN-\nmodel-calculated type confidences shown below each sherd.\nB. detect objects in archaeological sites\nAirborne laser scanning (ALS) is of great use in collecting\nand documenting detailed measurements from an area of inter-\nest. However, it is time consuming for scientists to manually\nanalyze the collected ALS data. One possible way to automate\nthis process is using deep neural networks.\nIn the paper[14], a hierarchical CNN model is builded to\ndetect objects in archaeological sites using digital terrain mod-\nels (DTMs) generated from ALS data. The data is acquired\nfrom the Harz mining Region in Lower Saxony, where a high\ndensity of different archaeological monuments including the\nUNESCO world heritage site Historic Town of Goslar, Mines\nof Rammelsberg, and the Upper Harz Water Management\nSystem can be found. Objects to be detected are archaeological\nobjects such as hollow ways, streams, pathways, lakes, streets,\nditches, heaps, mining shafts, and more, but for this study, the\nmodel is fit to detect 4 classes of objects: natural streams,\nlakes, tracks, and an ’others’ class which represents the rest\nof the objects for which enough labeled data is not available\nyet. To compare and validate the method in this paper, someLIN YUHANG, PENG SHUBO, DECEMBER 2022 4\nexperiments on the same data set using two existing deep\nlearning models were conducted. The first model is VGG-\n16; an image classification network pretrained on ImageNet\ndata. The second model is a stacked autoencoders model. The\nresults of the classification as analyzed in this paper show that\nour model is suitably tuned for this task as it achieves the best\nclassification accuracy of around 91 percent, compared to 88\npercent and 82 percent accuracy by the pretrained and stacked\nautoencoders models, respectively. A glimpse of this project\nis shown in Figure 6 and 7.\nFig. 6: Heat maps using filter size 48 x 48. Colors show the\nconfidence of the models in detecting objects at that location.\nFig. 7: Heat maps using filter size 98 x 98. Colors show the\nconfidence of the models in detecting objects at that location.\nC. More\nWe believe that there are many other studies using RNNs\nto study the meaning of certain patterns on ancient artifacts or\nto try to decipher ancient texts. But this is out of the scope of\nour survey.III. A RCHAIDE P ROJECT\nTo make things clear, a classic and mature classification\nproject is briefly shown in this part. The reason why classifi-\ncation is chosen rather than searching sites is that, the number\nof paper about classification is much more than searching area.\nAmong all applications in paper, the most classic and the most\nmature one is picked out here.\nThe name of project introduced here is “ArchAIDE”. The\nmost remarkable characteristic of this project is that, it not\nonly invent algorithm and do analysis on data from both view\nof shape and decorations, but also realize a system that could\nhave a realworld implementation.\nA. General Introduction\nArchAIDE project aims at optimizing the ceramic identifica-\ntion process. It developed two different deep neural networks\nto recognize pottery through images using a mobile device.The\nfirst network is specially used for image recognition, also\ncalled appearance-based recognition. The second network uses\nthe shape of fragmentation to identify.\nUnlike familiar worries about AI, ArchAIDE will not re-\nplace the knowledge of domain specialists. On the contrary,\nit put archaeologists’ role in the center of decision-making\nprocess in the identification workflow, which can be seen in\nIII-C.\nB. Materials\nA correct result of classification relies on two parts: the\nlabel or the name of each category, and the available data for\nboth shape-based and appearance-based recognition. But first,\nthe class of relics should be determined.\n1) Classes for training:\nAmong all categories of cultural relics, the project choose\nfour realworld classes for training:\n•Amphorae manufactured throughout the Roman world\nbetween the late 3rd century BCE and the early 7th\ncentury CE. (Figure 8a)\n•Roman Terra Sigillata manufactured in Italy , Spain, and\nSouth Gaul between the 1st century BCE and the 3rd\ncentury CE.\n•Majolica produced in Montelupo Fiorentino (Italy) be-\ntween 14th and 18th century.\n•medieval and post-medieval Majolica from Barcelona and\nValencia. (Figure 8b)\n(a) Roman amphorae\n (b) Majolica of Montelupo Fiorentino\nFig. 8: Material for trainingLIN YUHANG, PENG SHUBO, DECEMBER 2022 5\n2) Label:\nTo get correct and helpful label of categories, the project\nimplements following systems:\n•A digital comparative collection for pottery types, deco-\nrations, and stamps, combining digital collections, digi-\ntised paper catalogues, and data acquired through photo\ncampaigns.\n•A semi-automated system for paper catalogues’ digitisa-\ntion.\n•A multilingual thesaurus of descriptive pottery terms,\nmapped to the Getty Art and Architecture Thesaurus,\nwhich includes French, German, Spanish, Catalan, Por-\ntuguese, English, and Italian.\nThe digital collections and paper catalogues to create digital\ncomparative collections are from already-present databases.\nThe first one is “Roman Amphorae: a digital resource”[19],\ncreated by Simon Keay and David Williams of the University\nof Southampton and published as open data on the Archaeol-\nogy Data Service, that includes the principal types of roman\namphorae between the late 3rd century BCE and the early 7th\ncentury CE. The other one is “CERAMALEX” database[16], a\nproprietary database of the German and French Heritage 2021.\nLimited by space, detailed principles of them will not be\nshown here.\n3) Training images:\nMultiple photo campaigns were also carried out in several\narchaeological warehouses, involving more than 30 different\ninstitutions in Austria, Italy , and Spain. Overall, 3498 sherds\nwere photographed for training the shape-based recognition\nmodel. For appearance-based recognition, a dataset of 13,676\npictures was collected through multiple photography cam-\npaigns.\nTo offset disadvantages above in some term, each original\nimage is scaled into four different sizes. On each scaled image,\nthree versions are created: unflipped, horizontally flipped, and\nvertically flipped. All of these images are cropped, leaving just\nthe central square. As a result, 12 images from each original\none were obtained.\nC. Method\nThe decoration of pottery fragments have higher priority\nthan shape because decorations is more reliable than the shape\nof fragments. When decoration presents, decoration will be\nused to identify rather than appearance. Appearance-based\nrecognition is used only when the pottery is undecorated.\n1) Shape-based Recognition:\nThe recognition tool was designed as a two-phase process,\nwhere the classification algorithm was first developed on one\ndataset and then validated on other datasets for different types\nof pottery.\nThe dataset used 65 standardised toplevel classes defined\nin Conspectus catalogue[4]. 2D model are created from these\ndrawings and photos taken in archaeological warehouses\nthroughout Europe by extracting the profile of the entire vessel\nfrom 2D drawing. Then 2D model rotate around revolution\naxis to form 3D models.Then many 3D planes are generated randomly. Computer\nwill calculated how all the circles intersect the plane, con-\nnecting the intersection points from the circles along the\nprofile to generate the fracture face. To create a more realistic\nsynthetic fracture, we reduced its size to match real potsherds’\ndimensions. [7] The progress is shown in Figure 9.\nFig. 9: Stpes from extracting profiles from 2D drawings, to\ncreation of 3D models to be broken.\nThe network was trained based on the requirement to divide\nthe inner and the outer profile of the sherd, the relevance\nof the position of the points along the profile outline, the\nintrinsic noise in the tracing procedure, and the requirement\nto overcome sub-optimal data acquisition processes[2], the\nexample of which is in Figure 10.\nFig. 10: Extraction of the outer (green) and inner (red) profiles\nfrom image.\nTo emphasize, the goal is not to increase accuracy of top-\n1 but that of top-K, which matches straight-forward sense.\nBecause This fits with its function as a reference tool for\npottery specialists who would be glad to evaluate a shortlist\nof results as part of the obligatory expert validation but would\nbe disappointed to use a tool where the correct result is often\ncompletely omitted.\n2) Appearance-based Recognition: It is find in experience\nthat the most challenging factor that affected identification was\nvarying illumination. So different white balance, brightness,\nand contrast adjustments are simulated. Each pixel’s brightness\nis multiplied by a random factor to simulate different lighting\nlevel.\nMoreover, the background and ruler varied significantly,\nleading to an inherent bias. The foreground was extracted\nautomatically from the training images using the GrabCut\nalgorithm to avoid this conditioning [18].LIN YUHANG, PENG SHUBO, DECEMBER 2022 6\nD. App Workflow\nArchAIDE also create a mobile application connected to AI\nclassifiers to support archaeologists in recognizing potsherds\nduring excavation and post-excavation analysis, with an easy-\nto-use interface.\nArchaeologists take a picture of a potsherd and send it to\nthe specifically trained classifier, which returns five suggested\nmatches from the comparative collections. Once the correct\ntype is identified, the information is linked to the photographed\nsherd and stored within a database that can be shared online.\nAs shown in Figure 11.\nThe mobile application was also designed for allowing the\nuse in lack of internet connectivity , such as in storehouses or\nremote rural areas. Application provides an area called “my\nsites”, dedicated to registered users where it is possible to store\ninformation about sites and assemblages. When offline, the\napp permits storing new images of potsherds or browsing the\nreference database. The app registers the information locally\nwhen offline and then saves the information into the server\nonline.\nFig. 11: The double workflow for appearance-based and shape-\nbased recognition from an input image to top 5 results.\nIV. S HORTAGE\nDespite its help, ML still has some limitations.\nThe biggest difficulty is the shortage of training data.\nArchaeology is widely digitised, but rarely datafied[3]. ML\nprefers Big Data which is findable, accessible, interoperable,\nand reusable. But nature of archaeology makes it hard to\nproduce huge data, and much already-had data is unusable\ndue to copyright or legislation.[10]\nML relies on its previously created model too much, and\nrequires new data to be applicable to its model. Most ML\nmodels for archaeological data are less reliable than human\nexperts now, because algorithms can’t consider the variation\nand consistencies of data. That is, human experts can quickly\nhandle easy cases, and move extra time to complicate cases,\nbut algorithms handle every sample with same process and\nconsume similar time. This disadvantage may offset ML ad-\nvantages of high calculation speed and scalability benefits.[5]\nThe diversity of archaeological objects makes classification\nmore hard. The archaeological recovered samples may becomefragmented, or be covered by patina and vegetation. These\npoor preservation of samples makes classification hard. More-\nover, rare and unusual objects may be ignored by ML models.\nA look-like “normal” ceramic vessel may has an unusual\nsurface treatment, which will be easily noticed by human,\ncould be classified into normal category by ML models.[5]\nA third reason is the accuracy of evaluation function in ML\nmodel. This sort of bias is of particular concern for archae-\nologists using ML on data associated with Indigenous com-\nmunities. Archaeologists can use surveys based on acultural\nfactors to create models that are stripped of cultural context\nand meaning. But these assumptions are being more and more\nchallenged, especially when surveys focus on behaviors and\noutcomes rooted in cultural value systems.[6][8]\nV.FUTURE\nNowadays, archaeologists utilize AI in many ways, from\ncreating 3D models of historical sights to scanning territories\nwith a laser radar to find ancient graves or from matching\nand assembling pairs of ancient papyrus fragments containing\nmostly unknown scripture to detecting objects in archaeolog-\nical sites using DTMs generated from ALS data. There is no\ndenying that AI becomes more and more popular in the field\nof archaeology and plays an important role in it.\nOne direction for the future development of intelligent\narchaeology is helping archaeologists’ work. For example,\narchaeologists often face such problems as not knowing where\nexactly to dig. They can define the region but not the exact\nplace where an artifact or a grave lies. It’s when the neural\nnetwork comes in line. Instead of looking through millions\nof documents by themselves, archaeologists pass this work to\nneural networks. This technology can sort out information by\nutilizing a specific algorithm. By analyzing images, this system\nmight not only direct archaeologists in their groundworks but\nalso suggest territories that have similar patterns as potential\nobjects for excavations.\nHowever, knowing that many ML systems — especially\ndeep neural networks — are essentially considered black\nboxes. This makes it hard to understand and explain the\nresults given by a model. Because of this, it should be noted\nthat AI does not replace the need for experts in archaeology.\nInstead, AI technology needs the expertise from archaeologists\nto improve itself and to judge the correctness of the results.\nAnother direction is to strengthen the identification of the\nartifacts. In our world, one of the most urgent problems in\narchaeology is the fact that many artifacts are traded on the\ndark web. Although many models trained in this direction\nhave achieved quite good results, they are not yet ready to\nbe applied in practice. Currently, the majority of detection\noperations are performed manually. If an AI can succeed in\nthis direction, it would make an extraordinary contribution to\nthe prevention of art-related illegal activities.\nWe expect that in the future, artificial intelligence technol-\nogy will play an increasing, even irreplaceable role in the field\nof archaeology.LIN YUHANG, PENG SHUBO, DECEMBER 2022 7\n"}, {"role": "assistant", "content": "最终打分：8.5 (范围0-10分)\n1. 结构完整性得分：8.5, 占比20%\n2. 逻辑清晰度得分：8.5, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：8, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "Introduction to Artificial Intelligence         Final Thesis of 2022 Autumn Semester        Intelligent archaeological overview project\n1Abstract\nWith the continuous development of AI technology, \nmore and more industries are interlinked with AI, resulting \nin many fantastic reactions and bringing about huge \nindustry changes. In archaeological technology, the use of \nconvolutional neural network (CNN) algorithms for \narchaeological classification and analysis is necessary due \nto the gradual lack of talent and the high level of \nknowledge required. There are currently some attempts in \ndifferent archaeological fields. Most of the attempts are \nrelatively simple attempts at classification. In the case of \nceramic structure classification, CNN is a highly accurate \naccessible and effective method of classifying ceramic \nstructures that can be applied at a larger scale due to its \nhigh degree of texturization.\n1. Introduction\nIn archaeological activities, the sorting and \nreorganization  of large quantities of ceramic sherds, or the \nextraction of features or the analysis of their composition, \nrequires a great deal of time and qualified expertise, which \ninevitably leads to errors in the sorting process due to \nvarious subjective factors. The introduction of \nconvolutional neural networks as a tool for classifying \nceramic structures is therefore necessary, not to abandon \nmanual classification, but to combine the various expertise \nof different experts to achieve more objective and efficient \nclassification tasks and more accurate results. As CNNs \ncontinue to evolve, more and more learning sets are being \ndeveloped, but they have yet to be exploited in depth in the \nfield of archaeology. Convolutional neural networks, with \ntheir highly discriminative nature for texture, are uniquely \nsuited to classifying the composition of rock flakes.\nIn recent years CNN has been applied experimentally \nin various fields in the direction of archaeology, but all \nchanges are the same, in a nutshell, surface features are \nextracted by scanning and then the extracted features are \nrefined by machine learning methods for classification. The structural iterative specificity of different network \nmodels has different adaptations for processing \narchaeological data of smaller orders of magnitude.\n2.1. An analysis of the context of ancient ceramic \nresearch in archaeology\nThe classification of ancient ceramics or ceramic \nsherds is of great importance in archaeology. Firstly, the \ncarved or patterned surface features allow the local cultural \ncharacteristics of the ceramic's origin to be identified, \nwhich is of great importance for the study of cultural \ndiffusion. Furthermore, the analysis of the composition of \nancient ceramics, combined with the distribution of \nminerals in their place of origin, allows for an analysis of \nthe geological composition of the period, which can also \nshed light on plate movements. In addition, the study of the \nclassification of ceramics from different regions is also \nimportant for the study of the spread and distribution of \nancient trade routes. Because the huge amount of \nclassification work and the need for high accuracy make \nthis a very time-consuming and labor-intensive task. CNNs \nare a very efficient and promising solution to this problem, \nwhich can be solved with the help of other autonomous \nanalysis techniques.\n2.2. The main methods of classifying ceramic \nstructure\nThere are two main methods of classifying ceramics, \none of which is to classify the macroscopic features on the \nsurface of the ceramic, such as carved patterns, texts and \nthe content of paintings. This approach requires ceramics \nand ceramic sherds to be complete, reflecting essentially \nthe full range of features, and in the case of sherds a further \nlayer of classification criteria is required to integrate sherds \ncontaining similar features. This is the most intuitive way \nof classifying ceramics and is less prone to error by chance, \nbut for a time when painted ceramics or carving techniques \nwere not widespread, this intuitive way of classification is \nnot so easy, and it is difficult to analyse the colour of the \nfragments of clay alone to arrive at an efficient form of Application of CNN in classifying the structure of ancient ceramics\nPan Lei Chen ZhaoRui\n12010919 12010906\nMAE CSIntroduction to Artificial Intelligence         Final Thesis of 2022 Autumn Semester        Intelligent archaeological overview project\n2classification.\nAnother type of classification that is now more \ncommonly used is the analysis of micro-composition \ncomponents. This takes the form of thin slicing of \nceramics, cutting them off and using a microscope and \nsuperimposing different polarised light to obtain a two-\ndimensional image, a three-colour map or a greyscale map, \nwhich allows a good analysis of the composition, with \nthree main components: inclusions, clay matrix and gas \ninterstices, in another way, the analysis of the glass phase, \nthe mineral crystalline phase and the gas phase. This is \nessentially the same criterion as for the archaeological \nclassification of rocks, for the reason that ceramics are also \na man-made rock. Once the sections have been obtained, \nthere are two types of experiments: direct textural and \nmodal analyses, and indirect chemical analyses using \ndifferent chemical properties. This classification method \nhas the advantage of sufficient quantitative data and a high \ndegree of objectivity, but is not very efficient, with a large \nnumber of slices and statistics. It also requires the \nextraction of ceramic structures, which can cause a certain \namount of per se damage, especially when the data set is \nnot quite sufficient with a small sample size. Rather, this \nclassification, which is more dependent on the size of the \ndataset and has more objective and clear conditions for \nanalysis, is more suitable for CNN use than the other \nconvenience of 3D modelling that can be scanned for \nanalysis.\n3. Introduction to Convolutional Neural Network \nModels\n3.1 Basic Concepts of Convolutional Neural \nNetworks \n3.1.1 Basic concepts\nConvolutional neural networks (CNNs) are a class of \nfeedforward neural networks that include convolutional \ncomputation and have a deep structure, and are one of the \nrepresentative algorithms of deep learning. Compared to \nother deep learning structures, CNN can give better results \nin image and speech recognition. This model can also be \ntrained using back propagation algorithms. Compared to \nother deep, feed-forward neural networks, CNN require \nfewer parameters to be considered, making them an \nattractive deep learning structure.\nCNN consist of these main types of layers: an input \nlayer, a convolutional layer, a pooling layer, a fully \nconnected layer and an output layer. The input layer is \ncapable of processing multidimensional data and \nnormalising the input data. The convolutional layer \nconsists of three parts: the convolutional kernel, the \nconvolutional layer parameters and the activation function, which works by regularly sweeping through the input \nfeatures, multiplying them by matrix elements and \nsuperimposing deviations within the perceptual field. The \nconvolutional layer parameters include the convolutional \nkernel size, step size and padding, which together \ndetermine the size of the convolutional layer's output \nfeature map and are the hyperparameters of the \nconvolutional neural network. The convolutional layer also \ncontains an excitation function to assist in the \nrepresentation of complex features. After feature extraction \nin the convolutional layer, the output feature map is passed \nto the pooling layer for feature selection and information \nfiltering. The pooling layer contains a pre-defined pooling \nfunction that replaces the results of a single point in the \nfeature map with the feature map statistics of its \nneighbouring regions. The pooling layer selects the pooled \nregions in the same way as the convolutional kernel scans \nthe feature map, controlled by pooling size, step size and \npadding. The fully connected layer in a convolutional \nneural network is equivalent to the implicit layer in a \ntraditional feedforward neural network. The fully-\nconnected layer is located at the end of the implicit layer of \nthe CNN and only passes signals to the other fully-\nconnected layers. The feature map loses its spatial topology \nin the fully connected layer and is expanded into a vector \nand passed through the excitation function.\n3.1.2 The history of CNNs\nCNNs can be traced back to Hubel and Wiesel's 1968 paper \non how the visual cortex of cats and monkeys contains \nneurons that respond individually to small areas of the \nvisual field, and that if the eye does not move, the area of \nvisual space where a visual stimulus affects a single neuron \nis called its Receptive Field. Neighbouring cells have \nsimilar and overlapping receptive fields. The size and \nlocation of the receptive fields vary systematically between \ncortices, forming a complete visuospatial map. This \nresearch laid the foundation for local perception in CNNs.\nIn 1980, the neocognitron was proposed, marking the \nfirst initial CNN and the first application of the receptive \nfield concept in the field of artificial neural networks. The \nneocognitron decomposes a visual pattern into many sub-\npatterns (features), which then enter a hierarchically \nprogressively connected feature plane for processing.\n  In 1988, the Shift-invariant neural network (SNN) \nwas proposed, taking the CNN's capabilities one step \nfurther, enabling it to accomplish recognition even when \nobjects are displaced or slightly deformed.\nThe feed-forward architecture of the CNN is extended \nby lateral and feedback connections in the Neural \nabstraction pyramid. The resulting recurrent convolutional \nnetwork allows flexible incorporation of situational \ninformation to iteratively resolve local ambiguities. In \ncontrast to previous models, the highest resolution image Introduction to Artificial Intelligence         Final Thesis of 2022 Autumn Semester        Intelligent archaeological overview project\n3output is produced.\nAfter the appearance of a GPU implementation of \nCNNs in 2005, which marked a more efficient way to \nimplement CNNs, CNNs stood out for their high accuracy \nin the ImageNet competition in 2012, and deep learning \nwas officially brought into the limelight. \nNext we will describe several well-known CNN \nmodels.\n3.2 AlexNet\nAlexNet was designed by Hinton and his student Alex \nKrizhevsky (2012), and the model won the ImageNet \ncompetition in 2012. This model is the basis for many \nbetter and deeper neural networks.\nFigure 3.1 Structure of AlexNet\nAlexNet contains several relatively new technical \npoints: \n--Successfully using ReLU as the activation function \nof CNN and verifying its effectiveness over Sigmoid in \ndeeper networks, successfully solving the gradient \ndispersion problem of Sigmoid when the network is \ndeeper.\n--Dropout was used to randomly ignore some neurons \nduring training to avoid overfitting the model.\n--Maximum pooling using overlap in CNNs.\n--A LRN layer is proposed to create a competition \nmechanism for the activity of local neurons, making the \nvalues in which the response is relatively large and \nsuppressing other neurons with smaller feedback, \nenhancing the generalisation ability of the model.\n--Using CUDA to accelerate the training of deep \nconvolutional networks, the powerful parallel computing \ncapability of the GPU is utilised to handle the large number \nof matrix operations during neural network training.Figure 3.2 Multi-gpu parallel training of AlexNet\n--The data enhancement, randomly intercepting a \n224*224 sized region (and horizontally flipped mirror) \nfrom the original 256*256 image, corresponds to an \nincrease of 2*(256-224)^2=2048 times the amount of data. \nAlso, the AlexNet paper mentions that it will do PCA \n(Principal Component Analysis) process on the RGB data \nof the image and do a Gaussian perturbation of the \nprincipal component with a standard deviation of 0.1 to add \nsome noise, and this Trick can bring the error rate down by \nanother 1%.\n3.3 VGGNet\nVGGNet was developed in 2014 by researchers from \nthe University of Oxford's Computer Vision Group and \nGoogle DeepMind, and achieved second place in the \nclassification event and first place in the localization event \nof the ILSVRC 2014 competition. VGGNet can be seen as \na deepened version of AlexNet, but in a simpler form.\n3.3.1 Structure\nVGG consists of 5 convolutional layers, 3 fully-\nconnected layers, and 1 softmax output layer. The layers \nare separated from each other using a maxpool \n(maximisation pool), and the activation units of all hidden \nlayers use the ReLU function. In the original paper, the \nauthors designed six network structures, A, A-LRN, B, C, \nD and E, depending on the number of different sub-layers \nof the convolutional layers.\nFigure 3.3 Structure of VGGNet\nIntroduction to Artificial Intelligence         Final Thesis of 2022 Autumn Semester        Intelligent archaeological overview project\n4The six networks are similar in structure, all \nconsisting of five convolutional layers and three fully \nconnected layers, the difference being that each \nconvolutional layer has a different number of sub-layers, \nincreasing from A to E. The total network depth ranges \nfrom 11 to 19 layers. The convolutional layer parameters \nin the table are expressed as \"conv (receptive field size) - \nnumber of channels\", e.g. con3-64 means a 3x3 \nconvolutional kernel with 64 channels; maximum pooling \nis expressed as maxpool, with layers separated by \nmaxpool; fully connected layers are expressed as \"FC - \nnumber of neurons\", e.g. FC-4096 indicates a fully \nconnected layer containing 4096 neurons; and finally a \nsoftmax layer.\nIn the form, D denotes the well-known VGG16 and E \ndenotes the well-known VGG19. For example, the \nstructure of VGG16 is shown in the following figure:\nFigure 3.4 structure of VGG16\n3.3.2 Model advantages\nCompared to previous CNN models, VGGNet has the \nfollowing advantages.\n--Simple structure: Although VGGNet has more \nlayers, its overall structure is not complex. As mentioned \nabove, VGG consists of five convolutional layers (each \nwith a different number of sub-layers), three fully-\nconnected layers, a SoftMax output layer, and a \nmaxpooling (maximization pool) separating the layers \nfrom each other, and a ReLU function for the activation \nunits of all the hidden layers.\n--Small convolutional kernels: Instead of using the \nlarger convolutional kernel size in AlexNet (e.g. 7x7), \nVGG achieves the same performance by reducing the size \nof the convolutional kernels (3x3) and increasing the \nnumber of convolutional sub-layers (VGG: from 1 to 4 \nconvolutional sub-layers, AlexNet: 1 sub-layer)\n--Smaller pooling kernels: compared to AlexNet's 3x3 \npooling kernels, VGG uses all 2x2 pooling kernels.\n--More channels, wider features: each channel \nrepresents a Feature Map, more channels means richer \nimage features. 64 channels in the first layer of the VGG network, doubled in each subsequent layer up to 512 \nchannels, the increased number of channels allows more \ninformation to be extracted.\n--Deeper layers: By using successive small 3x3 \nconvolutional kernels instead of large ones, the network is \ndeeper and fills in the edges, the convolution process does \nnot reduce the image size. Also, only small 2x2 pooling \nunits are used, reducing the size of the image.\n--Fully connected layer to convolutional layer: this is \na tested method proposed by the authors that enables the \nnetwork model to accept image sizes of arbitrary size.\n--Parameter stability: The depth of the six network \nstructures, A, A-LRN, B, C, D and E, has increased from \n11 to 19 layers, but the number of parameters has not \nchanged much, due to the fact that basically small \nconvolutional kernels are used and the parameters are \nmainly concentrated in the fully connected layer.\n3.4 ResNet\nResNet was designed by Microsoft Research in 2015 \nand won the 1st places on the tasks of ImageNet detection, \nImageNet localization, COCO detection, and COCO \nsegmentation in ILSVRC & COCO 2015 competitions.\nEmpirically, the depth of the network is critical to the \nperformance of the model, and when the number of layers \nis increased, the network can perform more complex \nfeature pattern extraction, so theoretically better results can \nbe achieved when the model is deeper. However, the \nexperiments revealed a Degradation problem in the deep \nnetwork: when the depth of the network increases, the \naccuracy of the network saturates and even decreases. \nResNet solves the Degradation problem by learning the \nresiduals for a stacked layer structure (several layers are \nstacked together) when the input is x. The learned features \nare denoted as H(x), now We want it to learn the residual \nF(x) = H(x) - x, so that the original learned feature is \nactually F(x) + x. This is because residual learning is easier \nthan direct learning of the original features. When the \nresiduals are 0, the stacking layer is only doing constant \nmapping at this point, at least the network performance \ndoes not degrade, and in fact the residuals will not be 0. \nThis will also allow the stacking layer to learn new features \non top of the input features and thus have better \nperformance. The structure of residual learning is shown in \nFigure 3.5. This is somewhat similar to a \"short circuit\" in \na circuit, so it is a shortcut connection. \nIntroduction to Artificial Intelligence         Final Thesis of 2022 Autumn Semester        Intelligent archaeological overview project\n5Figure 3.5 Residual learning: a building block\nThe ResNet network is a reference to the VGG19 \nnetwork, modified from it and incorporating a residual unit \nthrough a short-circuiting mechanism, as shown in Figure \n3.6. \nAn important design principle of ResNet is that the \nnumber of feature maps is doubled when the feature map \nsize is halved, which maintains the complexity of the \nnetwork layers. As can be seen in Figure 3.6, ResNet adds \na short-circuiting mechanism between every two layers \ncompared to a normal network, which creates residual \nlearning, where the dashed line indicates that the number \nof FEATURE MAPs has changed. The 34-layer ResNet, \nshown in Figure 3.6, also allows for the construction of \ndeeper networks as shown in Table 3.1. As can be seen \nfrom the table, for the 18-layer and 34-layer ResNet, it \nperforms residual learning between two layers, and when \nthe network is deeper, it performs residual learning \nbetween three layers, with three convolutional kernels of \n1x1, 3x3 and 1x1, respectively. The number of feature \nmaps in the implicit layer is relatively small and is 1/4 of \nthe number of output feature maps.\nFigure 3.6 Example network architectures for \nImageNet.\nIntroduction to Artificial Intelligence         Final Thesis of 2022 Autumn Semester        Intelligent archaeological overview project\n6Table 3.1. Architectures for ImageNet.\nResNet uses two types of residual units, as shown in \nFigure 3.7. The left figure corresponds to a shallow \nnetwork, while the right figure corresponds to a deep \nnetwork. For short-circuit connections, when the input and \noutput dimensions are the same, the input can be added \ndirectly to the output. But when the dimensions do not \ncoincide (corresponding to a doubling of the dimensions), \nthis cannot be added directly. There are two strategies: (1) \nusing zero-padding to increase the dimensionality, which \ngenerally requires a downs amp first, and can be done by \npooling with stride=2, which does not increase the \nparameters; (2) using a new mapping (projection shortcut), \nwhich generally uses a 1x1 convolution, which increases \nthe parameters and also increases the computational \nvolume. Short-circuit connections can, of course, use \nprojection shortcut in addition to the direct use of constant \nmapping.\nFigure 3.7 Example network architectures for \nImageNet.\n4. Feasibility analysis of convolutional neural \nnetworks applied to ceramic classification\nAs artificial intelligence continues to develop, \nadvances in convolutional neural networks make it \npossible to use them on an increasingly broad level. In the \narchaeological direction, the large sample size and high \nlevel of knowledge required for feature recognition make \nsuch specialists in short supply, which makes the use of \nconvolutional neural networks a strong necessity. \nCurrently, there are already experts in different \narchaeological fields who have used the current popular network models for their applications and obtained good \nresults.\n4.1. Accuracy and confidence analysis of simple \nmodels that have already been applied\nBogacz and Mara (2020) used the ResNet50 network \nmodel in CNN when classifying cuneiform texts from \ndifferent periods. A 3d model was first modelled for the \ndifferent periods of cuneiform writing, a point cloud was \nformed and then machine learning was used and compared \nto accurate classification and an accuracy of 84% was \nobtained. Although the sample set for machine learning is \nnot very large and the CNN is not very sensitive to \nmacroscopic shapes, especially sharp corner turns, which \ncan make the point cloud distribution not very accurate, a \nhigh accuracy rate was still achieved.This attempt also has \nsignificant implications for ceramic archaeology, \nparticularly in terms of the macroscopic approach to \nclassification, and the same applies to the carved patterns \nand painted surfaces of ceramics.\nFigure 4.1 Example of cuneiform writing\nIn addition to this, there are many applications on a \nlarger macro level, such as geographic surveys, for \nexample, the location of graves and the determination of \nsite zones.  \nPawlowicz and Downum (2021) uses image \nrecognition technology, developed using CNN, to identify \nthe shape of various ceramic surface patterns. The \nresearchers classified Tusayan white pottery from \nnortheastern Arizona, which has a distinctive black and \nwhite patterned surface, and with the recognition system \nin operation, obtained an accuracy rate that was \nessentially the same as, and in some cases better than, that \nof an expert human discriminator.\nIntroduction to Artificial Intelligence         Final Thesis of 2022 Autumn Semester        Intelligent archaeological overview project\n7Figure 4.2 Tusayan white ceramic\nThe purpose of the project is essentially the same as \nthe one above, the ARCADIA Project (Chetouani et al. \n2018; Chetouani et al. 2020). Designed to enhance the \narchaeological heritage of the ceramic sherds extracted in \nSaran (France). These sherds date back to the heyday of \nthe Middle Ages. They propose to use a convolutional \nneural network (CNN) model to automatically classify \nthese ceramic shards. The ultimate goal is to form clusters \nof shards that result in a map representing the movement \nof potters. In order to reduce the workload caused by \nmodelling, the surface texture of the ceramics is printed \ndirectly on white paper by means of ink stamping, and \nthen scanned to extract the corresponding texture. As this \nmethod is not directly applied to relic antique ceramics, \ndamage to the ceramic surface does not need to be \nconsidered.\nFigure 4.3 Image processing steps\nFor the subsequent classification of simple ceramic \ntexture shapes, a variety of current CNN network models \nwere used for the experiments (ResNet18, VGG11, Alex \nNet) and the best performing network model was then \nselected and evaluated under different configurations. The \nresults obtained show the relevance of the method and the \nbest results are obtained with the ResNet18+SVM \nconfiguration.\nThe project above does not involve a specific project \napplication. Pawlowicz and Downum used deep learning \nto quickly sort sherds according to texture in order to better \nclassify Tusayan white porcelain.Figure 4.4 Texture classification of Tusayan white \nceramics\nIn their study, Pawlowicz and Downum showed that, \nif trained properly, deep learning models can assign types \nto digital images of decorated cattle with an accuracy \ncomparable to, and sometimes higher than, that of four \nexpert contemporary archaeologists. Pawlowicz trained the \nAI model used for the study in just a few hours on his PC, \nwhich was equipped with an NVIDIA GPU running a pair \nof common convolutional neural network models, VGG16 \nand ResNet50. Far from competing with existing \narchaeologists, the model could even be a valuable tool for \ntraining new archaeologists.\nResults from existing applications show that CNN \nholds great promise in terms of its ability to extract \nrecognizable surface features, and that accuracy rates can \nbe high.\n4.2. Feasibility analysis of ceramic micro \ncomposition analysis and classification using CNN \ntechniques\nIn the previous section of the technical background \nexamination, most of the techniques used the simplest \nCNN models to classify the macroscopic structure of \nceramics, and the application of CNN is not very mature \nfor the direction of component analysis of microscopic \nslices. However, scientists have been using artificial \nneuronal network (ANN) techniques to classify ceramic \nflakes as early as ten years ago.\nAprile, Castellano, and Eramo (2014) constructed \nthree network structures to separately identify the three \ndifferent phases in the rock thin section. They approach \ncombines image analysis techniques with artificial neural \nIntroduction to Artificial Intelligence         Final Thesis of 2022 Autumn Semester        Intelligent archaeological overview project\n8networks to automatically classify mineral inclusions and \npores in archaeological pottery sherds using optical digital \nimages. In particular, the automatic identification of quartz, \ncalcareous aggregates and secondary porosity was \nconsidered. A collection of planar and cross-polarized light \nimages obtained by means of a digital camera connected to \nan optical microscope in transmitted light was used. \nIt is shown that reliable features obtained by \nconsidering the optical properties of the class of interest \nand the specific neural network structure can provide good \nautomatic classification.\nFigure 4.5 Flow chart for classifying ceramic slices \nusing ANN\nThe insights from the above studies have led to the \napplication of CNNs in the field of slice structure \nclassification as well. Lyons, M. (2021) have built a deep \nlearning model to automatically identify and classify \nceramic fabrics, speeding up the classification process and \nreducing the need for experts. Thin-section images of rocks \nin cross-polarized light from the Coconut Tree period \n(1000-1525 AD) archaeological site in Guadalupe, on the \nnortheast coast of Honduras, were classified using two \ndivisions trained by VGG19 and ResNet50. Although the \nsample size and learning set are not large, the result of 93+% \nclassification accuracy in four tests demonstrates an \nefficient classification method, and although this use case \nis narrowly defined, it serves as a proof of concept that \nsuggests the method should be broadly applicable to any \nceramic assemblage of definable ceramic fabrics.\nFigure 4.6 Examples of the five ceramic fabric types \nanalyzed.  The unique sensitivity of convolutional neural networks \nto texture discrimination makes them an excellent \npredisposition for the analysis of ceramic flake \ncompositions. Previous research has applied different \nnetwork models not only for microscopic classification, \nbut also for macroscopic characterization of ceramic \nfragments, all with high accuracy, in a way that reduces \nmisclassification due to subjectivity, while allowing for \nsimultaneous multi-threading to improve efficiency and \nsolve the problem of insufficient scholarly reserves. In \nsummary, CNNs have a strong need and a promising \nfuture in the direction of ceramic classification. \n4.3.  Limitations and defects\nFor many archaeological applications, where ML is \nan aid to more detailed work, such analyses may be \nsufficient. However, they may not be as reliable as \ntraditional expert methods because they cannot yet handle \nthe range of variability and inconsistency in archaeological \ndata.\nOften the biggest obstacle to constructing good ML \nmodels is that they work best when they are built on large \ndatabases of information, such as thousands of catalogued \nimages or material from reliable sources, which can be \ndifficult to achieve, particularly in terms of archaeological \nbudgets and the diversity of data that may be available. \nPoor preservation of archaeological recovery samples \nmakes the task more difficult as fragmentation and surface \ncondition (including, for example, erosion, patina and \nvegetation cover) can affect the success of identification. \nExperts can often identify material that is difficult for ML \nmodels trained on ideal collections to handle.\nAnother aspect of ML is that models are very much a \nproduct of the data on which they are constructed. As such, \nthese models tend to be classified according to the \ncategories they know, which makes them (at least) \nsusceptible to two main forms of bias.\nThe first involves focusing a combination into one of \nthe previously identified categories. This means that rare \nand unusual objects can easily be categorized as more \ncommon types and ignored. For example, a ceramic vessel \nsimilar in shape to one of the models may look 'normal', \nbut may have an unusual surface treatment that an \narchaeologist would immediately notice.\nThe second form of bias, and perhaps the most \ncommon, is that the model does not fully incorporate the \nvariability of the features being categorized; ML analysis \ncan easily lose the 'forest for the trees' because the data \nused to train the model is often stripped of contextual \ninformation (especially in the case of images) or \nmanipulated on a limited number of pre-selected variables, \nwhich may not include sufficient variables to allow for the \nclassification of the model. These variables may not \ninclude enough information to distinguish important (i.e. \narchaeologically relevant) categories.\nIntroduction to Artificial Intelligence         Final Thesis of 2022 Autumn Semester        Intelligent archaeological overview project\n9ML techniques do have methods for checking \n'performance', but these methods still rely on internal \nmathematical metrics and require the attention of \narchaeological research to ensure that they provide good \nresults. Outside of archaeology, difficulties with ML have \nbeen repeatedly encountered, including exacerbating racial \nand gender bias in commercial settings (Gebru 2020).\n5. Conclusion\nFrom the above summarized study, we found that \nCNNs strongly tend to recognize texture rather than shape, \nand that ceramic fabrics are highly textural in nature. This \nis precisely the reason why CNN models can fit with the \ntopic of classifying the structure of ancient ceramics. In \naddition, several studies have shown that CNNs are highly \naccurate in classifying ceramic structures, a conclusion that \nprovides a piece of mind for researchers in the industry \nwho have questioned the rigour of CNN technology.\nAs deep learning techniques continue to evolve, the \naccuracy and efficiency of neural network models are \nbeing optimized and improved. Computer-related \npractitioners are happy to see this happen, and there is a \ngrowing willingness in other industries like archaeology to \nuse such techniques to aid their scientific work. We are \nseeing newer and more optimized convolutional neural \nnetwork models being used in many of the latest \narchaeological research projects. The use of computers and \nbig data is clearly more efficient than traditional manual \nexcavation and identification. We expect archaeological \nresearch to make even greater breakthroughs with the help \nof technology. \n"}, {"role": "assistant", "content": "最终打分：8.5 (范围0-10分)\n1. 结构完整性得分：9.5, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：8, 占比10%\n6. 课程知识掌握度得分：10, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "16 组  智能图书馆  技术报告  \n组长：谢绍康\n组员：李佳纯，郑微言，龚明道，冯柏钧\n \ndemo 运行问题  \n我们在一开始拿到项目的前后端代码的时候，尝试着去将他运行起来，但后来（接下来的两个多\n月）发现它一直会有一个 401Failure 的认证错误信息。\n在询问助教和那位写 deme 程序的后端的同学无果后，我们开始尝试自己去解决这个问题。\n经历了长达二十小时的 rust 学习以及 debug 中，终于定位到了问题所在 —— cookie 的添加和读取出\n现了问题，在前面添加进去的 cookie 到程序运行的后面却无法读取。在尝试解决无果后，我们决定放弃 cookie ，采用另一种 token 的存储方法 —— 数据库。我们将每次\n登入的用户信息传入数据库中，在需要读取时，再从数据库中读取，这样就很好地解决了其中地\ncookie 造成的 401 验证失败的问题。\n \n数据格式的匹配  \n根据后端文件中各个 model.rs 内的内容来确定数据库的数据形式，而后进行建表、存储、更新等。\n \n数据每日更新  \n我们针对每一张表用到的数据，都写了初始化脚本以及每日更新的脚本。目前每日更新是通过 linux shell 的 bash 脚本来运行各个 python 的更新脚本，并将其挂载在个人的\ninux 服务器上运用  crontab 定时任务在每日中午 12 点更新上一天的数据，并将信息打印在 log 文件\n中。  \n \n用户画像  \n图书馆画像标准更新的工作的主要难点在于参数和模型的选取。我们主要数据来源有 3 处：图书馆\n出入馆数据，讨论间预定数据和图书馆书籍借阅数据。经过讨论，我们抓取了 “ 出入馆天数，在图\n书馆内的总时长，预订讨论间次数，使用讨论间总时长，借阅书籍总数，借阅各类书籍数 ” 作为学\n生的特征。\n在数据预处理部分，由于部分同学没有图书借阅或讨论间的数据，对应数据项为 None ，所以需要\n先将所有的 None 改为 0 。另外，由于借阅图书的人数占比非常小，直接聚类效果非常不好，为了适应大部分同学的情况，在数据预处理部分，还增加了对数据取 log 的操作。\n该分类问题为无监督分类，我们首先使用了最经典的 KMeans 模型。调用 sklearn 库中的 KMeans 模\n型对数据进行聚类，根据需求，先后指定 k 等于 2 和 k 等于 3 来训练模型并聚类，并且指定了随机种子\n0 ，保证在不改数据和任何参数的情况下，模型多次聚类的结果是前后一致的。另外，还调用了\nGaussianMixture(GMM) 模型进行了和 KMeans 相似的操作。\n然后，通过轮廓分析，探究分类效果，并将其可视化。考虑到部分特征中可能包含重复信息，部分\n特征噪声较大，我们通过删减部分特征重新训练分类模型，并同样通过轮廓分析对比前后分类结果\n质量。后考虑到部分特征相关性不大，将特征分为两类，分别聚类，并且将聚类结果相应的提升至\n二维。（ “ 怎么样 ” 的 “ 某某者 ” ）\n最后，对比了十余种分类方法，选择通过 log 后的使用讨论间累计时长，在图书馆累计时长与借阅\n书籍总数作为特征，使用 KMeans 进行分类，通过两轮聚类（图书馆，讨论间特征一轮，  借阅数据\n一轮），划分出 2*3 一共 6 类画像。"}, {"role": "assistant", "content": "最终打分：6.8 (范围0-10分)\n1. 结构完整性得分：6, 占比20%\n2. 逻辑清晰度得分：7, 占比20%\n3. 语言连贯性得分：7, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：3, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "The application of deep learning in archaeology\n12011528 Yiyan Zhang\n12011526 Xun Sun\nDecember 30, 2022\nAbstract\nThe long history of human civilization has resulted in a magnificent culture. In or-\nder to study the living customs and cultural rituals of previous generations, scholars\nhave explored human sites and studied the antiquities left behind by ancient humans,\nand other important ways to know the way of life of previous generations and under-\nstand the culture and customs of that era. However, with the passage of time, many\nartifacts are weathered and it is difficult for scholars to analyze and compare them in\nbulk directly and quickly. In contrast, machine learning assists scholars to analyze an-\ntiquities, which seems more efficient and accurate in comparison, and mature machine\nlearning models will become a powerful tool in archaeology to facilitate the archaeological\nwork. In addition, analyzing and refining the characteristics of artifacts with the help\nof machine learning will help experts to discover the information carried by artifacts,\nperfect the history, and even identify artifacts that cannot be identified by relying on\nexpert knowledge now[1]. And zooming in on our perspective, in addition to helping\nwith artifact analysis and identification, deep learning can also assist with archaeolog-\nical excavation work and the study of sites. For example, by analyzing map data, can\nhelp excavate archaeological sites, find sites, and reconstruct the historical landscape of\nsites. In general, the widespread use of deep learning in archaeology today has brought\na new revolution in the field of archaeology, and this paper will briefly discuss the initial\napplications of deep learning models in archaeology.\nKey words: Archaeology; Deep learning; Heritage Restoration; Site Detection; Site\nReconstruction\n1 Introduction\n1.1 Analysis of the difficulties encountered in archaeology\nAs people become more and more interested in the study of history, archaeological work is also in\nfull swing.However, the deeper the archaeological work and related research, the more difficulties the\nresearchers encountered.\n1Although historical researchers have a wide range of knowledge and rich experience, few of them can\nmaster all aspects of archaeological knowledge.What a pity it is when new antiquities are unearthed\nand no one is able to identify them.Many unearthed cultural relics cannot remain unchanged for a\nlong time. If there is no correct interpretation and identification by experts and scholars within a\ncertain period of time, the difficulty of this rare cultural treasure will disappear, and we will lose an\nopportunity to understand the ancient culture.\nIn addition, another important issue for researchers to consider when conducting archaeology is\nto determine how to excavate ancient sites.After an ancient site is discovered, researchers survey the\nsurrounding terrain and discuss how to plan for excavation and development of the site. People want\nto do archaeological research with as little damage as possible.This is a difficult thing to do. It has\nbeen a long time since the ancient ruins were built until they were discovered today. The present\ntopography has deviated from the time when they were built, and any careless excavation may cause\nirreparable damage to the ancient relics.How to do archaeological work on the premise of protecting\nthe ancient relics and sites as much as possible is a thorny problem that archaeologists must face.\n1.2 The development of deep learning in archaeology\nArchaeology is a new and challenging field for deep learning.Deep learning first appeared in the 1940s.\nSome archaeologists have applied deep learning to archaeological work not long after the advent of\ndeep learning technology.\nIn 2014, a research team from China Remote Sensing Earth Institute spent three years in Xinjiang\nto conduct archaeological work on the ancient Great Wall in Xinjiang. Using remote sensing and deep\nlearning techniques to obtain weak information about ancient sites, researchers have ”restored” the\nancient Great Wall in Xinjiang, changing the long-held understanding that there is no Great Wall in\nXinjiang, and inspiring the archaeological community.\nIn recent years, researchers from four institutions, including Pennsylvania State University in the\nUnited States and the University of Sydney in Australia, have published research showing the potential\nof deep learning algorithms for discovering ancient human remains. The results show that by using\na deep learning algorithm, researchers have found several 3 000-5 000 years ago Native American\nsurvival relics - Shell Ring, the discovery of this relic will help to further understand the social and\neconomic structure of Native Americans(Fig.1). It is worth mentioning that using deep learning to\ndetect historic sites will not have a destructive effect on historic sites. It can accurately explore the\nhistoric site without causing spatial structural damage.\n2Figure 1: Shell Ring\n2 Applications of deep learning models to archaeological pro-\ncesses\nDuring the process of archaeological excavation, image information is necessary for the experts. There-\nfore, there are a lot of CNN’s applications in archaeological processes. And now they can be divided\ninto two categories. One is doing the detection and analysis tasks when the people are excavating a\nsite, and the other is finding the potential sites by analyzing the high-precision scanning pictures, like\nsatellite pictures. In the following sections, I’ll discuss each of these two types of applications of CNN\nin archaeology.\n2.1 CNN in internal structure scanning\nBefore excavations begin at an archaeological site, archaeologists need to explore the site’s internal\nstructure and make a plan according to the analysis result. In past years, people have used visualization\ntechniques with airborne laser scanning (ALS) technology to explore the internal structure[2]. However,\nthe representation of the characteristics of archaeological structures (e.g. size, shape, orientation,\nlandscape context, topographic position) varies greatly among VTs, and the experts can’t get very\ncomprehensive information, which may influence the identification and characterization decisions[3].\nBut then The use of CNN enables a new breakthrough in exploratory imaging tasks. CNN can deal\nwith complex images by dealing input data with several convolution layers and pooling layers. And\nafter CNN processing, archaeologists can get a more accurate and comprehensive scan of the interior\nof the site (Fig.2).\n3Figure 2: an example model of CNN. Data source: [3]\nBy applying different kernel filters to input images, the filters can intensify or weaken certain\nstructures (edge information) in the training images[4]. Therefore, Using CNN can clear the details in\nscanning data and effectively remove noise interference in raw data, which means archaeologists can get\ninformation from the images more easily. With more comprehensive and clear scans, archaeologists can\ndetermine the location of artifacts at a site in advance. And prepare in advance during the excavation\nprocess, which can reduce the possibility of cultural relics being damaged during the excavation process.\n2.2 CNN in spatial relation analysis for a group of sites\nIn archaeology, some experts will want to understand historical contexts and landscapes. For example,\nto explore where are farms or settlements located, where is agricultural land located, where are the\ndeceased buried, and how is the unsettled environment used. Up to now, only in some cases, researchers\ncould verify that burial mounds were located near and directly related to pathways[4]. The difficulty\nis to find proof of whether routes were already used when nearby burial mounds were built. In general,\npathways are difficult to date.\nAnd CNN can’t solve the problems directly, but it can help the experts in spatial relations between\nthe sites. Give the trained CNN model an aerial view, and view all sites in the image as a point. And\nthen make a circle for every point, and it means there are relics discovered in this range of circles. And\nthen combining the information of relics, like the year of manufacture, the model can tell you which\nsites may be in the same generation and which sites can be a cluster. Although the result of CNN\noutput is not explainable, experts can refer to the model’s spatial relations in the analysis process.\nThe experts can easily remove some wrong sites from their research and combine and analyze different\nsites’ information.\n42.3 CNN in the automated detection of archaeological sites[5]\nCNN’s powerful image feature processing ability can be used to detect archaeological sites by processing\nhigh-precision satellite images. The research (Bonhage et al., 2021) on detecting RCH sites shows the\npossibility of finding sites by satellite pictures. And Relict charcoal hearths (RCHs) are anthropogenic\ngeomorphic features with an average diameter of 12 m found in many forests of Central Europe and\nin the eastern USA wherever pre-coal iron production took place or other industries demanded the\nproduction of charcoal1. They use the augmented image data to train a CNN model and get the result\non the test set (part of result, Fig.3)\nFigure 3: RCH detection by CNN. Data resource: [5]\nIn figure 3, the yellow rectangles are the position of RCHs given by the CNN model and the white\npoints are the real RCHs. And we can see the result has high accuracy in figure 3, which is part of\nthe test result.\nAccording to Bonhage’s report, the final result has high recall and high precision. It means the\nmodel has great accuracy and the result is reliable. Although the research just verified the ability of\nCNN in searching for potential RCHs, all archaeological sites are similar to RCH for CNN. We just\nneed to use different characteristics to train different models to automate detecting archaeological sites\nwith different characteristics. Moreover, we can finally gather the data and train a multiple-objects\ndetection CNN model. Then we can efficiently detect the potential archaeological sites and excavate\nthem.\n3 Simulation of antiquities identification by deep learning model\n3.1 Transfer learning on automatic recognition and classification of stone\nimage\nTransfer learning is a machine learning method. It uses the similarity between data, tasks, or models to\napply the learned or trained models in the old fields to the new fields.Rock and jade are very important\n1definition comes from www.sciencedirect.com/science/article/abs/pii/S034181622030446X\n5parts of archaeological work, and they can provide important information to researchers.But how to\nidentify and classify a large number of jade or ore is a difficult problem for archaeologists.If we apply\nin-depth learning to jade identification, it may be possible to solve this problem.\nFigure 4: Inception-v3\n[6]\nResearchers use Google convolutional neural network Inception-v3 module. As a pre training model,\nthe original dataset contains 1.2 million images, divided into more than 1000 Categories.Inception v3\nmodel can extract effective information from images.First, input and process a rock image in the\nfeature extraction model, and the feature extraction model is the convolution layer and pooling layer\nin the Inception v3 model; The convolution layer and pooling layer in the model are used to calculate\nthe image characteristics of the rock image.The structure system of the whole network starts from the\ninput end. First, three roll up layers are set to connect one pooling layer; Set 2 more roll up layers,\nconnect 1 pooling layer, and finally connect 11 hybrid layers.\nThe rock image samples used in the experiment are collected by different means such as photos,\nrock database and network search. Granite, phyllite and breccia are mainly selected for testing,\nidentification and analysis. The rock type is mainly composed of three kinds of images: laboratory\nrock specimen, on-site rock specimen and on-site large scale rock. In general, granite images are mostly\nof granular texture, phyllite images show phyllitic structure, and breccia images show porphyritic\nstructure.\nIn general, from the classification and recognition results, there are two images of granite with\npoor classification effect, one image of breccia with poor classification effect, and phyllite with good\nclassification effect. The reason for poor classification effect may be that the training set is limited,\nand there are few or no rock images similar to the test set, leading to the failure to extract the rock\nfeatures in the image.\nBased on Inception-v3’s rock image depth learning migration model, the effective recognition of\ngranite, phyllite and breccia is realized, and the recognition probability can reach more than 0.8, and\nsome results can even reach more than 0.95; The model extracts features by searching image pixels\nwithout manual operation, which reduces the influence of subjective factors. Moreover, the training\nprocess has low requirements on the size, imaging distance and light intensity of rock images. In\naddition, by adding the cut rock image to the training set for re training, it shows that continuous\n6learning of rich training sets is an important factor affecting the model recognition ability.\n3.2 Deep learning in authentication of ancient paintings\nTraditional Chinese painting is not only a creative form of ancient painting art, but also a form of\nexpression of traditional Chinese culture. The identification of traditional Chinese painting is conducive\nto the appreciation and protection of traditional Chinese culture.But there are two problems[7].The\nfirst is that in the Internet era, digital traditional Chinese painting has become popular due to its\nadvantages of easy preservation, fast retrieval, etc. However, in the authenticity verification task,\nexperts cannot quickly locate the category of painters when facing the unsigned digital traditional\nChinese painting images of different styles, which affects the efficiency of verification.\nThe second is that with the development of the times, the form of painting forgery is no longer just\nthe traditional methods of imitation, copying, and painting. The use of a new generation of information\ntechnology to imitate the artist’s style makes it difficult to identify the authenticity of the generated\npaintings. However, the traditional identification methods, which mainly rely on expert experience,\nare subjective and vulnerable to interference, affecting the identification accuracy. Therefore, in the\nidentification of digital traditional Chinese painting, an objective and quantifiable reference index is\nparticularly necessary.\nIn the authenticity identification task, experts cannot quickly locate the artist category in the face of\nunsigned digital traditional Chinese painting images of different styles, which affects the identification\nefficiency. Therefore, the researchers used the traditional Chinese painting painters only classification\nalgorithm based on multi branch attention mechanism network.\nFirst, the image is divided into blocks, and the image contrast is calculated to remove the code\nblocks with low contrast, that is, less feature information; Secondly, according to the characteristics\nof traditional Chinese painting, it uses multi-dimensional features such as strokes, textures, colors,\netc. to intelligently classify painters, and uses edge detection, local binary mode, and color moment\ncalculation to completely pre generate features[8].Then, a multi branch attention mechanism network\nis designed to achieve the fusion of pre generated stroke features, texture features and color features to\nachieve accurate classification. Finally, the proposed algorithm is proved to be effective by parameter\nsetting experiments and comparison experiments.\n4 The comparison between intelligent archaeology and tradi-\ntional archaeology\nCompared with traditional archaeology, intelligent archaeology uses deep learning technology. And\ndeep learning can comprehensively analyze the hidden information under the complex data compared\nwith the people. In traditional archaeology, the experts’ ability is limited and they can’t conclude\nand analyze the data of all aspects. So some potential information always can’t be found and some\n7conclusions may not be as correct as they think. However, the addition of deep learning models\nallows for more sophisticated curation by subject experts who would be relieved of the burden of rote\nprocessing[9]. And In the intelligent archaeology generation, experts can use more powerful tools to\ndo the research. For example, multi-label classification, multi-task classification, unsupervised image\nclustering, and explainability.\nThe replacement of traditional archaeological methods by intelligent archaeology is also an in-\nevitable change brought about by the changes in The Times. In the digital generation, more and more\ndetection technologies are created so that archaeologists also have more methods to excavate the sites\nand do some research. However, the tendency also means a more large number of data collected and\nanalyzed. Traditional archaeology doesn’t have the ability to deal with this order of magnitude of\ndata. Therefore, as new detection techniques are applied, new analysis methods, like using CNN, have\nto follow.\nAlthough deep learning models are so powerful and convenient, traditional archaeological methods\nare still necessary. As nowadays the combination of archaeology and deep learning is not perfect, the\nmodels are often limited by the small datasets of archaeology[10]. And 90 percent of archaeology’s\ndatasets can be seen as small datasets for the deep learning model[11]. What’s more, the output of\nthe model is always unexplainable, and the experts can’t know how the model calculates the result.\nTherefore, we still need to use traditional archaeological methods to check out the validity of the results.\nAnd the work of excavating the history behind cultural relics cannot be performed by computers, and\nexperts still need to analyze and summarize it. At present, the application of intelligent archaeology to\nlearning models is still in the stage of a simple application. Most contributions are in the development\nof visual or textual features which form the input to the model as a pre-processing step[12]. And few\nworks have used advanced models of clustering and classification.\n5 Future insights in intelligent archaeology\nIn conclusion, deep learning has gone into the field of archaeology and plays an important role. This\ntransformation is shifting both academic and cultural resource management inquiry. Many of its\napplications are evolutionary, greatly improving the types, scale, and complexity of analytic tools that\narchaeologists already use.\nWith the deep learning model, the experts can remodel some defective cultural relics, like ancient\npapyrus[13], and get more accurate structure information by combining advanced detection technolo-\ngies and deep learning model, like sites’ structure analysis[4]. And according to our survey, we think\nintelligent archaeology still has a long path to explore. As I mentioned above, the use of deep learning\nmodels in intelligent archaeology is still in its infancy. And if it can use some more advanced models\nand methods, this field will get a technological revolution again. And some problems that existed in\nthe application, like the accuracy of auto identification caused by low data[14], will be solved. More-\nover, I think intelligent archaeology will generate more subjects that analyze historical information\n8from different perspectives utilizing the ability of deep learning models. And now there are some new\nresearch subjects, for example, Molecular Archaeology[15].\nAnd in the future, intelligent archaeologists need to try to use deep learning models more flexibly.\nAnd utilizing some models to explore the potential that draws upon an ever-improving and ever-\nexpanding library of data, which can deal with a big problem in intelligent archaeology that lacks\nsufficient datasets. The revolution will be in integrating these outcomes into both academic and\ncultural resource management frameworks, which is a significant challenge given that archaeologists\nwill have to become competent in managing this much richer and more diverse information[16]. And\nthe experts in this field also need to actively explore the work pattern. The deep learning models are\ngenerated by the statistics so that some rare cultural relics can’t be identified and analyzed correctly\nby the model. Therefore, the experts need to explore how to arrange the work of experts and models\nin the workflow to minimize the amount of experts’ work and not miss rare cultural relics.\n"}, {"role": "assistant", "content": "最终打分：9 (范围0-10分)\n1. 结构完整性得分：9.5, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：8.5, 占比20%\n5. 参考文献规范性得分：10, 占比10%\n6. 课程知识掌握度得分：10, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "对于医生：1、部分医生不重视康复；2、医生时间繁忙，无法全程指导。\n对于医院：1、部分小医院没有康复科；2、大医院有床位周转率的要求。\n二、研究目的\n本研究的目标是基于人工智能算法开发一套基于移动终端的骨科术后下肢康复\n监测系统，其意义在于能够帮助患者在家中获得良好的康复训练指导，改善疾病\n预后，提升病人满意度，提高医院床位周转率，同时为科学研究提供临床大数据\n（研究意义）。\n引言：\n    最近我在网络中看到这样一则笑话，39.5度的司机载着38.5度的医生开的\n药给37.5度的患者送去，在这看似开玩笑的语气下隐藏着一个不争的事实，就\n是如今的医疗资源稀缺。为什么我要说这些呢，因为这和我今天要讲的主题：\n“智能骨科康复训练指导系统”有关。\n智能骨科康复训练指导系统指的是一套能由患者自主操作的系统，这套系统\n能给患者做出康复指导，并对患者的康复训练进行监测，帮助患者在居家环境下\n也有较好的恢复效果。\n除了前文所述，开发这套系统的原因有二：对于医生：1、部分医生不重视康复；\n2、医生时间繁忙，无法全程指导。对于医院：1、部分小医院没有康复科；2、\n大医院有床位周转率的要求。\n接下来我将播放一小段视频，来帮助大家了解康复训练是怎样检测的。（上\n视频，然后对视频进行解说） 大家可以看到，视频中的人物在在做深蹲运动，并\n且人体上的某些关键点（胳膊，肩膀，膝盖，胯，脚腕）被追踪且识别了出来，\n很多动作都要根据这些关键点进行特征提取，将记录这些点的数值变化。\n说到底，这种技术其实是人体姿态估计，它属于关键点检测和目的地跟踪，\n它本质是一个回归问题，要求输出关键点的坐标，\n（mscoco 上的17个关键点）\n至于某些应用要求多人检测，则需要用其他方法先将人之间分离（对人进行\n粗略的定位）再检测。但这里仅仅作为提及，不深入讨论（单-多）\n既然人体姿态估计能解决这么多的问题，也衍生出一些新问题。那么解决这\n个问题的算法有那些呢？\nOpenPose:(支持多人):https//www.jianshu.com/p/3aa810b35a5d中文文档 \n由美国宾夕法尼亚大学开发。\nDeepCut：先把所有关键点检测出来，再把它们组装成一个一个的人。\nSimple baselines\n这个项目使用的是Mediapipe， 它是一个在移动端和终端设备上进行边缘计\n算的库，所以它的模型非常轻量化，它运行的速度非常快，可以在智能手机这样\n的廉价的设备上运行。\nMediapipe:BlazePose\n专门为部署在移动设备和边缘终端设备上计算而生的，所以它天生就是为了\n追求机制的速度而不是极致的准确。（16：30）\n这张图我相信很多同学已经见过很多次，它只会再第一正的时候用一个人体\n检测器人脸检测器，把人体的区域ry（Regina 偶发 in）检测出来，然后再把它\n喂给关键点检测网络里，输出关键点，用上一帧得到的关键点数据，来推算出下\n一帧人体可能出现的区域，也就是上一帧的输出可以被下一帧的输入作为辅助。\n往后只要这个人还出现再画面中，我们也就不需要再调用这个人体检测器了。可\n以推算出，这样就可以大大加快速度，不需要再去调用额外的face detector。\n所以其实它并不能把轨迹真正的勾勒出来，而是\n上一帧的输出可以被下一帧的输入作为辅助。\n     第二张图：它结合两种人体姿态估计的两种主流技术，第一种heatmap,热\n力图，第二种是回归技术，\n热力图是输出的是一张图像，比如输出的是灰度图，这张图和原图大小是一模一\n样的，只不过里面像素某个关键点出现的概率，比如在肩膀这热力图就会有比较\n高的值，这是在高诉你这是肩膀可能存在的区域。\n输入图像，输出热力图。这里有33个关键点，就有各自33张热力图，没个关键\n点给出3个方向的offset的偏移量，所以就是99，它输出的是64*64的图，这\n个图有99个通道，表示的是，这33个关键点的99个偏移量。这就是网络的第\n一个图。另一个头是，直接让网络预测回归出关键点的坐标，实现的箭头是跨层\n链接，虚线箭头是不会反向传播回去的，而输出的时候直接使用预测的结果，能\n大大减少时间。\n脸检测是怎样实现的：\nOp：ar 准确率高，大规模集群gpu但速度慢\nPck@0.2是人体姿态估计的一个经典指标，它表示的是预测的点的坐标和正\n式值之间的欧式距离小于人体躯干距离的百分之二十， 那么就判断这个点预测对\n了。所以看这个数据就能反映性能。\n23：30 knn\n获取坐标了之后，怎么判断做了一个深蹲呢，举例摸脸，只需要检测脸和手\n之间的距离就行了。显然深蹲要复杂得多。这时我们需要更加细节的测量：主要\n节点间都进行距离的测量，有必要的话测量角度，用这些距离和角度作为特征就\n能定量一个特定的动作了， 当一个动作来的时候只需要把提取同样的特征然后把\n这个特征向量和训练集中原有的特征向量进行对比，判断这个动作属于那类。构建数据集：拍照：各个角落，完整人体，样本量越多越好\n第一步：拍摄深蹲二分类数据集（也就是蹲下状态和站立状态），把每一张\n照片用mediapipe来进行关键点检测，然后构建出一个深蹲的模型，也就是说有\n新的照片来的时候它从而能够判断出是蹲下的还是站立的， 并且能够量化出它的\n动作是否标准，从而实现计数。\n两个端点拍照，蹲下和站立\n再用blazepose来跑每个图片，把每张图片的特征都提取出来，再构建一个\n训练集。\n可以用knn 邻近算法 来做。就是提取图片的特征，与训练结集中与当前特征\n向量相差最近的向量相同。\n阈值设置\n总结\nBlaze 意为“火焰、失火”，反映了这一系列算法是从算法设计到实现都竭\n力追求“极速”的特点，方便对运行速度要求比较高的AR/VR开发者使用。\nBlazePose，一个轻量级的卷积神经网络架构，用于人体姿势估计，它是为\n移动设备上的实时推理量身定做的。在推理过程中，该网络为一个人产生33个\n身体关键点，并在Pixel 2手机上以每秒超过30帧的速度运行。因此它特别适\n合实时用例，如健身追踪和手语识别。在Pixel 3 上 GPU 运行，BlazePose可以\n达到112 FPS(frames per second)。\nBlazePose是从达芬奇的《维特鲁威人》中得到的启发，可预测人的臀部中\n点、外接整个人的圆的半径以及连接肩部和臀部中点的直线的倾斜角度，共计可\n预测33个人体关键点。根据手和脚的比例和方向信息，即使是非常复杂的情况，\n比如特定的瑜伽姿态，其也能得到一致的追踪。"}, {"role": "assistant", "content": "最终打分：6.5 (范围0-10分)\n1. 结构完整性得分：5.5, 占比20%\n2. 逻辑清晰度得分：6, 占比20%\n3. 语言连贯性得分：6.5, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：5, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "第30 组  常见皮肤病智能分析  技术报告  \n组长：李思锐  12012719\n组员：徐剑  12010923 ，张富珩 12011322\n \n一、项目背景  \n随着计算机技术的发展，基于人工智能的辅助诊疗系统已经陈伟一个主要的研究和发展趋势。到目前为\n止，这些系统已经在眼科、肿瘤学科和放射科学领域开发和研究，在皮肤科领域，一系列研究已经研究\n了良性和恶行皮肤肿瘤的分化，如良性痣、恶性黑色素瘤和角质细胞癌，例如反射共焦显微镜、皮肤镜\n和皮肤超声。然而，根据临床照片确定诊断良性色素性皮肤病尚无先例。我们的目标是提高医疗诊断的\n数字化、网络化以及基层医院的服务水平，减少医师培训时间，在提高医生的效率同时降低误诊率。\n \n二、数据集介绍  \n我们本次使用的数据集是由香港大学深圳医院的曾丽华医生所提供的八分类皮肤病数据集。详细信息如\n下：\n \n三、工作介绍  \n（一）训练框架  \n我们使用较为传统的深度学习框架来进行我们的训练。首先我们设置了随机种子，以确保得到稳定结\n果。其次我们令 learning rate=0.0001 、 epochs=100 ，同时我们以交叉熵函数 (CrossEntropy) 作为 Loss  \nfunction ，使用 adam 作为优化器，并使用配套的余弦退火调度器 (CosineAnnealing Scheduler) 来进行\n学习率的调整。 Batch Size 我们将在不同的网络模型上进行微调。上图为我们参数设置的具体代码。\n考虑到原始数据集数据总量较少，仅有不到 3000 张输入图片，我们还使用了基础的数据增强来增强输入\n数据的多样性。我们所使用的数据增强有： RandomHorizontalFlip ， RandomVerticalFlip 以及\nRandomRotation 。\n（二）不同网络模型对比  \n我们主要对比了三种比较主流的图像处理模型： Resnet50 ， Efficientnet-b4 以及 Densenet201 。考虑到\nEfficientnet-b4 与 Densenet201 的网络大小较大，其 batch size 若与 Resnet50 设置成一样的话可能会导\n致显卡内存不够而使得训练无法正常进行，因此我们将二者的 batch size 调设置为 8 ， learning rate 调整\n为0.001 ，而 Resnet50 的 batch size 设置为 100 ， learning rate 仍然为 0.0001 。我们所使用的网络都已经\n过ImageNet 的与训练。\n最终训练出来的效果如下图所示：\n由此可见， Efficientnet-b4 是最适合我们此次训练任务的网络模型。后续我们的一系列尝试也将使用\nEfficientnet-b4 作为 Baseline 。（三）图像混合策略使用  \n通过查阅文章我们得知，图像混合策略能够比起传统的旋转等数据增强更有效地增加输入特征的多样\n性，使得训练出模型的泛化性与鲁棒性得到提升。因此，我们选用了 mixup 以及 Cutmix 两种简单但却有\n效的图像混合策略，以期得到模型性能的提升。两种策略的简介如下：\nmixup  \nmixup 通过叠加两张不同的输入图像来产生新的图像。我们可以控制超参数混合比例 lam ∈ [0,1] 来\n控制不同输入在混合图像中的显著程度。由于在图像混合的同时标签也会有所混合，而常见网络并\n不会处理带小数的软标签，因此我们需要把混合图像在两张输入图像的所属类下分别计算 loss ，再\n将loss 以混合比例 lam 进行叠加。\nCutmix  \nCutmix 与 mixup 原理类似，只是产生新图像的方式由叠加混合变成将输入图像 1 的一部分挖去，再\n填上输入图像 2 的对应部分来代替。混合比例 lam ∈ [0,1] 主要是指两个输入图像在输出图像中所占\n的面积。 Loss 部分的计算与 mixup 相同。\n我们在保留 （二）不同网络模型对比 中Efficientnet-b4 训练参数的基础上，分别加入了上述两种策略，\n取得的结果如下：\n可以看到，相较于 mixup ， Cutmix 在皮肤病数据集上取得了相对优秀的结果。我们分析这是因为皮肤疾\n病图片特征较少且相对明显， mixup 所带来的两张图片混合叠加可能会带来相对较多的不必要特征混\n合，因此效果可能稍微逊于 Cutmix 。（四） Focal Loss 的尝试  \n我们发现，初始的数据集其实呈现的是长尾分布 (Long-tailed Distribution) ，其特征为部分类别数据量较\n大，而部分类别数据量较少。这样的数据分布会使得深度学习网络在训练的过程中过度关注头部类（多\n数类）的特征，而忽略尾部类（少数类）的特征，从而降低训练出模型的性能。\n通过查阅文献我们得知， Focal Loss 是在解决长尾分布问题中应用较为广泛的一种 Loss function ，其在\n传统 CrossEntropy 的基础上加入了一个权重系数，使得头部类分类分错给模型参数调整带来的影响降\n低，而使尾部类分裂分错带来的影响提升。 Focal Loss 与 CrossEntropy 在具体数据上 Loss 的对比如下\n图：\n但是比较遗憾的是，我们尝试在我们的数据集上引入 Focal Loss 时，模型的性能并没有得到很好的增\n加，甚至比起 Baseline 略有降低，而且我们一直未能找到原因所在。这是我们整个项目目前为止最大的\n遗憾，希望我们能在未来成功的找到问题所在！\n我们在 Efficientnet-b4 中加入 Focal Loss 所得的训练结果如下：\n但是，我们注意到 Focal Loss 的核心思维是 “ 加权重 ” ，那么别的形式的权重是否也能起到类似的效果？我\n们按  总样本数  / 该类样本数 生成了新的权重，并将其加入了 CrossEntropy 中：\n我们将加权重后的模型进行训练，并将加权重后的模型与 Cutmix 进行结合，取得了较为不错的最终结\n果：\n（五）知识蒸馏使用  \n知识蒸馏是一种模型压缩方法，是一种基于 “ 教师 - 学生网络思想 ” 的训练方法。我们将已经训练好的模型\n包含的知识，蒸馏提取到另一个模型里面去。\n温度 T 是知识蒸馏中的一个重要超参数，下面这个公式是原始的 softmax 函数 :\n直接使用 softmax 层的输出值作为 soft target, 这又会带来一个问题 : 当 softmax 输出的概率分布熵相对较\n小时，负标签的值都很接近 0 ，对损失函数的贡献非常小，小到可以忽略不计。因此 \" 温度 \" 这个变量就派\n上了用场。下面的公式时加了温度这个变量之后的 softmax 函数 :\nT 越高， softmax 的输出分布越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训\n练将更加关注负标签，从而学到更多信息。 \n在我们的实验中，则是运用了多个教师模型的知识蒸馏，我们将数据集的类别按照样张的数量分到三个\n子集中，每个子集中样本的数量差距较小，在一定程度上改善了原本数据集的不平衡问题。然后，我们\n在这三个数据集中分别训练出教师模型。再用这三个教师模型共同指导学生模型的训练，完成知识蒸\n馏。\n在结果中我们可以看到，尽管整体的 acc 提升不大，但在这些样本较少的类别中， f1-score 有了显著的提\n升。这意味着模型对少数类的辨别能力有了很大的提升，从而说明这一方法起到了作用。\n未来工作  在使用知识蒸馏这一方法时，我们发现，教师在关系子集上的表现仍然有提升空间。这是因为，即使划\n分了关系子集，每个子集中各个类别的样本数量仍然存在着一定的不平衡的情况。针对这一问题，我们\n计划在教师的训练过程中融合进更多的训练方法，例如上文中提到的图像混合策略和 Focal Loss 。在提\n升了教师模型的表现后，我们认为蒸馏出的学生模型的表现也会有所改善。\n课程学习  (Curriculum learning, CL)  是近几年逐渐热门的一个前沿方向。 Bengio 首先提出了课程学习\n（Curriculum learning ， CL ）的概念，它是一种训练策略， 模仿人类的学习过程，主张让模型先从容易\n的样本开始学习，并逐渐进阶到复杂的样本和知识 。CL 策略在计算机视觉和自然语言处理等多种场景\n下，在提高各种模型的泛化能力和收敛率方面表现出了强大的能力。在这样的不平衡数据集中，我们可\n以先将从数据集中提取出各类别平衡的数据，让模型在这个子数据集上进行训练。随着训练的进行，我\n们将剩余的样本逐渐加入训练集中，从而实现一个从易到难的训练过程。\n除此之外，由于我们在这一项目中使用的方法大多可以广泛应用在数据不平衡这一问题上。这意味着我\n们的数据集将可以不局限于皮肤病，在视网膜眼底图像等数据集中我们的方法应当同样适用。我们将会\n结合更多类型的数据集对我们的方法进行进一步的验证。"}, {"role": "assistant", "content": "最终打分：7.6 (范围0-10分)\n1. 结构完整性得分：8, 占比20%\n2. 逻辑清晰度得分：8.5, 占比20%\n3. 语言连贯性得分：7.5, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：3, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "人工智能在颅内动脉瘤图像识别与分割方面的应用\n王谦益1 王天瑞1\n1 南方科技大学 广东 深圳 518055\n摘 要 颅内动脉瘤是常见脑血管意外的一种。这一疾病危害极其严重需要尽快发现患病患者，及时找出动\n脉瘤所在位置然后进行及时、恰当的治疗。当医生得到通过医学成像技术获得的影像资料之后，将会对影\n像资料进行人工评估，过程繁琐、工作量极大，便需要借助人工智能的手段来简化医生的工作量。在这篇\n文章中将介绍颅内动脉瘤的医疗背景以及深度学习等人工智能技术在颅内动脉瘤的图像识别方面如何应\n用。\n关键词：人工智能；颅内动脉瘤；深度学习；图像识别；\n中图法分类号 R443\nApplication of Artificial Intelligence in Image Recognition and \nSegmentation of Intracranial Aneurysms\nWang Qianyi1 and Wang Tianrui1\n1 Southern University of Science and Technology, 518055, China\nAbstract: Intracranial aneurysm is one of the common cerebrovascular accidents. It's a very serious disease \nand it's important to find the patient as soon as possible, to locate the aneurysm and to treat it appropriately. \nWhen doctors get the image data obtained by medical imaging technology, they will manually evaluate the \nimage data. The process is tedious and the workload is huge, so artificial intelligence is needed to simplify \nthe workload of doctors. In this paper, the medical background of intracranial aneurysm and the \napplication of artificial intelligence technology such as deep learning in image recognition of intracranial \naneurysm will be introduced.\nKeywords Artificial intelligence, Intracranial aneurysms, Deep learning, Image recognition\n1引言\n颅内动脉瘤指的是脑动脉壁局部异常凸起的\n部分，是造成蛛网膜下腔出血的首位病因。根据数\n据统计，中国人当中每100人之中就会有3到7个\n人左右患有此疾病。而一旦颅内动脉瘤破裂造成出\n血，将会有至少五分之一以上的患者重度残疾甚至\n导致死亡。由此可见，颅内动脉瘤这一疾病危害极\n其严重。在找寻颅内动脉瘤位置这一方面，我们可\n以通过不同医学成像手段，比如数字减影血管造影\n技术（DSA）等等。而人工智能则在对医学影响分\n析、识别、分割方面有突出作用，可以很好的简化\n医生工作量，并且增加识别颅内动脉瘤的精确度。\n常见的人工智能技术运用了机器学习和深度学习\n模型，通过对这两大类模型的研究我们可以大致了解人工智能技术在颅内动脉瘤检测上的应用，从这\n两方面的分析有利于更加清楚的了解人工智能在\n这一领域的优势与不足。\n2背景介绍\n颅内动脉瘤大多是出现在颅内动脉管壁上的\n异常膨出，在脑血管意外中，仅次于脑血栓和高血\n压脑出血，位居第三。这种疾病没有准确的发病年\n龄区间，任何年龄的人都有可能患有此疾病。在已\n有的案例当中，多数发病患者为40岁到60岁之间\n的女性。\n未破裂的动脉瘤一般是没有相应症状的， 通常\n是在检查的时候发现出来。但是比较巨大的动脉瘤\n在压迫到脑神经的情况下可能会有相应症状，比如\n说视力下降、视物重影、眼睑下垂、肢体无力等。而颅内动脉瘤破裂出血则是造成蛛网膜下腔\n出血的首位病因。蛛网膜下腔出血的常见症状为剧\n烈爆炸样头痛。这种疼痛难以忍受，并且常常与恶\n心，呕吐，枕颈部僵硬等症状共同出现，严重者会\n出现躁动、嗜睡甚至昏迷等症状，部分患者还可能\n回出现肢体无力、语言障碍、眼睑下垂等症状。\n颅内动脉瘤的形成原因到现在为止都没有具体的\n定论，不过大多数的学者都认为颅内动脉血管管壁\n的局部先天性缺陷缺陷以及血管腔内部的压力上\n升是颅内动脉瘤形成的主要因素。高血压、脑动脉\n硬化、血管炎等疾病都与颅内动脉瘤的形成与发展\n有着密不可分的联系。\n动脉硬化主要指的是动脉的血管壁不断增厚\n变硬的现象，这会导致血管壁失去了弹性，最终引\n起血管腔的缩小。一般来说，常常讲的动脉硬化其\n实主要是指动脉粥样硬化，这是动脉硬化中最常见\n和最重要的一种类型。动脉粥样硬化是一种动脉斑\n块在动脉内积聚的状态，最终会导致颈动脉狭窄。\n而这种斑块中通常含有脂肪、胆固醇以及其他的一\n些物质。引起动脉粥样硬化的因素同样复杂、难以\n明确阐释。其中最为主要的危险因素包括高血压、\n高血脂，大量吸烟，糖尿病、肥胖以及遗传因素等\n等。\n颅内动脉瘤经常发生在脑底动脉环上， 而其中\n有80%都是发生在脑底动脉环的前半部分。根据文\n献调查的57例病例当中，在内耳道附近的有45例\n占比80%；环路远端的病例有6例，占11%；环路\n近端有1例；位于第一分叉处的有1例；而内侧支\n处有3例占比5%。\n目前对颅内动脉瘤的检测技术大致上有三种。\n如图一所示，第一种是数字减影血管造影技术，即\nDSA。这是动脉瘤诊断方面以及复查与否的黄金标\n准，因为DSA技术能够最为准确、清晰地判断颅内\n血管中是否有动脉瘤存在，并且帮助判定动脉瘤的\n治疗方式。第二种是计算机断层扫描血管造影，即\nCTA。这种技术在重建图像后进行判断的效果方面\n与DSA相比稍差，但是胜在简便易行，可以用于脑\n血管疾病的筛查及蛛网膜下腔出血之后对动脉瘤\n筛查的情况中。第三种是核磁共振血管造影，即\nMRA。这是一种无创无辐射的技术，同样可用于动\n脉瘤的筛查，但是微小动脉瘤的检出率偏低，对与\n动脉瘤情况的评估也比较差。图 1 三种常见的检测技术图形\nFigure 1. Three common detection techniques\n据全球的数据统计结果来看， 目前普遍认为认\n为动脉瘤手术治疗发送严重并发症的概率约为3%\n到5%，但动脉瘤一旦破裂病死率将超过20%，所\n以一般会推荐对动脉进行积极的手术治疗，但也会\n根据动脉瘤的大小、形态、部位等具体情况而定。\n破裂的动脉瘤在条件允许情况下都需要进行\n治疗以降低再次破裂出血的风险，这是因为再次破\n裂出血后的致死致残率将超过50%。而未破裂动脉\n瘤的破裂风险以及是否需要手术治疗则通常需要\n进行一系列的评估，目前经常使用的评估手段有\nPHASES评分等。PHASES评分主要是针对动脉瘤破\n裂相关的常见危险因素进行评估， 其中包括高血压、\n年龄、动脉瘤尺寸部位、既往出血的病史等因素。\n近期，基于人工智能的动脉瘤破裂风险模型评估，\n较PHASES评分系统更加准确。动脉瘤形态学评估、\n血流动力学评估、高分辨率核磁动脉瘤壁稳定性评\n估等对于动脉瘤破裂风险的评估也具有重要作用。\n目前，由于介入治疗创伤更小，术后恢复时间\n更短，介入治疗已成为颅内动脉瘤治疗的首选手术\n方式，主要包括单纯弹簧圈栓塞、支架辅助栓塞、\n血流导向装置植入、瘤内封堵装置植入等。\n对于介入困难、风险高、复发率高、出血量大的动\n脉瘤，推荐选择开颅手术治疗，包括夹闭、包裹、\n孤立等，部分复杂动脉瘤同时需进行血管搭桥治疗\n甚至杂交手术治疗。\n3人工智能算法运用\n颅内动脉瘤的手术及各种治疗都需要较好的\n评估仪器和手段。基于人工检测的高昂成本和计算\n机检测的快速和高效性，将计算机算法应用于动脉\n瘤图像识别上是非常有意义的。而随着计算机科学\n的进步，当今人工智能在颅内动脉瘤里的应用分为\n机器学习和深度学习两个部分。\n3.1机器学习算法\nInes等人早在2014年就提出了一个完整的检\n测方案用于检测大脑2D-DSA图像中的斑点和动脉\n瘤。[1]第一个分类阶段通过多个分类器的融合提取\n脑动脉管系统。第二个检测阶段涉及使用模糊数学\n形态学从分割图像中检测血管树中的动脉瘤。这个\n想法使用了模糊逻辑对动脉瘤的大小和形状进行\n不精确建模。通过测试图像，该实验结果表明了方\n法的实用性，结果显示出模型具有高灵敏度和低误\n报率。\nYu等人的研究是利用计算机断层扫描血管成\n像( CTA )。[2]虽然效果略低于使用DSA图像，CTA\n在脑动脉瘤的诊断、治疗评估和监测中也起着至关\n重要的作用。巨大颅内动脉瘤( GIA )的CTA医学图\n像的分割提供了血栓和动脉瘤几何特征的定量测\n量，允许三维重建。事实上，GIA显示了部分或完\n全自发性动脉瘤内血栓形成的神经放射学特征和\n倾向性。尽管Yu等人对医学图像分割进行了深入\n的研究，但动脉瘤(管腔、血栓和父母血管)的分割\n仍然是一个尚未解决的难题。基于此Yu等人提出\n了一种用于CTA图像中动脉瘤分割的玻尔兹曼晶格\n法( LBGM )，以同时估计血栓和动脉瘤的体积。虽\n然CTA图像中的噪声非常强，而且血栓的边缘与周\n围组织差别不大，但是模型还是有效地分割了动脉\n瘤。基于这些结果，使用圆顶-颈部纵横比( AR )参\n数来评估自发性血栓( ST )现象的方法显示了这种\nLBGM在临床应用中的潜力。\n2019年，Ines等人设计出一种用于检测二维\nDSA图像里的颅内动脉瘤。[3]在第一阶段使用统一\n的局部二值模式( LBP )确定初始动脉瘤候选。从这\n些图像中计算LBP，以识别动脉瘤和非动脉瘤类的\n纹理内容。第二阶段是使用基于轮廓的形状描述符\n去除假阳性：通用傅里叶描述子（GFD）。Ines证\n明了所提出的检测方法成功地识别了颅内动脉瘤\n的形态学特征。结果表明，手动和自动检测之间具\n有极好的一致性。在计算机化IA检测框架下，所有\n动脉瘤都被正确检测到零假阴性和低FP率。\n传统的机器学习算法曾广泛运用于各种图像\n识别中，但传统方法由于需要进行人工对象分割或\n特征提取工作，而无法直接学习图像数据。相比较\n而言深度学习方法能直接使用图像中的像素值，减\n少了人工标注的冗杂过程，取得了较好的性能的同\n时还降低了许多成本，故而深度学习模型现被广泛应用于多种疾病的诊断过程。\n3.2深度学习算法\nJoseph等人较早的实现了深度学习网络在动\n脉瘤检测方面的应用。[4]基于CNN实现的神经网\n络模型可以在一定程度上准确地预测动脉瘤的大\n小。 但是Joseph等人的研究尚未扩展到完全三维的\n数据集，没有包括更多的训练案例，所以得到的模\n型准确性不足。在更多的训练案例的支持下Joseph\n等人认为可以实现动脉瘤体积的大规模自动提取，\n用于研究和临床应用。\nTim等人提出需要结构增强过滤以突出显示\n潜在的动脉瘤位置，用于区域血管形状编码和降维\n的血管内距离映射以及卷积神经网络以自动确定\n动脉瘤检测的最佳特征和分类规则。[5]这样可以在 \n3D 脑血管造影中及早和更灵敏地检测小动脉瘤，\n以防止潜在的致命破裂事件。Tim等人使用 15 个 \n3D 数字减影血管造影进行评估，与增强过滤和基\n于随机森林的方法相比，所提方法的性能更好，因\n为它在低误报数（每个数据集 2.4 个）下实现了 \n100% 的检测灵敏度。Tim等人提出的方法也适用\n于其他血管造影方式。基于此提出了一种检测3D\n脑血管造影中囊状动脉瘤的新方法，其中首先使用\n基于Hessian的过滤器执行预分类步骤，该过滤器\n增强球形和椭圆形结构，例如动脉瘤并减弱血管造\n影中的其他结构。接下来使用在血管内距离图上训\n练的 2D CNN 提高分类性能， 距离图是通过从预先\n分类的体素投射光线并检测血管结构的首次命中\n边缘来计算的。与以前的方法相比，CNN进一步提\n高了特异性，同时保留了动脉瘤检测的高灵敏度。\nZeng等人提出了一种基于未重建3D-RA序列\n的颅内动脉瘤计算机辅助诊断方法。[6]通过将空间\n信息整合到二维数据中，我们显著提高了单幅图像\n的检测精度。最终，该方法准确率为98.86%，灵敏\n度为99.38%，特异性为98.19%。他们首先调整序\n列的亮度和对比度，然后进行数字减法和降噪。所\n得图像效果与动脉瘤诊断的金标准DSA图像效果\n相似。Zeng等人将序列的SIF特征图像作为训练数\n据，通过对VGG16的结构稍作调整，应用快速迁移\n学习进行训练。为了评估SIF特征的有效性，Zeng\n等人研究了SIF特征在不同尺度下的影响，并进一\n步测量了SIF特征在不同大小动脉瘤上的表现。研\n究结果表明SIF特征存在规模的上限，其可以在一定程度上有效地提高动脉瘤的检测精度，但超过一\n定限度后会引入冗余信息，导致有效的动脉瘤特征\n被淹没。\nJin等人利用网络结构基于通用的U形设计，\n开发一种新的全自动检测和分割深度神经网络框\n架，以帮助神经科医生在诊断过程中评估和勾勒出\n二维的时间数字减影血管造影（DSA）序列的颅内\n动脉瘤。[7]该网络包括一种完全卷积技术，用于检\n测高分辨率DSA帧中的动脉瘤。此外，在网络的每\n个级别引入双向卷积长短期记忆模块，以捕获通过\n二维 DSA帧的对比介质流的变化。由此产生的网\n络结合了来自DSA序列的空间和时间信息， 并且可\n以进行端到端的训练。此外，Jin等人还实施了深度\n监督以帮助网络融合。所提出的网络结构由来自\n347名IA患者的2269个DSA序列训练。之后，在\n盲测集上对来自146名患者的947个DSA序列进行\n评估，收获了不错的评估效果。基于结果JIn等人\n认为这种深度神经网络有助于成功检测和分割2D \nDSA序列中的动脉瘤，并可用于临床实践。\n结束语\n基于医疗背景， 处理颅内动脉瘤具有及其重要\n的医疗意义。人工智能如今也已广泛运用于医疗领\n域，先关的实验研究也非常的丰富。人工智能从发\n展至今经历了机器学习算法到深度学习算法的演\n变，而与此也有对应的关于处理颅内动脉瘤图像的\n算法分析。从传统机器学习方法到深度学习方法，\n我们看到不同的模型对于不同数据的分析，了解到\n各种方法和模型之间应用场景的差异。这对于未来\n能够有针对性的在颅内动脉瘤领域应用人工智能\n具有很重要的意义。\n人工智能在临床领域应用还很漫长。 大部分人\n工智能研究只能停留在理论验证阶段，距离转为临\n床应用还有很大的差距。这需要大量的样本数据保\n证：具有不同人口统计学、成像设备、外科医生和\n治疗方案的外部数据集，都会影响模型验证、适应\n和准确处理未知数据的评估。"}, {"role": "assistant", "content": "最终打分：9.1 (范围0-10分)\n1. 结构完整性得分：9.5, 占比20%\n2. 逻辑清晰度得分：9, 占比20%\n3. 语言连贯性得分：9.5, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：10, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "基于深度强化学习的无人驾驶决策算法研究\n宋北丞，韦洁\n南方科技大学\n摘 要行为决策系统能够综合环境及自车信息， 使自动驾驶车辆产生安全合理的驾驶行为，\n是实现无人驾驶的核心。强化学习算法采用一种自监督学习的方式，使自动驾驶车辆的决策\n系统在与环境的交互过程中，通过不断改进自身策略自主学习到最优的决策模型，为构建有\n效的决策系统提供了方向。 文中介绍了目前无人驾驶决策算法的基本框架与一些基于学习的\n与自动驾驶相关的算法与一些传统的决策算法， 并介绍了近年来基于深度强化学习的决策相\n关算法的研究进展。文中也列举了决策算法在无人驾驶汽车中的具体应用，从礼让行人、紧\n急避险、变道以及通过红绿灯路口四个方面详细说明了决策算法的具体决策过程。文章最后\n从尚可提升之处与当今时代有利于无人驾驶发展的有利之处两个方面提出了决策算法的研\n究前景。\n关键词：无人驾驶；决策算法；深度学习\nResearch on Autonomous Driving Decision Algorithm Based on \nReinforcement Learning\nBeicheng Song and Jie Wei\nSouthern University of Science and Technology\nAbstract  The behavior decision system can integrate the environment and self-vehicle \ninformation to make the self-driving vehicle produce safe and reasonable driving behavior, which \nis the core of the realization of driverless driving. Reinforcement learning algorithm adopts a \nself-supervised learning method, so that the decision-making system of autonomous driving \nvehicle can learn the optimal decision-making model through continuous improvement of its \nown strategies in the process of interacting with the environment, which provides the direction \nfor the construction of an effective decision-making system. This paper introduces the basic \nframework of autonomous driving decision algorithms, some learning-related algorithms and \nsome traditional decision algorithms, and also introduces the research progress of decision \nrelated algorithms based on deep reinforcement learning in recent years. The paper also lists the \nspecific application of the decision algorithm in the driverless car, and explains the specific \ndecision process of the decision algorithm in detail from four aspects: comity to pedestrians, \nemergency avoidance, lane change and crossing traffic lights. At last, the paper puts forward the \nresearch prospect of decision algorithm from two aspects, which can still be improved and the \nadvantages which are conducive to the development of unmanned driving in the current era.\nKeywords  Autonomous driving; decision algorithm; reinforcement learning随着科技的发展，自动驾驶因其在军事、商业以及民用领域广泛的应用前景与巨大发展潜力而成为近\n年来的一大研究热点，而无人驾驶汽车则是该项技术蓬勃发展背景下的一项新兴产物。无人驾驶汽车是一\n个集外界环境感知与信息收集、行驶路径规划与决策制定、驾驶动作执行与运动控制为一体的一项综合智\n能系统。环境感知系统可以通过车载摄像头、车载雷达等设备对外界环境信息进行收集，并利用计算机视\n觉与信息融合技术对收集到的信息进行处理。决策系统能够通过对处理后的外界信息以及车辆自身的状态\n信息进行判断并做出合理的驾驶行为决策。运动控制系统可以根据决策系统做出的决策信号控制车辆的运\n动。决策系统作为整套无人驾驶系统的核心与“大脑”，是自动驾驶最为核心的技术，对于整个系统的性\n能与效率有着举足轻重的地位。因此决策系统，特别是安全高效且智能的决策系统成为自动驾驶技术领域\n的一大研究热点。\n强化学习是一种新兴的人工智能算法，智能体在与环境的不断交互反应中得到不同的反馈信息，并基\n于这些反馈信息不断改善自身的算法策略以及部分参数，从而在不断的学习与调整中得到能够适应环境达\n到目的的最优算法，强化学习由于其显著优越性与高度灵活性在很多领域中都展现出了较强的应用潜力。\n深度学习类似于传统的机器学习，但是深度学习可以利用神经网络结构学习数据特征并进行回归或分类，\n深度学习在图像处理方面有较强的优势。将强化学习与深度学习相结合并应用于自动驾驶的决策算法中有\n助于构建一个具有高自由度的车辆行驶决策模型，通过多次模拟各种真实环境下的车辆行驶场景来训练并\n优化该决策模型，最终可以得到一个能够高效做出最优决策并对突发情况做出有效应对的自动驾驶决策模\n型。\n目前已有许多在相关领域的算法研究综述，分别专注于算法模型的不同方面进行归纳总结与展望，在\n文献[1]从决策精度、决策广度、应对不确定因素三个方面对于基于强化学习的决策算法模型研究进展进行\n了简要介绍；文献[3]简要介绍了基于学习的自动驾驶决策算法的基本概况、目前应用以及未来研究前景；\n文献[3]则介绍了无人驾驶决策规划领域的新趋势，如端到端学习以及交互式路线规划等。这些综述都基于\n某些特定方面对于现有的自动驾驶决策算法进行分类，并对每一类算法进行分析与比较，但由于缺少具体\n的应用场景所以直观性较弱；本文不仅对于自动驾驶的决策算法相关信息进行了详细的阐述，还结合某些\n特定应用场景更加直观地体现出基于强化学习的决策算法的优势与运行逻辑，并对于现有决策算法在自动\n驾驶汽车中的应用进行了系统的分析与前景展望。\n本文共分为四个部分，在第一部分中将简要介绍目前无人驾驶决策算法的简要概括与基本框架，第二\n部分介绍了一些基于学习的与自动驾驶决策相关的算法以及一些经典的决策算法，第三部分介绍了近年来\n基于深度强化学习的决策相关算法的研究进展，第四部分结合一些具体场景总结了决策算法在现有无人驾\n驶汽车之中的应用，第五部分讨论了自动驾驶汽车决策算法未来的发展方向以及研究重点。\n1决策算法的基本概述1.1 决策系统的基本框架\n自动驾驶系统中的决策部分是感知环境与信息收集部分以及驾驶动作执行与运动控制部分的过渡，通\n过处理感知系统收集到的关于外界的信息从而做出相应的决策并由执行系统进行实现。 对于决策系统来说，\n其输入即为外界环境信息以及车辆自身的状态信息，而输出即为一系列动作执行命令，包括输入运动执行\n系统的高级与低级行为控制命令。决策系统的基本框架如图1所示，输入信息包括由车载传感器收集到的\n周边的环境信息、车辆自身状态信息以及高清地图信息，而输出的信息包括高级与低级行为控制命令。高\n级行为控制命令包括超车、变道等，低级行为控制命令包括控制车辆的速度与加速度等。\n1.2 决策系统的基本准则与约束\n决策系统的目的是通过一系列的标准与规范得出类人的安全高可靠的驾驶决策。而决策系统的基本准\n则可以简要归纳为五个方面：合理正确地生成决策，决策的实时性好，行车安全与效率的平衡，车辆行驶\n稳定性好，高故障检测能力与突发情况应对能力。\n为了能够得到更加高效可靠的决策，决策系统需要将多方面因素纳入决策方法的约束中，包括车辆周\n边的环境信息如道路信息、周围车辆与行人的行为预测、交通标志信息等，当地的交通法规信息，车辆自\n身状态，每一决策导致的后续结果，历史决策结果参考，驾驶道德等。\n图 1 决策系统基本框架（Liu,2021）\n2基于学习的决策算法与传统决策算法的简要介绍\n2.1传统决策方法\n一般来说，传统的决策方法根据决策依据可以大致分为三类：基于规则的决策方法，优化决策方法，\n基于概率的决策方法。\n2.1.1 基于规则的决策方法\n基于规则的决策方法根据由各种交通法规、驾驶知识与经验等构成的规则数据库以及车辆自身状况等\n信息进行决策。这类决策方法具有较好的决策广度与较强的可积性与适应性，但是由于缺少决策深度所以\n很难在复杂的驾驶环境下做出正确高效的决策，且对于动态驾驶环境的鲁棒性较低。\n此类决策方法中最具代表性的是有限状态机（FSM）。FSM是一种具有离散输入与输出的数学模型，能\n够根据对外界环境的响应生成对应的动作，智能体的状态也对应的发生了改变。FSM方法目前已经运用在\n一些传统的自动驾驶车辆中。\n2.1.2 优化决策方法\n优化决策方法通常通过奖励函数或者效用函数来生成决策结果。这类方法可以更好的将不同交通组成\n部分之间的互相作用进行建模，并且可以生成更加优化的决策结果。但是所谓的“优化策略”对于不同的\n场景往往有不同的含义，因此也会影响系统做出决策。\n模型预测控制（MPC）可以生成自我车辆的高级行为，而其他车辆都会收到自我车辆行为的影响。博\n弈论是另一种解决决策问题的优化决策方法，博弈论首先提出所有智能体都将采用“最优策略”的假设，\n并将根据其他智能体的相应策略产生行为。\n2.1.3 基于概率的决策方法\n基于概率的决策方法可以根据数学中的概率论来生成每一行为对应的结果，并需要通过建立概率模型\n来得到行为决策。这类方法很便于与其他类型的决策方法相结合，但是这类方法具有较低的计算效率并且\n很难在较为复杂的环境下得到最优决策。\n2.2 基于学习的决策方法\n基于学习的决策方法是指通过使用人工智能技术实现自动驾驶决策，通常需要首先建立一定的数据样\n本，之后采用不同的学习方法或网络结构，根据不同的环境做出合理决策，实现系统的自主学习。基于学\n习的决策算法主要分为三类：基于统计学习的决策算法，基于深度学习的决策算法，基于强化学习的决策\n算法。\n2.2.1 基于统计学习的决策算法\n基于统计学习的决策算法可以通过处理大量的训练集数据从而达到类人的决策水平。此类决策算法具\n有良好的通用性，很适合用于在拥有大量环境数据下的简单情景决策；但是此类算法具有较低的决策准确\n度，且需要大量的训练数据集来进行自我学习。\n基于统计学习的决策算法中较为经典的有SVM和AdaBoost等。SVM可以将相对位置和行驶速度作为输\n入进行变道决策，在考虑变道收益、安全度等约束条件的情况下，结合贝叶斯参数优化能够更好地确定变\n道模型。该算法最终在实际运用检验中得到了较高的准确率。AdaBoost可以用来进行车辆插入场景下的风\n险评估决策，该算法将车辆的速度作为输入，将碰撞距离（DTC）作为输出，能够实现该情况下的安全机动。\n2.2.2 基于深度学习的决策算法\n基于深度学习的决策算法框架与传统的机器学习算法类似，但是主要区别在于深度学习使用神经网络\n结构来处理数据提取数据特征，并对数据进行分类或者回归产生对应结果。由于深度学习算法在处理图像\n是具有优势，在一些特定场景下具有较高的决策准确度；端到端的决策系统保证了对于环境信息的充分利\n用，被用于自动驾驶汽车的决策系统。该类方法通常将感知系统收集到的数据作为输入，通过神经网络输\n出低级控制命令。\n由于视觉传感器的高性价比，其被广泛配备在自动驾驶汽车上，因此一些端到端的系统就是基于视觉\n图像进行的。基于卷积神经网络（CNN）进行端到端系统的决策培训，可以成功实现不同环境下是否转向的\n决策任务。注意力分支网络（ABN）将输入的原始视觉图像进行处理生成注意力图，再将注意力图与原始图\n像的卷积特征和车辆的预期速度相结合，得出是否转向与是否踩油门的决策命令，实现端到端的决策任务。\n2.2.3 基于强化学习的决策算法\n强化学习是目前最常用的基于学习的决策算法方法之一，强化学习通过尝试各种行为来得到最大获利\n的行为决策，算法中的参数与行为可以根据奖励函数进行调整。基于强化学习的决策算法更适合对于不确\n定的或动态的环境进行建模，且灵活的算法框架使整个方法更具扩展性；但是此类方法非常依赖于其内的\n奖励函数且较为不稳定，在深度强化学习算法下可能会导致过拟合。\n基于强化学习的决策算法通常依赖马尔可夫决策过程（MDP）来描述智能体于环境之间的互作状态，\n同时还提出了部分可观测马尔可夫决策过程（POMDP）来描述更加真实的状态空间。\n3近年来基于深度强化学习的决策算法的研究进展\n近年来随着深度学习与强化学习领域的快速发展，深度强化学习（DRL）也展现出在智能决策方面的\n巨大潜力。深度强化学习的核心思想是将神经网络整合到强化学习中，代表性的方法有Deep Q Network \n(DQN)、Deep Deterministic Policy Gradient (DDPG) 和 Asynchronous Advantage Actor-Critic (A3C)等。\n深度强化学习算法结合了深度学习的表征优势和强化学习的决策优势，显著地提高了决策的规模与维度，\n通过处理高维信息输入实现了高精度的决策规划。图2展示了深度强化学习的基本框架，通常由四部分构\n成：交互环境E、智能体Agent、状态转移的规则、奖励函数及奖励规则。图 2 深度强化学习框架（Huang，2020）\n3.1 DQN算法\n深度强化学习往往将问题建模为部分可观测马尔可夫决策过程（POMDP），这是一种离散随机控制过\n程，在该过程中环境的状态是部分可观测的，并且当前的行为会对未来的状态产生影响，并进一步影响未\n来的奖励函数输出结果。这意味着智能体在进行决策时需要考虑到所做出的决策对于未来产生的影响。而\n深度Q算法（DQN）就是通过将卷积神经网络（CNN）与Q-Learning算法相结合得到的一种基于值的决策算\n法，智能体可以给每一对状态与动作预测一个对应的Q值，并从所有的集合中选取具有最高值的动作作为\n最优策略，同时智能体还可以利用这些值来训练模型进行优化。DQN的主要目标就是通过不断学习优化得\n到最优Q函数，通过不断实验对不同状态下的不同行为进行赋值并计算奖励函数输出，并在优化过程中使\n用贝尔曼方程不断更新神经网络从而得到最优Q函数。目前有许多原始DQN的变形模型，如Double DQN与\nDueling DQN，通过将动作与值预测分流来达到更加高效与稳定的学习效果。\n图 3 POMDP模型（Aradi，2020）\n3.2 DDPG 算法\n深度确定性策略梯度算法（DDPG）基于DQN算法理念进一步提出，能够将深度强化学习拓展到连续的\n动作域中，实现更好的性能。相较于离散型动作输出的DQN模型，连续型输出的DDPG决策模型通过端到端\n的驾驶决策具有更加优越的决策效果。DDPG算法属于Model-Free深度强化学习算法， 采用Actor-Critic（\nAC）结构体系：Actor网络，Critic网络以及两个网络各自对应的目标网络共同构成体系的网络部分，并\n且还有随机噪声以用于增加系统的环境探索能力和经验回放池（Replay-Buffer），并通过离线策略（\nOff-Policy）的方式对整个网络进行训练。\n图 4 DDPG系统模型（Huang，2020）\n4决策算法在现有自动驾驶汽车中的应用\n决策算法在开始其实并不局限于自动驾驶领域，其决策的能力在很多的方面都有应用，\n并且在深度学习算法将深度学习的表征能力与强化学习的决策能力相结合之后， 显著的强化\n了学习空间的规模和维度。就比如在阿尔法狗程序的开发上，首次将卷积神经网络和\nQ-Learning算法结合以后提出了深度Q网络算法。 被训练以后的深度Q网络算法已经在某\n些游戏中的表现高于了人类玩家，在其和蒙特卡洛树搜索算法结合之后，阿尔法狗便被创造\n了出来， 并且在各大比赛的表现中屡次击败人类玩家， 如今它的学习水平仍然在不断的进步。\n更多的在不同领域的决策算法的应用数不胜数， 接下来我还是主要介绍一下决策算法在自动\n驾驶汽车的应用。\n4.1自动驾驶中对于礼让行人的决策：\n首先是信息收集:选取不受红绿灯控制的斑马线路段，收集真实车辆驾驶员向行人让行\n的数据，包括车速、车与行人的距离、行人在斑马线处的位置、车辆减速等。第二步是信息\n处理。对第一步收集到的数据进行筛选，剔除驾驶员突然刹车行为和行人强制停车行为的数\n据，保留驾驶员主动让行和减速行为的数据。第三步是建立规则:将第二步中保存的数据进\n行分类组合，根据分类组合后的数据建立司机向行人让行时的逻辑模糊规则。第四步是规则\n应用:应用第三步中建立的驾驶员向无人驾驶汽车让行时的逻辑模糊规则，控制无人驾驶汽\n车在没有红绿灯的斑马线处向行人让行。从这里，我们可以看到自动驾驶仪对行人让行行为\n的学习过程。该方法根据无人驾驶汽车当前的速度和与行人的距离，自动实现符合真实驾驶\n员操作的减速，使车辆在行驶过程中更加稳定和拟人化。\n4.1.1 自动紧急刹车系统 AEB：\nAEB系统和POMDP规划器一起工作。 它使用了POMDP规划器生成的驾驶轨迹来计算碰撞\n风险。如果碰撞不可避免会发生，AEB系统就会触发，进行紧急刹车，紧急停止具有最高优\n先级，并能够否决掉POMDP的计划。由于AEB系统能够要求更强的制动干预，因此将POMDP\n规划器与AEB系统相结合可以获得更高的平均速度。下图显示了紧急制动干预的数量，当行\n人出现的概率较高时，紧急制动干预的数量就会减少。随着干预次数的减少，系统在通过阻\n塞区域时变得更加保守。\n配备AEB系统的POMDP规划者能够更快地行驶，但需要紧急制动干预以避免碰撞。\nPOMDP计划器在块区域前减速。以低于50公里/小时的速度行驶，避免与被阻挡的行人相\n撞。在这种情况下，AEB系统无法避免以50公里/小时的速度发生碰撞。在高速行驶时，反\n应时间不足以让车辆停下来。障碍物前的速度过高，减速不足，这说明了基本AEB系统的优\n势。AEB系统不能避免所有碰撞，但两个POMDP规划器可以避免所有碰撞。POMDP规划器和\nAEB系统的联合实施可以更快地通过障碍物。平均速度更高，但会触发四次紧急制动干预。\n4.2自动驾驶中对于紧急避让的决策：\n在自动驾驶避免紧急情况的情况下，需要以下模块进行决策:\n环境感知模块:用于获取环境信息，包括:车辆位置参数，障碍物与无人车之间的距离\n障碍物的形状信息，无人车与其他物体之间的相对距离，相对速度，运动角度和方向，障碍\n物的二维图像信息，收集车道线，红绿灯信息，无人车制动压力信息，车辆减速，加速信息，\n车辆质量信息，阀门位置信息、车速信息;根据障碍物的二维图像信息和雷达获得的点云数\n据，可以推断出物体的三维信息。\n危险识别模块:根据获取的环境信息，分析车辆与障碍物之间是否存在碰撞危险，输出\n碰撞判断信息，确定障碍物类别，输出障碍物类别信息。在判断车辆与障碍物是否会发生碰\n撞时，会根据获得的环境信息，计算无人驾驶车辆与障碍物之间的安全距离，判断是否发生\n碰撞事故。如果是，则继续跨越障碍的决策步骤。否则，车辆不需要激活套期保值，车辆按\n照原计划进行。在判断障碍物类别时，会根据获得的三维信息判断障碍物类别，并判断障碍\n物是否是易损物体。如果是，则判断障碍物所在车道为不可行的任务路径，输出碰撞判断信\n息和障碍物类别信息，进入智能决策步骤继续执行。否则，判断障碍物所在车道为可行任务\n路径，输出碰撞危险信息和障碍物类别信息，进入智能决策步骤继续执行。智能决策模块:根据获得的环境信息、碰撞判断信息和障碍物类别信息，构建可行任务\n池，分析可行任务的事故严重程度，确定最优可行任务。\n越障判断步骤:识别并判断障碍物是否为可跨越障碍:若障碍物为可跨越障碍，则车辆不\n需要启动规避危险，车辆按原计划行驶;如果没有，请进入确定障碍类别的步骤。\n4.3自动驾驶中对变道的决策：\n整个决策过程分为信息采集及信息传输，模拟计算和最终决策控制四个部分。在信息采\n集方面，通过对路面及路边障碍物、行车的车道边界，当前车辆方圆若干距离内的周边车辆\n的行驶速度、 相对本车的距离等外界的道路信息进行记录， 并且将此进行稍微粗糙的预处理，\n提取有效的数据部分传输给主控制模块等待下一步的操作；接下来执行模拟计算任务的就是\n主控制模块，其主要通过对于上述采集的信息与设定的安全信息进行模拟对照并且计算，得\n出相应情况下的各个关键数值后，与标准数值进行对比分析，进行变道效率、变道安全性、\n变道合理性等多方面的主成分分析考虑后得出最优解， 并且将此最优解转化程具体方案最终\n传输到控制执行模块最终实现变道的整个过程中对于车速和方向的把控。\n值得一提的是，整个变道的过程，信息采集模块采用的负反馈系统，而并不只是一次性\n的采集与决策，与路况进行实时互动，并且实时更新决策结果，利用动态的分析使得车辆可\n以随时观测路面情况的变化以预防突发状况的发生， 使得无人驾驶汽车在实现自主变道时候\n的安全性和有效性得到极大的提升。\n而实现上述功能的根本是在于决策系统和决策计算方法：此决策系统使得之前所述的四\n个模块，也就是用于最终进行真实操作车辆的主控制模块，对信息进行实时采集的信息采集\n模块， 对于指令是否最终执行的控制执行模块和最终与信息进行事实交互并进行负反馈调整\n的信息反馈模块，通过以上四个系统之间的相互交互，最终计算出本车需要的转向角角度和\n加速度。\n4.4自动驾驶中对红绿灯路口决策：\n对于红绿等路口决策， 采用基于交通信息检测领域中目前最为权威的无人驾驶的动态失\n事路口甄别方法。此技术首先针对待处理图像中ROI进行切割，并通过经验值对与所关心ROI\n（也即红绿灯高度相关区域）之外的东西进行过滤。其次，对标准的红绿灯模板进行建模作\n为对照组，并绘制二维直方图作为HSV色彩空间的标准直方图方便进一步对比。对的待处理\n图像，设置反转块投影进行搜索，基于位置，在YCBCR空间中对颜色指标进行进一步甄别。\n然后分别获得红、绿区域的坐标位置并进行比较，根据红、绿的位置信息和智能车辆的车道\n信息决定是否行驶。本发明可以实时动态地检测红绿灯信息，并应用于无人驾驶车辆。要解决这个问题，首先必须能够从复杂的背景图片中获得红绿灯的大致位置，然后判断\n红绿灯的具体信息。目前，无人驾驶车辆的红绿灯识别方法主要包括神经网络等方法。该方\n法具有一定的检测精度，但受样品特性的影响。如果只有样本足以代表问题的特征，效果是\n明显的，而且样本的数量要适当，所以样本的选择就显得尤为重要。\n对交通灯dst_Cut2的位置进行颜色识别。以上两步已经得到了红绿灯的准确位置，下一\n步就是识别颜色了。RGB空间也可用于颜色识别，但受光照等环境影响较大，因此转换到\nYCBCR空间的时间短，识别准确。提取符合交通灯颜色的cr通道值，对符合条件的区域进行\n二值化处理，得到绿灯二值化图bw_green和红灯二值化图bw_red。这样就可以对交通灯进\n行准确的颜色识别了。最后一步就是决策算法做出对应的正确的决策。\n5自动驾驶汽车决策算法的研究前景\n5.1 尚可提升之处\n当前的决策算法不能完全保证所有道路问题的安全， 决策模块不仅需要考虑汽车的安全\n性和舒适性，以确保尽快到达目标位置，还需要确保乘客在复杂路况下的安全，甚至恶意损\n害乘客的利益和生命。因此，一方面，决策模块需要对驾驶计划进行长期规划；另一方面，\n它需要预测周围车辆和行人的行为。此外，无人驾驶中的决策模块对安全性和可靠性有严格\n的要求。现有的无人决策模块通常是根据规则构建的。虽然基于规则的构建可以处理大多数\n驾驶情况，但基于规则的决策系统不可能列举驾驶过程中可能发生的各种意外情况。我们需\n要一个自适应系统来处理驾驶环境中的各种紧急情况， 因此这也是决策算法需要改进的地方。\n只有乘客的人身安全得到保障，无人驾驶才会被更多人接受。\n道路的安全问题往往并不是我们在进行建模的时候就可以考虑完全的。 无人驾驶在未来\n如果想要取得更加长足的进步， 就要更加综合的只有将环境的不确定性纳入到决策控制算法\n中，才能更好的保证乘客的人身安全。\n5.2 发展的有利之处\n首先，无人驾驶汽车有政策倾斜。2022年1月，《现代综合交通运输体系建设“十四\n五”规划》提出，要合理发展自动驾驶和车路协同等出行服务，鼓励在港口、物流园区等限\n制区开展自动驾驶测试应用，推进智能公交、智能停车、智能安检等发展。由于政策利好，\n无人驾驶行业有望迅速发展。第二个方面是5G技术的加入。 随着技术的发展， 无人驾驶的决策也越来越精准， 加入5G\n算法以后，无人驾驶也会越来越兴起。同时其也会和新能源技术产生配合，更加速无人驾驶\n技术的发展。\n第三个方面是应用物流推动无人驾驶产业的发展。目前，对“物流无人”的需求不断增\n加。展望未来，物流领域的无人驾驶应用将进一步拓展，无人物流可以有效的避免病毒的传\n播，可以有效地保护居民的生活物品的供给，所以物流会不断推动无人驾驶产业的发展。\n无人驾驶汽车的产业化如今还存在着各种问题， 但无人驾驶汽车本质上是建立在车辆主动安\n全技术和智能技术逐步升级的基础上。 未来的交通系统可能会发生我们现在无法想象的变化，\n目前无人驾驶技术的发展重点仍然是辅助驾驶，从而提高人工驾驶的安全性。\n"}, {"role": "assistant", "content": "最终打分：9 (范围0-10分)\n1. 结构完整性得分：9, 占比20%\n2. 逻辑清晰度得分：9, 占比20%\n3. 语言连贯性得分：9.5, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：10, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "人 工 智 能 导 论  项 目 报 告  \n一、项目信息  \n组 号 ： \n组 长 ：匡亮（） 组 员 ：龚凌琥（ ），李浩宇（ ）\n项 目 类 型 ：project\n主 题 ：（7.）智能遥感 —— 风力发电装置数量统计与分析\n特 别 感 谢 ：曾振中教授和江鑫学长对我们的指导与支持。感谢你们提供了这个\n选题和宝贵的训练数据，以及对我们使用各类工具时的指导。我们在做这个项目\n的过程中学到了很多！\n成 果 简 述 ：我们训练出了一个可以在遥感图像上识别风力发电装置（即风车）\n的 YOLO v3 模型，并在甘肃酒泉最大的风电场的遥感图像上标识出了所有风\n车。\n二、背景简介  \n风能是公认的清洁无公害的可再生能源，近年在我国电源结构中的比例快速增\n长，已经成为我国的第三大电能来源。获取风力电场占地、分布数据，在风力电\n场监管中十分重要。\n国产遥感技术的发展，使得卫星遥感影像的空间分辨率不断提高，从而让利用卫\n星遥感影像，结合目标检测算法，进行大区域内的的风电场分布统计变得快速准\n确，可以解决人工勘测统计耗时长、难度大的、易引入人工误差的问题。\n我们拥有一张甘肃酒泉最大的风电场的卫星遥感图像：\n这个图像有  5GB 大，在  QGIS 软件里可以放大得到各个部分的细节。我们有一些较小的风电场图像，人为地用  shp 文件标记出了上面的风机位置。现\n在我们希望通过人工智能的方法识别出这张图上所有的风机，并生成一个  shp 文\n件以便直观地观察风机密度。\n三、项目过程  \n1. 确 定 使 用  YOLO v3 模 型  \n和曾教授、江学长交流过后，我们准备采用  YOLO v3 进行目标检测。  YOLO v3\n具有高速度快、准确率高的特点，适合用于大量遥感影像进行目标检测。\n2. 尝 试 使 用  Gluon CV 框 架  \n最开始我们选用的是  Gluon CV 框架。这个框架具有新颖、便捷的特点，且有一\n些中文的视频资料可以参考。我们在第一次和导师汇报项目进度时，使用的仍然\n是这个框架。\n江鑫学长给我们提供了一些风机的  tif 遥感图像和记录坐标的  shp 文件。在学习\n了相关知识之后，我们用  python 实现了读入  tif 和  shp 文件，并用  PIL 切成一\n个个风机的小块图像，由  png 格式转成  jpg 格式，制作成了  Gluon CV 要求的数\n据集格式。\n但在深入研究后，我们发现  Gluon CV 主要支持的是使用他们预训练的模型，或\n使用  Pascal VOC 数据集进行训练，使得检测常 见、简单的物品变得十分简单，\n而并不倾向于支持用户自己训练数据集识别特定的物体（风车）。好在数据集还\n是可以用的。于是我们换成了  MMDetection 框架。\n3. 使 用  MMDetection 框 架  \n用 MMDetection 框架训练后，我们的模型可以 在测试集上正确地标识出风车\n了。\n接下来，我们要设法读入甘肃酒泉风电场遥感图像并进行标注。这个  tif 文件的\n大小有  5GB ，因此我们需要将它切成小块，分别检测后，生成对应原图的  shp\n文件。这一步我们遇到了很多问题。首先，为了避免目标被切开而无法识别，切割图像\n时要留出部分的重叠空间；然而留出重叠空间后，一些对象又会被多次识别。于\n是我们将模型识别出的坐标结果暂存，经过多次阈值的调整，区分开了距离较近\n的目标和被多次识别的目标。其次，从  shp 文件里读入信息比较简单，但写出一\n个 QGIS 能够识别的正确的  shp 文件并不容易。\n第一次生成的比较完整的结果如下，此时还有些许的遗漏或重复。\n图：调整前 →调整后\n最终写好的  shp 文件在  tif 图像上的投影效果如 下：四、总结语  \n根据统计结果进行初步判断，这张图上共有 约 1500 个风机对象。\n当然，我们的结果并非尽善尽美。由于对框架了解不够，我们有不少时间浪费在\n了不合适的框架上；又由于训练用的数据和次数较少，我们的模型有小概率误判\n或漏判。最终在教授学长的耐心指导、几位组员的通力合作下，还是基本完成了\n我们的目标。"}, {"role": "assistant", "content": "最终打分：7.5 (范围0-10分)\n1. 结构完整性得分：8, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：3, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "AI技术在人机接口智能假肢上的原理与实现\n彭彦兮计算机科学与工程系南方科技大学深圳广东，刘一予计算机科学与工程系南方科技大学深圳广东\n摘要：对智能假肢进行了基本的概念解读和结构总结，总结了智能假肢的基本定义和功能结构，分为\n“信号源”，“算法单元”，“运动单元”三个模块；从人工智能和脑机接口的研究角度出发，对相关实际案\n例进行调研分析，对现有文献进行了较为系统的总结提炼，得出了目前行业内对智能假肢的主要实现方式\n与处理办法：信号源方面，主要应用在肌电信号与脑电信号，算法实现方面，主要采用深度学习中的卷积\n神经网络和循环神经网络，并对智能假肢在信号采集输入和算法实现方面的相关研究进行了较为详细的总\n结与提炼。\n关键词：智能假肢，人工智能，脑机接口，肌电信号，脑电信号，深度学习\nThePrincipleandImplementationofAITechnologyinHuman-\nMachineInterfaceIntelligentProsthesis\nPengYanxiandLiuYiyu\n1.CSdept,SouthernUniversityofScienceandTechnology,CityShenzhen,\n2.CSdept,SouthernUniversityofScienceandTechnology,CityShenzhen\nAbstractThebasicconceptandstructureofintelligentprosthesisareinterpretedandsummarized,andthebasic\ndefinitionandfunctionalstructureofintelligentprosthesisaresummarized,whicharedividedintothreemodules:\n\"signalsource\",\"algorithmunit\"and\"motionunit\".Startingfromtheresearchperspectiveofartificial\nintelligenceandbrain-computerinterface,therelevantpracticalcaseswereinvestigatedandanalyzed,the\nexistingliteraturewassystematicallysummarizedandrefined,andthemainrealizationandprocessingmethods\nofintelligentprostheticsintheindustrywereobtained:Intermsofsignalsource,itismainlyappliedto\nelectromyographicsignalandEEGsignal.Intermsofalgorithmimplementation,itmainlyadoptsconvolutional\nneuralnetworkandcyclicneuralnetworkindeeplearning.Moreover,itsummarizesandsummarizesindetailthe\nrelevantresearchesofintelligentprosthesisinsignalacquisitioninputandalgorithmimplementation.\nKeywordsSmartprosthetics,artificialintelligence,brain-computerinterfaces,electromyography,EEG,deep\nlearning\n1.智能假肢的定义及基本结构\n1.1智能假肢的定义\n智能假肢，作为一种生物电子装置，是一种\n利用现代科技（如生物电子学技术）为有肢体障碍的患者将人体的神经系统与假肢装置相互连接\n起来，形成信息的双向传递，从而代替人体缺失\n躯体功能的智能设备。\n从功能上的角度来说，智能假肢区别于传统假肢的最大特点，就是智能假肢在基于传统假肢\n的基本功能外，还要求假体本身能够与人体交换\n信号，通过语音控制还是神经控制等信号输入手\n段，以仿生的方式代替失去的肢体完成动作，代\n表例子便是各种具有“感觉”的双向交互假肢。\n1.2智能假肢的基本结构\n对于一副完整的智能假肢来说，其构成应该\n满足三个部分，第一是在假肢与人体之间检测并\n传导信号输入输出的“信号源”；第二是“算法单\n元”，这一部分做整个智能假肢的核心单位，其主\n要功能包括实现对信号源传出传入的数据进行处\n理与解算，将传入假肢的信息解读成一系列运动\n信号，以及准确识别穿戴者的各种输入信号中所\n传递的运动意图；第三是“运动单元”，这一部分\n包括了马达，轮轴，等一系列机械结构，在接收\n算法单元解算后输出的运动信号后，实现对躯干\n机体的运动模拟，从而实现诸如抓取，移动，旋\n转，弯曲等一系列动作。\n（图1假肢工作原理图在运动单元方面以捉握和放开两个动作作为参考）[1]\n以上图为例，其中“捉握与放手思想”属于\n穿戴者的原始数据，其与“脑电波采集”“模式选\n择对比”“重新采集”模块共同组成“信号源”单\n元；其中两个“控制算法”与“控制信号”，属于“算法单元”；其中“假肢控制”与“捉握目标物\n体”和“放开目标物体”属于智能假肢的“运动\n单元”。\n1.3智能假肢的相关核心技术\n1.3.1运动单元模块:假肢关节结构及调控方式\n目前智能假肢关节在机械结构上主要分为单\n轴式和多轴式。单轴式具有结构简单、重量轻、\n易加工组装的优点.而多轴结构也称为多连杆结构\n因此虽然其具有结构复杂、加工装配难度大、能\n量效率比低等方面的不足，但是仍然是目前各类\n智能假肢膝关节所采取的主要结构，尤其是其中\n结构相对简单的四连杆结构，目前被广泛采用。\n[2]\n从关节调控方式上，假肢膝关节分为全被动\n型、半主动型和动力型。\n传统的被动型假肢的主要作用还是以支撑作\n用为主，并没有设置智能化模块，只能在残肢运\n动的基础上进行摆动；而半主动型假肢虽然不能\n与被动型假肢相似，都不能提供助力，但是“改\n变关节内部气缸或者液压阀门的开度，来调节阻\n尼进而改变膝关节的摆动速度”[2]；除此之外的\n动力型假肢，可以实现对人体躯干的主动助力，\n具有更高水平的能量转化效率，是未来智能假肢\n的主要发展方向\n1.3.1运动单元模块：智能假肢的驱动控制\n目前智能假肢的控制系统主要分为三个层次，\n分别是感知层，转换成以及执行层。感知层作为\n最高位，对人体的运动意图进行识别和解析，转\n换层将运动意图输入相应的控制算法，而最后的\n执行层根据输出的相应参数与数据对马达电机等\n机械设施实现控制——这些符合“信号源”“算法\n单元”“运动单元”的整体分类。\n对于执行层中的驱动控制部分来说，目\n前所采取的主要控制方法包括有限状态机、无模\n型动态矩阵、迭代学习控制、摆动器阻抗控制等\n方法；对于智能假肢控制策略来说，主要以无模\n型的专家系统、无模型动态矩阵方法、无模型自适应动态规划等方式进行实现\n2.智能假肢的信号源\n2.1智能假肢使用的信号源类型\n目前智能假肢的信号源主要有以下几种：\n1）以机械运动作为信号源\n这种假肢的控制思想主要是利用患者的肌肉\n组织及残存肌肉组织的运动功能，通过传动装置\n触发相应的开关进行控制。截肢患者的残端肌肉\n及运动相关部位肌肉的收缩与舒张、肢体的机械\n运动（如抬升、下垂、外展、内收等）都可以作\n为假肢驱动的信号源。\n这类假肢主要应用于上肢假肢，其控制结构\n简单，造价低廉，因而受到了患者的欢迎。\n2）以声音触发作为信号源\n这种假肢采用声控技术，利用患者的声音信\n息，通过数字处理技术，将其转化为相应的控制\n指令。其线路简单，有发展多自由度假肢的潜力。\n3）以脑磁信号作为信号源\n肌肉与神经体内的电流会产生外部磁场，如\n心磁、脑磁等。这种生物磁信号理论上能够用作\n信号源来控制假肢。在脑磁控制的假肢的发展中，\n如何构造人机接口仍没有标准答案。此外由于脑\n磁的复杂性，人们对它的认识刚刚起步，对它的\n研究尚停留在脑科手术与病理评估等，将其用于\n控制假肢仍处于探索阶段。[3]\n4）以生物电信号作为信号源\n在智能假肢的控制方式中，能够按照人的意\n志控制假肢是一种最理想的方案。用意念控制假\n肢的理论基础即是利用人体的生物电信号。人通\n过神经控制身体活动，会产生生物电信号，这种\n信号中载有人的行为信息，能够直接反映人的意\n图。通过解码人体生物电信号，就能实现识别\n“意念”，让机器按人的想法工作。\n对于截肢患者来说，他们往往存在一种幻觉，\n认为能够感受到已经失去的肢体的存在，这被称\n为“幻肢感”。尽管“幻肢”并不存在，但患者试图运动自己的“幻肢”仍然能够触发神经电信号，\n因而可用这种神经电信号作为假肢的信号源。\n2.2利用生物电信号\n生物电信号根据提取位置的不同，可以分为\n不同类别。目前来说，最为广泛关注的生物电信\n号包括肌电（Electromyography，EMG）及脑电\n（Electroencephalography，EEG）。肌电信号\n（EMG）是众多肌纤维中运动单元动作电位在时间\n和空间上的叠加。而脑电信号（EEG）是脑神经组\n织的电生理活动在大脑皮层表面的总体反映。[4]\n2.1.1利用肌电信号（EMG）\n1）肌电控制假肢的发展概况\n基于肌电信号的智能假肢在很长一段时间内\n占研究主流。1948年，在德国的ReiholdReiter\n的研究下，世界上第一个肌电控制假肢问世，实\n现了人体神经系统对假肢的控制。此后，单自由\n度肌电控制假肢一直占据假肢手的主流。\n进入21世纪，随着生机电一体化技术的产生\n和发展，假肢进入了第二次技术革命。肌电控制\n假肢再次迅猛发展。在一系列研发计划的推动下，\n新一代智能肌电假肢问世。2007年，英国Touch\nBionics公司成功研制了世界上首个各手指可独立\n运动的多自由度假肢i-Limb。2010年与2012年，\n德国Vincent公司和英国RSLSteeper公司分别\n成功推出了Vincent和Bebionic假肢手。2014年，\n在美国“革命性假肢”计划下，经约翰霍普金斯\n大学等30多家研究机构参与的“卢克之手”研制\n成功，并于2014年通过美国食品药品管理局认证，\n于2016年进入临床应用。\n2）对肌电控制假肢的评价\n基于肌电信号的智能假肢研究相对成熟，但\n有相应局限。\nEMG信号的质量受限制。肌电控制假肢的信号\n提取原理是通过贴在肌肉表面的电极片感受肌肉\n收缩时产生的电信号，进而通过电信号强弱与假肢关节运动量的对应关系来控制假肢连续运动，\n或者当电信号超过一定阈值时产生关节动作。由\n于这是神经电信号在人体运动末端的体现，其信\n号较弱、噪音较大、易受到环境等外界因素干扰。\n此外，对于EMG信号的采集一般倾向于无创式。\n例如，利用表面肌电信号（SurfaceEMG，sEMG）\n就是通过无创采集。其受到众多研究者青睐，有\n较为广泛的应用。然而，这样的无创采集同时也\n会限制其信号质量。\n肌电控制假肢的显著局限是灵活性不足。人\n类的手共有27个自由度（每个手指中有4个，拇\n指上5个，手腕上6个），而假肢却难以实现。其\n原因即是EMG信号的稳定性与精度无法支持如此\n高的自由度。\n一方面，随着自由度的升高，智能假肢在控\n制上的算法模型将会极其复杂，而质量受限的肌\n电信号无法支持其运算结果的精确性；另一方面，\n假肢控制模型的复杂化也会给假肢的使用者带来\n困难。对于残障人士来说，肌电控制假肢并非能\n安装即用，还要进行较长时间的训练。而训练的\n过程，即是建立肌电信号与假肢关节运动的对应\n关系。随着肌电假肢控制模型的复杂性上升，其\n所需的用来训练模型的样本数据量就更大，其模\n型训练的时间就越长、难度就越大，这就给假肢\n的使用者带来了较大困难。\n综上原因，实际应用中的肌电假肢，大都仍\n然是单自由度，运动功能相对受限。\n2.1.2利用脑电信号（EEG）\n1）脑电控制假肢的发展概况\n脑电信号一般通过脑机接口（BCI）进行提取。\nBCI是一种在人脑和外部设备之间建立起来的直接\n通讯通道。人们常把它用于研究、替代、辅助、\n增强、或修复人的认知功能或感觉-运动功能。\n1973年，JacquesVidal首次使用BCI这一概念\n来描述任何能够产生关于脑功能详细信息的计算\n机系统。[5]第一次BCI国际学术会议对BCI做出\n了权威定义：是一种不依赖于大脑外周神经与肌\n肉正常输出通道的通讯控制系统。[6]它通过采集并分析人的脑电信号，在人脑与计算机之间建立\n起直接的交流和控制通道，从而实现不借助语言\n或肢体动作，直接通过大脑来与外界设备交互。\n早在上世纪90年代末，纽约州立大学的科学\n家就通过实验证实，利用脑皮层神经元集合信号\n可以控制机械手臂运动。而近期，随着Neuralink\n在2019年的一项实验中在小鼠身上高效实现了脑\n机接口，实现了超大带宽和无痛手术两个技术上\n的突破；又在2021年的一项实验中实现了让一只\n猕猴完全使用意念玩游戏，脑机接口再次成为热\n点。\n脑机接口是当下热门的研究领域，其发展潜\n能很大。通过大脑信号来控制活动的这一可能性\n吸引了全球各领域广泛的重视和研究。而其潜在\n的价值和意义也在不断推动BCI研究这一新兴领\n域的发展。近年来对BCI相关领域的研究呈快速\n增长态势。[7]\n2）基于脑电信号的脑机接口的实现方式\n基于EEG的BCI系统有不同的实现方式。在\n目前的研究中占主流的是基于运动想象（motor\nimage，MI）电位的BCI、基于事件相关电位P300\n的BCI，和基于稳态视觉诱发电位（steady-state\nvisualevokedpotential，SSVEP）的BCI等。\n基于MI的BCI以感觉运动节律信号作为输入，\n[8]该信号相伴运动或感知运动产生，依据频率\n不同可分为不同类型的节律。这些不同节律在进\n行运动准备、想象或执行运动时会有特定的变化\n规律。通过观测这些电位变化，就能够实现解读\n人脑做出的运动想象或运动指令，进而控制假肢。\n基于事件相关电位P300的BCI利用EEG在受\n刺激后产生的电位偏移现象进行控制。当刺激事\n件发生，就会触发P300电位。[9]利用视觉刺激\n或触觉刺激等都可引发该电位。典型案例是BCI\n文字输入系统。受试者只需注视虚拟键盘就可以\n完成打字。因为注视这些字符可以诱发相应的脑\n电刺激。\nSSVEP由持续波动的视觉刺激产生。常见的是\n视觉闪烁刺激。例如，当屏幕上的目标以不同频率闪烁时，受试者只需注视闪烁的目标，就可以\n在大脑的枕叶区测到同样频率的EEG信号，这一\n信号即是SSVEP。[10]\n3）对脑电控制假肢的评价\n脑电信号相较肌电信号更为复杂，其包含信\n息也更丰富。脑电信号的另一个优势在于它可以\n达到毫秒级别的高时间分辨率，可以较好的进行\n实时监测和在线传输。作为运动意图的直接反映，\n其具有较强实时性的特点意味着脑电控制能够对\n运动意图有更为迅速的识别能力。\n目前，脑机接口的主要手段包含植入式和非\n植入式两类。植入式脑机接口优点是信号采集效\n果更好，但有植入手术风险。而非植入式脑机接\n口目前较多采用的是头部表皮脑电图信号，实现\n脑和外部世界的简单通讯，缺点是信号带宽有限\n且抗干扰能力差。\n3.智能假肢的智能体现\n3.1脑机接口的运动想象实现\n脑机接口（BCI）的运动想象（MI）实现的思\n想是通过想象进行活动。在BCI假肢的实现上，\n这是利用意念控制假肢最为理想的方式。2006年，\n一项发表在Nature上的研究验证了通过想象进行\n活动的可行性。[11]研究人员做了一款神经运动假肢（NMP），旨在通过将大脑中与运动相关的信号\n传递到外部效应器，取代或恢复瘫痪患者的大部\n分运动功能。\n他们的研究探讨了通过MI控制假肢的操作层\n面的问题，即如何界定想象行为与实际行为的界\n限，如何区分EEG中的非指令性运动想象与实际\n运动指令。受试者在80秒内被要求想象执行并实\n际执行一些动作，然后其神经活动被记录。研究\n分析了其神经元在宏观层面上的激活率与运动想\n象在时间与空间上的联系。\n研究发现不同类型的运动想象在神经元的激\n活程度、激活模式、感觉运动节律上有显著且微\n妙的不同。这对BCI的特征提取、特征信号分类\n识别以及最终“解码”来自人脑意念的行为意图\n提供了思路。\n（图2在80秒内，MN被要求想象执行并实际执行一些动作；图a.神经元的激活率随\n时间变化的关系，箭头表示运动指示，小条码表示实际执行；图b.完成某一特定动作，从接收指示到实际执行过程中神经元的活跃程度）\n该研究成功实现了将采集到的脑电信号转化为运动决策。通过对安装在受试者脑门部位的皮\n层传感器记录到的神经信号的解码处理，研究人\n员模拟得到一个“神经光标”，能够反应受试者的\n意图。受试者得以实现打开模拟电子邮件、操作\n设备等，甚至是在交谈时完成这些动作，即能够\n一心多用。“光标控制实验”得出，光标的移动\n是在想象与预期动作的指导下进行的。\n3.2假肢的感知反馈\n长期以来，感知反馈一直是假肢领域的一个\n瓶颈。人类肢体在运动过程中会时刻接收外界信\n息，并回传到中枢神经系统形成感觉反馈，让肢\n体产生本体感觉。这使人能够在无意识下控制自\n己的肢体、感受到自己肢体的状态。而对于假肢\n来说，如果无法给使用者提供本体感觉，假肢就\n仍然是异物，无法完成真正的“人机融合”，很难\n真正配合使用者的活动。\n2015年，上海交通大学的STJU神经康复工程\n实验室在一项研究中，利用经皮神经电刺激\n（TENS）来描述前臂截肢者残肢皮肤上的诱发触\n感现象（ETS），证实了以某种特定方式刺激残肢\n部位的皮肤就可以唤起已经缺失肢体部位的触觉。\n[12]\n研究人员在11名前臂截肢者的残肢皮肤上鉴\n定了ETS的投影手指图（PFM），并使用TENS比较\n了9名前臂截肢者和8名健全人的ETS的知觉属\n性。通过调制电流振幅、脉冲宽度和双相矩形电\n流刺激的频率，获得每个手指投影区域的最敏感点的感知阈值轮廓。\n研究人员对5例前臂截肢患者的PFM远期稳\n定性和ETS知觉阈值进行了为期11个月的监测，\n发现在残肢长度相对较长的前臂截肢者的残肢皮\n肤上，可以独立识别出五个手指特异性的投影区。\n其PFM的形状逐渐与远端截肢的手相似。并且，\n截肢者残端皮肤的PFM与正常人的正常皮肤都能\n唤起类似的触觉、压力、振动和疼痛等感觉模式。\n在感知反馈在假肢的实现上，稳定的PFM和ETS\n的感觉阈值能够支持非侵入性神经接口，从而稳\n定地实现将肢体的触觉信息从假手反馈给前臂截\n肢者。\n（图3图a.三种典型的截肢水平上肢；图b.确定与这些截肢相对应的PFMs\n的水平；图c.感觉水平检测，即通过机械触摸检测不同手指投射区域的感觉水平，不同区域有不同的触觉灵敏度）\n关于具有感知反馈的假肢是否能够显著提高对使用者的活动的配合程度，提高其活动能力，\n2021年匹兹堡大学的一项研究证实了使用触觉反\n馈可以大幅提高用户的活动速度和效率。因此，\n向大脑提供直接的触觉反馈可以显著增强用户对\n假肢的控制能力，提高假肢与人体的融合程度。[13]\n（图4使用与未使用感知反馈技术时的成功尝试率与尝试时间，ICMS即用于产生感知\n反馈的皮层微电流刺激）\n3.3神经肌肉骨骼假体\n2022年1月，NEJM发表了一项成果，报道了\n三位手臂截肢的瑞典患者，通过佩戴可以体验触\n摸感，同时受精神控制的手臂假肢，已正常生活\n了7年之久。[14]该项目综合使用了肌电控制、感\n知反馈等技术，实现了一款较为自然的假肢，是\n一个较为成功的案例。\n该假肢被称为“神经肌肉骨骼假体”，是一种\n骨锚式独立机械臂假肢。其植入物允许假手与植\n入上臂神经和肌肉的电极之间进行双向通信，并\n通过骨整合(骨细胞附着在人造表面而不形成纤维\n组织的过程)固定在肱骨上。该假肢使用植入的肌\n外电极获取的信号作为假体的控制源，使用了感\n知反馈技术，这种通过直接的神经刺激引起的感\n觉被描述为类似“笔尖触碰”的感觉。该假肢的\n使用不需要正式的训练，依赖于用户的直觉意图\n来激活假肢的运动和感觉反馈。\n在4名受试者中，除一名患者由于假体植入\n产生不良反应导致无法继续使用该假肢，另外三\n名患者在使用该假肢后幻肢疼痛逐渐减轻。随着\n时间的推移，患者辨别感觉强度细微变化的能力有所提高，其使用假肢的熟练度逐渐上升，对假\n肢的信任度也逐渐提高。\n4.智能假肢的算法实现\n4.1智能假肢算法概述\n目前，在智能假肢领域，涉及到人工智能的主\n要算法包括卷积神经网络（CNN）、循环神经网络\n（RNN）、支持向量机（SVM）、随机森林（RF）、多\n层感知机（MLP），其中应用最广效果相较而言较\n为突出的是深度学习中的卷积神经网络和递归神\n经网络两种算法。目前已经有研究团队表明，由\n于边缘计算的进步，深度学习人工智能可以有效\n地部署在便携式，独立的系统之上。[15]\n4.2以循环神经网络为例\n循环神经网络具有良好的时序卷积特性，这\n使得它能够在时间与空间两个维度来对特征数据\n进行卷积，这较好的对应了现实中假肢动作在时\n空上的不同变化。\n关于循环神经网络在智能假肢上的实现，目\n前已经有许多团队提供了不同的应用思路，其中NormanDiPalo及其团队发布在\nTowardsDataScience上的文章表示，他利用脑电\n图作为信息源，试图从中解读大脑活动与脑电信\n号之间的关系。在此期间研究人员利用了特定手\n势所对应的脑电图模式，并以此设计神经网络。[16]\n脑电图（EEG），是一种基于前文脑电信号相关描\n述的基础上的一种非侵入式技术，它通过从患者\n头皮上固定的电极来获取并记录脑电压波动。在\n该项目中，研究者在头皮周围固定了32个电极，\n以500Hz的频率，用来记录脑电波的总体活动\n（图5利用EEG技术采集到的脑电波信号图）\n实验者在检测过程中，为患者提供不同的现\n实场景和主题，用于模拟现实环境对脑电的影响，\n并着重将手臂和手势动作与大脑活动给对应起来，\n并一一分类，并利用数据可视化的手段实现更具\n体的特征呈现和分类结果。最后设计神经网络的\n目标可以实现输入脑电图数据后，可以输出患者\n试图实现的多个可能动作的相应概率分布。其中\n实验者将所有可能的输出设置为0到1之间的\n值，并使用阈值来确定是否检测到动作。如果每\n一个动作都低于阈值，则认为没有动作产生。\n（图6固定在患者头皮除的32个电极分布位置）\n在数据采集完毕后，对时序数据进行数据可\n视化处理，其中纵轴代表时间，横轴代表不同电\n极，其目的是为了更直观地显示脑电信号在时间\n和空间上随手势动作所产生的不同信号。\n（图7可视化处理后所得到的EEG时序热图）\n在实验处理阶段，研究人员对原始数据进行\n预处理，目的是对EEG的高速数据变化频率进行\n筛选和降噪，提高分类输出的精度。在这个阶段，\n研究人员提供了“使用低通滤波器过滤数据”这\n种过滤手段，筛除了高频数据（在该项目中研究\n人员所选取的识别手势对应的数据频率多较低）。\n这有助于降低时间维度以及数据的相关性，使数\n据更加时间稀疏。\n在正式的循环神经网络设计阶段，该算法基\n于经过预处理后的数据，用卷积在时序热图上同时在时间与空间上进行加权求和，提取特征，并\n以此发现在电极信号与相关手势动作之间的联系\n和变化特征，解决了智能假肢中最核心的算法问\n题——准确判断识别用户动作意图。\n4.3以卷积神经网络为例\n关于卷积神经网络，其与循环神经网络最大\n的区别就在于其在时空卷积存在一定的缺陷，因\n为循环神经网络本身的特性使得它可以对数据进\n行时间上持续的卷积操作，从而达到时间和空间\n上卷积，更好的模拟现实情况的效果。\n然而卷积神经网络同样对处理智能假肢的运\n动意图识别也具有优势——来自美国明尼苏达州\n明尼阿波利斯市明尼苏达大学的研究团队就利用\n卷积神经网络对假肢信号进行了解读。\n（图8研究团队在患者小臂上用神经束内微电极采集肌电信号，以及信号\n传递和最后的效果模式图）\n（图9图A:基于python的三线程假肢实现流程图图B：基于深度学习的\n运动解码器架构，属于“算法单元”的核心流程示意图，可部署多达五个血毒学习模\n型，每个模型控制一个或多个手指的运动）\n在具体的对运动模式进行解码的过程中，该\nCNN算法基于对应过去一秒神经信号的队列的最新\n特征数据来进行深度学习的特征提取。五个深度\n学习模型分别对应解码一个或多个手指的运动。\n所有模型都具有相同的架构，但可以在不同的数\n据集上进行训练，以优化特定手指的性能。原因\n在于，虽然单个模型可以产生[5×1]预测矩阵，\n但是通常难以训练针对全部手指优化的单个模型。\n对于上图图B，输入矩阵的维数控制在\n[224*50]=[16个信道]*[14个提取特征]*[50个时\n间步长]。在50Hz的接受频率下的50个相应特征\n数据点对应过去一秒内的机体神经数据，并输出\n一个维数为[5*1]的输出矩阵，分别对应五个手指。\n因为与拇指相关联的控制信号在该特定截肢\n者的手指中具有最高的信号强度，所以拇指训练\n通常在1-2个周期内收敛，故不再需要更多地模\n型训练。最终的预测输出发送到手部控制器马达，\n用于操作机械手，并且通过蓝牙将输出数据发送\n到远程计算机，用于调试。\n初始卷积层标识数据输入的不同表示。随后\n的编码器-解码器利用门控循环单元（GRU）来表\n示在时间相关方面的运动解码。此时两个线性层\n对解码器的输出进行分析，并产生最终的输出矩\n阵。结束语\n综上，我们对智能假肢进行了较为全面的研\n究与资料整理，从智能假肢的定义到基本结构，\n从不同的信号源输入到具体的算法实现案例分析\n作了较为系统的调查和阐述。其中重点对信号源\n输入和AI算法实现进行了结合并针对目前已有的\n具体研究案例及成果的分析。从研究结果上来说，\n该综述在运动实现方面的调查（如具体机械结构，\n马达实现方式）等还存在一定缺陷，考虑到项目\n的主要目的是从人工智能以及脑机接口的方面对\n智能假肢进行研究，故这一部分的内容略显单薄。\n今后希望我们自身，或是有别的研究团队能够在\n该综述的基础之上，对智能假肢的信号源，算法\n实现以及运动单元三个角度进行更详细，更有深\n度的研究。\n\n"}, {"role": "assistant", "content": "最终打分：8.3 (范围0-10分)\n1. 结构完整性得分：8.5, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：7, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "百度无人车第 10 组 \n小组成员：王一舟，文启豪，朱家骆，王乙，李佳奇 \n \n1. 项目总体框架： \n1.1 寻找自动驾驶数据集： \n        数据集来源：阿里云 \n1.2 代码编写： \n \n \n2. 代码框架： \n2.1 文件读取： \n              通过 opencv库读取图片，以numpy 矩阵的形式保存 \n2.2 图像卷积： \n        将图像进行卷积，形成 10*10 像素矩阵 \n2.3 模型训练： \n        尝试使用自己编写的 BP算法进行训练，使用 sigmoid transfer function： \n                层数：4 层，2隐含层 \n                每层节点数：[300,30,10,3] \n                结果：不理想 \n        尝试使用外部 BP算法进行训练，使用 tanh transfer function： \n                层数：4 层，2隐含层 \n                每层节点数：[300,30,10,3] \n数据预处理 ---> 训练 <--(反复循环这两个步骤)--> 检测正确率 ---> 输出                 来源： https://www.bbsmax.com/A/LPdoMYXyJ3/  \n                结果：较为理想 \n        经过测试，learning rate 取 0.1，总共1000测试样例，将其打乱顺序后固定 900 个作\n为训练集，剩下100 用来计算准确率。如果准确率不足 90%则将测试集重新学习。每隔 10\n次学习后测量 error并记录。 \n2.5 效果可视化： \n        每次循环打出其正确率。使用 matplotlib进行结果可视化，将输出层 error 与学习次数\n的关系显示成图像。 \n3. 运行结果： \n \n \n可见输出层 error随着学习次数增加而减少。 \n \n \n循环了三次，准确率随之升高，最终准确率达92% \n4.训练模型代码： \n \n"}, {"role": "assistant", "content": "最终打分：6.5 (范围0-10分)\n1. 结构完整性得分：6, 占比20%\n2. 逻辑清晰度得分：6, 占比20%\n3. 语言连贯性得分：6.5, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：4, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "                                                                                                                                                            http://www.jsjkx.com\n                                                                                                                                                           DOI: 10.11896/jsjkx.xx                                                                                                         \n到稿日期：        返修日期：         \n基金项目：XX基金（基金号）；YY项目（项目号）\nThis work was supported by XX (No.) and YY (No.).    基金项目名称、编号的英文翻译确保无误。\n通信作者：姓名（E-mail）   深度学习技术在医学图像分析应用中的发展\n王韧立1  王茹奕1\n1 南方科技大学 广东省 深圳市 518055\n（12112321@mail.sustech.edu.cn）\n摘 要 医学图像分析，是指应用医学影像，人工智能，数值算法等技术，对医学图像提取关键信息等操\n作。影像技术在医学的诊断与治疗中正发挥着日益重要的作用。当前，基于深度学习的算法被广泛地运用\n于医学图像分析中。文章通过总结研究相关的文献资料，对医学图像分割与分析技术的发展过程进行了大\n致梳理，并通过对比分析卷积神经网络和transformer这两种主要图像分析方法，揭示了二者的适用场\n景 、优势与不足，同时探究了其近年来的发展趋势，预测了其未来可能的研究方向，对深度学习技术在\n医学图像分析应用中的发展进行了较为全面的描绘与总结。\n关键词： 医学图像分割；深度学习；卷积神经网络；Transformer；图像处理\nR445\nDevelopment of Deep Learning in Medical Image Analysis  \nWANG  Ren-li1 ，WANG Ru-yi1     \n1 Southern University of Science and Technology， Shenzhen  518055，China\nAbstract  Medical image analysis refers to the application of medical imaging, artificial intelligence, numerical algorithms and other \ntechnologies to extract key information from medical images. Imaging technology is playing an increasingly important role in the \ndiagnosis and treatment of medicine. At present, algorithms based on deep learning are widely used in medical image analysis. By \nsummarizing the relevant literature of the research, this paper roughly sorts out the development process of medical image segmentation \nand analysis technology, and reveals the applicable scenarios, advantages and disadvantages of convolutional neural network and \ntransformer by comparing and analyzing the two main image analysis methods, and explores its development trend in recent years, \npredicts its possible future research direction, and comprehensively describes and summarizes the development of deep learning \ntechnology in medical image analysis applications.\nKeywords Medical image segmentation, Deep learning, Convolutional neural networks, Transformer, Image processing\n \n1引言\n医学图像分析，是指应用医学影像，数字建\n模，人工智能，数值算法等学科交叉领域的技\n术，对医学图像进行分割，提取关键信息等操\n作。影像技术已经日益成为医疗机构疾病诊断手\n术计划和预后评估的重要手段之一[1]。精确分\n析图像的关键区域可以帮助医生对疾病作出正确\n的判断。在临床决策中，图像分割可为计算机辅助诊断和治疗提供可靠的依据，对于定量分析和\n手术指导也具有重要作用。与此同时快速和准确\n的图像分割，也是三维可视化、定量分析等后续\n环节开展之前需要进行的重要步骤，为图像引导\n手术、治疗评估和放疗计划等重要临床应用奠定\n了最根本的基础[2,3]。近年来，基于深度学习\n的自动分割算法取得了较为迅速的进展。在早期\n的医学图像分割研究中，采用的主要是直接进行Computer Science   计算机科学 Vol.x, No.y, Month.2020                                                   (偶数) 页码\n图像处理的一些经典方法，例如基于灰度的区域\n增长算法以及边界提取等，由于图像包含有众多\n不同的信息，传统的方法容易遇到计算缓慢，精\n度不高等的问题，随着技术手段的发展，计算机\n视 觉 相 关 理 论 的 丰 富 ， 诸 如 卷 积 技 术 ，\ntransformer以及一些融合了多种技术的先进方\n法都被运用于医学图像处理之中[4]。本文将大\n致梳理医学图像分割技术的发展过程，并对不同\n的图像处理技术进行对比分析，探究其近年来以\n及未来的发展方向，以期对医学图像的分析形成\n较为完整准确的认识。\n2深度学习在医学图像分析中的发展\n2.1传统图像处理方法\n在传统的医学图像分析领域，从感兴趣区域\n（ROI）提取特征并应用图像分类算法进行图像\n的识别与分类是一个重要方法[5]。图像分类算\n法，顾名思义，是一种根据输入图像描述的内容\n进行图像分类的方法。它有五个基本步骤，即图\n像数据集输入、图像预处理、特征提取、分类器\n训练和图像识别。\n2.1.1图像数据集的输入\n首先，将两类数据集，训练集和测试集，输\n入电脑。训练集中的图像已经被分类和标记，是\n供计算机学习数据特征的；而测试集则包含了大\n量各种类型的图像，用来测试计算机是否已经训\n练完毕。\n2.1.2图像预处理\n在输入图像数据集后，需要先对图像进行预\n处理，提高图像质量，减少一些不必要的干扰，\n使计算机能够更好地学习特征。由于计算机没有\n视觉，不能直接识别图像，因此有必要研究如何\n通过各种处理来提取有用的信息，以获得图像的 \n\"非图像 \"表示，如数值、向量和符号等。然\n后，我们就可以通过训练过程教会计算机如何理\n解这些特征，从而使计算机具备识别图像的能\n力。2.1.3特征提取\n在提取时，通常要根据各类图像的特点选择\n最佳的特征提取算法，下面就介绍两种典型的算\n法。\n2.1.3.1 LBP特征提取\nLBP利用了结构化思想提取图像分割的各个\n区域的特征，然后对各个区域进行统计，作为提\n取的最终图像特征。    早期的LBP算子定义在\n一个3*3的区域内，以这个区域的中心像素为标\n准值，将其与相邻的8个区域的灰度值进行比\n较，得到一个8位的二进制数，即得到该区域的\nLBP值，这个值也反映出这个区域的纹理信息。\n方法如下：首先，将待提取的图像划分为\n16*16个区域。然后，对于每个区域中的一个像\n素，通过上面提到的获取LBP值的方法获得该区\n域中心像素的LBP值。随后，根据该值计算出每\n个部分的直方图，即每个数字的出现次数，然后\n对直方图进行归一化处理。最后，将各部分得到\n的直方图衔接成一个特征向量，也就是整个图像\n的LBP纹理特征向量。\n2.1.3.2 HOG特征提取\nHOG是计算机视觉和图像处理中用于物体检\n测的一种特征描述符。它通过计算和统计图像局\n部区域的梯度方向直方图构成特征。它的主要思\n想是，图像中局部目标的外观和形状可以由梯度\n或边缘的方向密度分布很好地描述。步骤如下：\n第一， 对一幅图像进行灰度化处理。\n第二，用伽马校正法对输入图像的颜色空间进行\n归一化（normalize），目的是调整图像的对比\n度，减少图像中局部阴影和光照变化造成的影\n响，同时可以抑制噪声的干扰。\n第三，计算图像每个像素的梯度（包括大小和方\n向），主要目的是捕捉轮廓信息，同时进一步弱\n化光照的干扰。\n第四，将图像划分为小单元。\n第五，统计每个单元的梯度直方图（不同梯度的\n数量），形成每个单元的描述符。\n然后，每隔几个单元形成一个区块，将一个区块\n中所有单元的特征描述符串联起来，得到该区块\n的HOG特征描述符。作者，等：题目名称                                                                                （奇数）页码\n[键入文字]最后，图像的HOG特征描述符可以通过连接图像\n中所有块的HOG特征描述符得到。这就是最终的\n特征向量，可以用于分类。\n2.1.4   分类器训练\n首先，使用训练集中的图片数据，知道训练\n集中每个图片数据的分类标签。然后，根据这些\n已知前提，找到相应的判断函数或标准，设计判\n断函数模型。最后，根据训练集中的数据来确定\n函数模型中的参数。\n2.1.5图像识别\n分类器被训练完毕后，就可以对要分类的图\n像进行分类和识别了。\n2.2以 卷 积 神 经 网 络 （Convolutional \nNeural Networks, CNN）为基础的医学图像分\n析方法\n2.2.1卷积神经网络的介绍，定义及其由来\n卷积神经网络是用于分析视觉图像的一种常\n用的深度学习模型，是深度学习的代表算法之\n一，通常指代具有类似于人工神经网络系统结构\n的一种多层感知器，包含卷积计算且具有深度结\n构。卷积神经网络之灵感主要源于神经科学中对\n于人类视觉系统中的视觉皮层的研究。在视觉皮\n层中，感光细胞的种类不同，其对于特定区间的\n敏感程度也存在差异，卷积神经网络借鉴并发展\n了这一特点以更好地处理图像[6]。于1988年，\nWei Zhang 提出了第一个用于医学图像分析检测\n的二维卷积神经网络，为平移不变人工神经网络\n（SIANN）。在深度学习的相关理论于2006年被\n提出之后，卷积神经网络的表征学习能力得到了\n较为广泛的关注[7]。\n2.2.2卷积神经网络的基本结构\n一般而言，卷积神经网络的主要结构由数据\n输入层，卷积计算层，池化层以及全连接层这四\n个层级构成。其中卷积层可以进一步细分出卷积\n核，卷积层参数，激励函数等要素。在部分特殊的卷积神经网络中，各层级的顺序和包含关系可\n能会有所变化。原始输入的图像信息经过各层级\n的处理，提取出关键的特征。\n2.2.3卷积神经网络的结构分析和运算步骤\n卷积神经网络的输入层是能够处理多种维度\n的数据的，不过，在医学图像分析相关的卷积运\n算中，常见的输入数据类型为二维或三维，在该\n层的主要工作是对原始的二维输入图像进行简单\n的预处理，对于nxn的二维图像数据，输入依旧\n为nxn的二维神经元，而对于RGB格式的图像数\n据，输入则为3xnxn的三维神经元。在卷积层\n中，主要进行的工作是对输入层的数据进行特征\n提取，以二维为例，用卷积核按照既定的次序规\n律扫过输入数据构成的矩阵区域（若对边缘数据\n进行滤波，则可对输入值边缘进行零填充后泛卷\n积），在卷积核感受野内的输入数值，与卷积核\n上的权重进行矩阵元素的乘法并求和，再将得到\n的加权和叠加上偏差值，若数据线性不可分，则\n可通过激励函数的变换变为可分（如求平方值\n等）。在池化层，主要的工作是对上一步中得到\n的过多数据进行压缩和简化，常见的方法有将数\n据划分成若干个子区域后，对各子区域取平均值\n（平均池化）或最大值（最大池化）。在全连接\n层，主要的工作是对池化层的数据按照特征进行\n整合分类，可以通过全连接将卷积后得到的二维\n特征图转化为一维的向量。总体而言，卷积神经\n网络各步骤间环环相扣，从而对原始数据进行了\n简化和特征的提取[8]。一些不同的用于医学图\n像分割的卷积神经网络被广泛地提出，如\nAlexNet[9]，GoogleNet[10]，SequeezeNet[11]\n等，这些卷积模型主要的区别在于引入了不同数\n目的卷积和池化层数，并运用了相应的运算方\n式，从而产生了不尽相同的图像分割效果[12]。Computer Science   计算机科学 Vol.x, No.y, Month.2020                                                   (偶数) 页码\n2.2.4卷积神经网络在医学图像分割与分析中\n存在的不足和改进\n首先卷积神经网络中的全连接层无法处理不\n同大小的输入数据，具有全连接层的卷积神经网\n络并不能被很好地用于具有不同尺寸的图像分割\n的任务，因为在该任务中，关键要素出现的数目\n是不固定的，因此，输出层的长度不能是定值。\n一种对上述缺陷的改进方案是采用全卷积神经网\n络 （Fully Convolutional Network,FCN）\n[13]。全卷积网络与传统的卷积神经网络主要的\n区别在于前者中所有的层都为卷积层，用卷积网\n络替换了传统的全连接网络，因此，取消了全连\n接层对输入的神经元个数的限制，从而使得卷积\n层的输入可以是不同尺寸的图像。在全卷积操作\n的最后过程中，可以通过转置卷积层进行上采样\n将特征图的尺寸扩张为原始输入图像的尺寸大\n小，解决了卷积和池化导致尺寸变小的问题\n[12]。然而，传统的全卷积仍然存在一定的局限\n性，例如实际速度仍然较慢以及难以考虑全局信\n息的关联。同时，由于每个输出值对应的感受野\n是固定的，导致图像分割效果不足。在一些研究\n中提出了较为完善的全卷积模型，例如\nParseNet[13]，采用了全局均值池化等方法，从\n而在一定程度上考虑了全局信息的关联[12]。\n2.3以transformer为基础的图像处\n理方法\n2.3.1定义\nAshish Vaswani在2017年的《Attention \nIs All You Need》中提出相比于主流的基于\n复杂的循环、包含encoder和decoder的卷积\n神经网络，通常在encoder和decoder之间使\n用注意力机制（Attention）的模型性能更\n优，于是他彻底抛弃RNN，提出了完全基于注\n意力机制的transformer算法[14]。如图1所示，在以往的经典模型中一般使用\n两个RNN，一个作为编码器，一个作为解码器。\n编码器的作用是将输入数据编码成一个特征向\n量，解码器则将这个特征向量解码成预测结果。\n图1  \nFig.1  \n这个模型的问题是只将编码器的最后一个\n节点的结果进行了输出，但是对于一个序列长\n度特别长的特征来说，这种方式无疑将会遗忘\n大量的前面时间片的特征。既然如此，我们不\n如将每个时间片的输出都提供给解码器，给解\n码器提供更好的特征。那么解码器如何使用这\n些特征就是我们这里介绍的Attention的作\n用，如图2所示\n图2  \nFig.2  \n在这里，Attention是一个介于编码器和\n解码器之间的一个接口，用于将编码器的编码\n结果以一种更有效的方式传递给解码器。一个\n特别简单且有效的方式就是让解码器知道哪些\n特征重要，哪些特征不重要，即让解码器明白\n如何进行当前时间片的预测结果和输入编码的\n对齐，如图3所示。Attention模型学习了编\n码器和解码器的对齐方式，因此也被叫做对齐\n模型（Alignment Model）。作者，等：题目名称                                                                                （奇数）页码\n[键入文字]\n图3  \nFig.3  \n而在2020年Alexey Dosovitskiy 提出了\n将Transformer直接应用于图像领域，进行图\n像分类任务，而不修改Transformer架构，也\n不使用CNN的思路。其主要想法是将image将\n图像分割成大小一致的patch小块，并将这些\n块转化为线性嵌入序列，作为Transformer的\n输入。在patch前加入一个class表示，对应\n全局信息（图片分类），进行最终的预测,如\n图4所示[15]。\n图4 \nFig.4  \n2.3.2  attention算法步骤[16]\n步骤一，生成编码节点。将输入数据依次输入到\nRNN中，得到编码器每个时间片的隐层状态的编\n码结果，并将编码器的最后一个输出作为解码器\n的第一个状态的输入（decoder hidden \nstate）。步骤二，为每个编码器的隐层状态计算一个得\n分。使用当前编码器的当前时间片的隐层状态和\n解码器的隐层状态计算一个得分，得分的计算方\n式有多种，如点乘操作等。\n步骤三: 使用softmax对得分进行归一化。将\nsoftmax作用到步骤 2得到的得分上，得到和为\n1的分数。\n步骤四: 使用分数对隐层状态进行加权。将上述\n的分数与隐层状态进行点乘操作，得到加权之后\n的特征，这个特征也叫做对齐特征（Alignment \nVector）或者注意力特征（Attention \nVector）。\n步骤五: 对特征向量进行求和。这一步是将加权\n之后的特征进行加和，得到最终的编码器的特征\n向量。\n步骤六: 将特征向量应用的解码器。最后一步是\n将含有Attention的编码器编码的结果提供给解\n码器进行解码，注意每个时间片的Attention的\n结果会随着decoder hidden state的改变而更\n改。\n3对于医学图像分割中各个方法的比\n较分析\n3.1对比对象\n     卷积神经网络以及transformer都是医学\n图像分析中得到广泛运用的模型，两者有许多的\n共同之处，但在算法和效果上存在明显的差异\n性，接下来将着重对这两者的异同点进行比较分\n析。\n3.2产生不同的方面和内容\n3.2.1参数和数据规模\n卷积神经网络所学习的参数主要是静态参数\n在经过一定数量的学习后即可保持不变，而\nComputer Science   计算机科学 Vol.x, No.y, Month.2020                                                   (偶数) 页码\ntransformer所具有的参数是动态的，可以随着\n数据量的增加而不断调整。随着计算机视觉相关\n技术的发展，需要处理的数据规模不断扩大，一\n般而言，大多数的卷积神经网络模型更适用于监\n督性学习，在面对小规模数据时即可发挥作用，\n不过在面对海量数据时，其适配能力并不突出，\n与此同时，根据Dosovitskiy等人的研究可知，\ntransformer由于需要对空间关系进行预训练学\n习，在面对大数据时具有更好的适配能力，经过\n适当的优化，随着数据量的增加，其模型的表现\n更好，优势更加突出[17]。不过，transformer\n无法直接利用原始图像尺度，平移不变性，等先\n验知识，依赖后期的学习，在数据集较小的情况\n下，适用性弱于卷积神经网络。\n3.2.2关键点和自注意力机制\n对于卷积神经网络而言，由于卷积核上的权\n重是被输入值共享的，因此卷积能够很好地捕获\n相邻区域的特征，且具有平移不变性。同时，在\n卷积的处理过程中，相邻的数据点之间被更加紧\n密地关联在了一起，从而具有高度的局部性。而\ntransformer具有的多头自注意力机制，使其能\n够高效地处理长程依赖关系，更多地学习到了所\n有特征之间的相互关系，不完全局限于分散的数\n据本身，而是将局部的信息推广到了全局的信\n息，具备更加优越的普适性[18,19]。如图5所\n示为一项研究中分别运用卷积神经网络模型和\ntransformer模型分别对于肝脏血管的图像进行\n分析运算后的效果[18]。通过图片可以注意到，\n由于卷积神经网络的局部性，难以关联全局信\n息，部分原始图像中连结为一个整体的部分被错\n误地断开了，而以Visual Transformer 为基础\n的输出图像，这些部分仍然保持连结，并没有出\n现CNN中出现的异常断开的问题，正确地反映了\n全局依赖的关系。图5\nFig.5  \n3.2.3层级之间的相似程度\nMaithra Raghu等研究者对卷积神经网络和\nVisual Transformer中 的 一 些 关 键 层 级 的\nCentered Kernel Alignment (CKA)进行了检\n验，用以大致研究在随机初始化和不同程度的训\n练情况下，两种神经网络模型各自的各个层级间\n的关联程度[20]。该研究表明，在底层（接近输\n入的层级）中，两者表征的内容十分相似，然\n而，由于两个模型基于表征截然不同的处理方\n式，经过层层的运算，最终上层（接近输出的层\n级）中的表征具有相当大的差异[20]。同时，研\n究人员还发现，Visual Transformer中，数据\n从底层到达高层的相似程度要普遍高于卷积神经\n网络[20]。经过其分析，产生这种现象的原因主\n要在于Visual Transformer 中的跳跃连接结构\n较为成功地保护了表征信息从底层到高层的传\n递，实验表明，若撤去特定步骤上的跳跃连接结\n构，相应的表征信息就无法得到传递[20]。据\n此，该项研究的结果表明，对于更高层级的表征\n而言，transformer相对于常见的卷积神经网络\n（ResNet）能够更为细致地保留局部空间位置信\n息。实验中Vit与ResNet的结果的对比图如下\n作者，等：题目名称                                                                                （奇数）页码\n[键入文字]\n图所示[20]。\n图6  \nFig.6  \n4未来发展方向\n随着人们对于Transformer的研究不断深\n入，其优势也不断展现，不断有研究者发现\ntransformer在性能与理解自然语言等方面的优\n势。这不经令人疑惑：在未来transformer是否\n能完全取代CNN。\n其实，Transformer 和 CNN二者可能并非\n取代和被取代的关系，而是互相融合，取长补\n短。\n目前Transformer仍有许多问题需要解决，\n首先，transformer模型的参数量一般比较大，\n在训练和推理的计算成本较高，在CV中还需要\n更多的探索；其次，transformer需要使用大量\n的训练数据进行预训练，而在某些领域，特别是\n本文关注的医学图像领域，数据量较小，如何高\n效地利用数据仍是一个需要解决的问题；然后，\ntransformer对输入图像的处理不太符合直觉，\n其理论原因也还不太清楚，因此还需要更多的改\n进与理论分析[21]。\n由于 CNN 和 Transformer 各有优势和不\n足，二者融合的做法出现在很多 Transformer \n的跨界论文中。\nCNN 网络在提取底层特征和视觉结构方面有\n比较大的优势。这些底层特征构成了在 patch\n上的关键点、线和一些基本的图像结构。这些底\n层特征具有明显的几何特性，往往关注诸如平移、旋转等变换下的一致性或者说是共变性。\nCNN 网络在处理这类共变性时是很自然的选择。\n但当我们检测得到这些基本视觉要素后，高层的\n视觉语义信息往往更关注这些要素之间如何关联\n在一起进而构成一个物体，以及物体与物体之间\n的空间位置关系如何构成一个场景，这些是我们\n更加关心的。目前来看，transformer 在处理这\n些要素之间的关系上更自然也更有效[22]。但\n是，反过来说，如果全部将 CV 任务中的 CNN \n换成 Transformer，我们会遇到很多问题，比如\n计算量、内存占用量过大等。\n总之二者的结合还有很大的探索空间，现有\n的 Visual Transformer 都还是将原有的 \nTransformer 结构套到视觉任务做了一些初步探\n索，未来针对 CV 的特性设计更适配视觉特性的 \nTransformer 将会带来更好的性能提升；而且现\n有的 Visual Transformer 一般是一个模型做单\n个任务，近来有一些模型可以单模型做多任务，\n未来可以尝试有一个世界模型，处理所有任务。\n5总结\n医学图像分析是一个不断发展进步的过程，\n对医学图像的处理涉及许多学科的交叉领域，相\n关的算法与模型也在不断地更新与迭代，层出不\n穷。本文主要介绍了医学图像分析中算法的发展\n过程，并着重对于卷积神经网络以及以Visual \nTransformer为基础的图像处理方法进行了研究\n和对比。其中，卷积神经网络借鉴了视觉系统中\n的视觉皮层，包含卷积计算，具有深度结构，并\n通过替换全连接层为卷积层的全卷积模型，实现\n了对任意尺寸的输入图像进行处理，再通过上取\n样的方式，还原输出为原始输入相同的尺寸，实\n现了图像的分析。Transformer由Encoder和\nDecoder两个部分组成，利用了由多个自注意力\n层构成的多头自注意力机制，有效提取和记忆了\n长距离的信息，减少了计算量并大大提高了并行\n效率，在医学图像的分析中发挥了重要的作用。Computer Science   计算机科学 Vol.x, No.y, Month.2020                                                   (偶数) 页码\n卷积神经网络和transformer在图像处理上具有\n较大的算法区别和各自更适用的情况，由于充分\n利用了平移不变性和局部特征等先验条件，卷积\n神经网络在数据集较小的情况下具备优势，效率\n更高；而由于多头自注意力机制下处理长程依赖\n关系的能力以及需要对足够数据进行预学习的特\n点，transformer在面对更大的数据规模时具有\n很好的适配性。同时，在多层级的情况下，\nVisual Transformer在接近输出端能保留比主\n流的卷积神经网络更加精细的空间位置信息，从\n而具有相对更强的性能和实用性。在未来的发展\n过程中，与transformer相关的研究与优化可能\n会进一步引起广泛关注，同时部分卷积神经网络\n相关的运算方式也可能被运用到其中，对医学图\n像的处理方式将会更加丰富和高效。\n结束语\n本文梳理了医学图像析技术的总体发展过\n程，着重介绍了以卷积神经网络和transformer\n为基础的模型，并比较分析了二者的异同点和各\n自的优势，探索了可能的发展趋势。由于知识水\n平的局限，文章存在着一些不足，例如卷积神经\n网络和transformer主要分析的是基础的模型，\n一些特殊模型的算法步骤并没有给出详细的介\n绍，所用于分析特点和对比优缺点的样本数量过\n小等。未来可能会对不同的具体模型进行更加深\n入的研究。"}, {"role": "assistant", "content": "最终打分：8.3 (范围0-10分)\n1. 结构完整性得分：9, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8.5, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：7, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "组长：王天飓\n组员：石轩宇、李达辉、王立全、黄德赐CS103 智能图书馆项目报告  \n \n概述  \n本项目结合南方科技大学图书馆学生及教职工有关借阅书籍，入出图书馆和使用讨论间等数据，通过应\n用在 “ 人工智能导论 ” 课程中所学到的分析方法，实现以下功能：\n1. 接入 CAS 系统，实现在校生和教职工的登录功能\n2. 完成对读者借阅书籍特征的个性化分析，并以雷达图的方式直观地呈现特征3. 实现基于对读者借阅书籍特征的雷达图数据和读者本人专业情况，为读者提供个性化的推荐书友服\n务\n4. 基于读者借阅数据的分析和为其推荐的书友，为读者提供个性化的推荐书籍服务\n \n数据处理  \n本次智能图书馆项目的数据处理部分分为对书籍数据和借阅数据的预处理和相似度计算三部分。\n书籍数据预处理  \n1. 利用爬虫文件可以从数据库中爬取书籍数据，共有 22 种已有书籍类型，如\n数理科学和化学\n环境科学、安全科学\n生物科学\n天文学、地球科学\n1. 根据该 22 种类型再人为分成五大类，自然，工程，文艺，社科，军事。再对 22 种书籍类型与五大类\n的关联程度，将总权重 5 分配给各个类型（数字顺序对应相应类别），如\n数理科学和化学 -50000\n环境科学、安全科学 -41000\n历史、地理 -00230\n借阅数据预处理  \npython 的内置容器类型列表和字典与 json 结构十分相近，因而处理 json 文件时具有独特的优势。\n利用 python 将 json 文件转为每条记录一行，各个数据以下划线分割的形式，方便接下来的处理。\n处理形成两个数据文件，第一个 (tt.txt) 包含人名 学号 借阅书的类型，第二个包含学号 书的 isbn 书名。相似人数计算  \n对第一个文件（ tt.txt ）进行处理 , 使用 hashmap 建立学号到五个权重的映射，然后进行五个权重的标准\n化，至此每个人拥有一个标准化的五维向量。\n设置差异参数 error ，计算相似的人数，根据相似人数反馈调节 error 变量，直到在某一 error 下相似人数\n几乎不变。相似度可以转换为两个五维向量之间的距离进行量化。\n最后相似的人可以作为书友进行推荐，书友阅读的书可以作为推荐书籍。由于人数限制，选择的书友的\n人数小于等于 5 。书友在重新运行后端或者重新运行项目中一个方法后可以进行更新。\n图片爬取  \n1. 目的\n利用爬虫将书籍图片提前保存至本地，减少服务器响应时间。\n2. 分析\n其中爬取的目标网址是 南科大的图书馆荐购网址 。\n分析网络通信得知需要的图片采用动态加载，该网站用了 另一个网站 的数据，因此需要参考荐\n购网址的请求头仿造请求。\n3. 过程与问题\n由于文件量很大，爬的过程十分漫长\n第一次尝试的时候爬了快 10000 张后被拒绝访问网址\n推测其检查 User-Agent 字段来检测用户，同一个 User-Agent 访问过多便拒绝访问\n弄个 User-Agent 的列表，被拒绝就换下一个访问\n第二天尝试的时候又发现拒绝访问\n发现荐购网址向图片网址的请求头中有个 authorization 字段\n推测其一天一换，更新一下就又能跑了\n图书馆每本书给了 2-4 个 isbn 号，而且有些数据不规范，如 9781118927489q(epub) 等，需要\n用正则表达式过滤\n4. 反思\n事后得知爬虫存在一定的法律问题，可能需要提前和图书馆 / 信息中心确认\n \n前端  \n前端采用 Vue 框架，使用 yarn 作为包管理器。使用 echarts 绘制前端数据图表，使用 axios 向后端发起\nhttp 请求然后接受数据。前端分为 component ， router ， view, assets 四个目录，分别管理前端组件，\n前端页面跳转路由，前端页面展示，前端静态资源四个部分，结构清晰。\n目录结构如图.\n \n后端  \n后端采用 SpringBoot 框架，用 Maven 进行依赖管理、项目打包和编译构建\n后端使用了 controller 、 service 、 dao 的三层结构，逻辑清晰，降低了代码耦合性\n.\n为了便于前后端进行数据交互，我们采用 Json 作为数据格式，后端定义了通用的响应类 ApiResponse 使\n响应格式统一，便于前端处理为了前后端对接简便，我们采用了开源项目 Knife4j 作为文档生成器，这样后端免去了撰写接口文档的麻\n烦\n只需添加 Configuration 并且注入 Docket 的Bean 即可\n生成的文档效果如图 👇\n.\n引入spring-boot-starter-web 依赖可以方便快捷地搭建 Web 服务，只需编写 Controller 类@Data\npublic class ApiResponse<T> {\n    private int code;\n    private String message;\n    private T data;\n    public ApiResponse(int code, String message) {\n        this.code = code;\n        this.message = message;\n    }\n}\n@Configuration\n@EnableSwagger2WebMvc\npublic class Knife4jConfig {\n    @Bean\n    public Docket docket() {\n        return new Docket(DocumentationType.SWAGGER_2)\n                .apiInfo(new ApiInfoBuilder()\n                              .description(\"# RESTful APIs\")\n                              .version(\"1.0\")\n                              .build())\n                .select()\n                \n.apis(RequestHandlerSelectors.basePackage(\"cn.edu.sustech.lib.controller\"))\n                .paths(PathSelectors.any())\n                .build();\n    }\n}在对接 CAS 时，后端使用了 okhttp 库，便于发送向 CAS 发送 http 请求，具体逻辑如下\n这里做的时候踩了坑， Okhttp 的 Client 创建时默认会采取重定向策略，所以得到的 Response 不是原始响\n应，所以在创建 Client 时应该把重定向策略关闭\n \n总结及改进  \n在本次智能图书馆的项目中，我们使用了提供的数据库搭建了一个智能推荐系统，这个系统采用了前后\n端分离的架构，实现了推荐书籍书友，分析读者借阅个性化特征的功能。@RestController\npublic class ExampleController{\n    \n    @GetMapping(\"/example\")\n    public ApiResponse<String> example(@RequestParam Integer id){\n        return new ApiResponse(200,\"ok\");\n    }\n}\n@SneakyThrows(IOException.class)\npublic boolean login(Integer userId, String password) {\n    Request getRequest = new Request.Builder()\n                                    .url(LOGIN_URL)\n                                    .get()\n                                    .build();\n    @Cleanup Response response = CLIENT.newCall(getRequest).execute();\n    if(response.code()!=200){\n        return false;\n    }\n    //`execution` is the form information sent by CAS's frontend\n    @SuppressWarnings(\"ConstantConditions\")\n    String execution = response.body().string().split(\"name=\\\"execution\\\" value=\\\"\")\n[1].split(\"\\\"\")[0];\n    FormBody body = new FormBody.Builder()\n                                .add(\"username\", userId+\"\")\n                                .add(\"password\", password)\n                                .add(\"execution\", execution)\n                                .add(\"_eventId\", \"submit\")\n                                .build();\n    Request postRequest =new Request.Builder()\n                                    .url(LOGIN_URL)\n                                    .post(body)\n                                    .build();\n    response = CLIENT.newCall(postRequest).execute();\n    return response.code()==200;\n}\nprivate static final OkHttpClient CLIENT = new \nOkHttpClient.Builder().followRedirects(false).build();在改进方面，本系统仍有许多功能可以扩展，在项目前中期，组内曾多次尝试运用更先进的算法进行相\n似度分析和样本筛选，也曾考虑过用统计学中心极限定理处理总体数据，但因大部分样本数量太少，大\n部分同学借书并不能满足大基数的数据运算方法而搁置。选择需求样本数少但误差小的相似度方法是一\n个好的改进方向。在项目后期，组内也曾多次讨论推荐书友的机制。本系统可以从借阅相似度，不太专\n业需求，同年级阅读倾向，书籍借阅热度等方面进行综合推书，以进一步提升借书质量。\n \n本项目后续也可以采用 Docker-Compose 进行容器化部署，便于校内访问。\n首先前端使用 yarn build 命令将源代码编译成静态文件，生成在 /dist 目录下，然后利用 Docker 的卷\n挂载功能，将其关联到 Nginx 的静态文件夹 /nginx/html 。Nginx 同样可以加入证书实现 https 服务，这\n要求需要有响应的域名指向服务器。前端的容器开放 http 的默认 80 端口以及 https 的默认 443 端口\n关于后端的部署，首先在根目录下使用如下命令跳过测试打包\n然后将生成的 jar 包利用卷挂载直接关联到容器内，这样做可以避免撰写 Dockerfile 利用其中的 ADD 操作\n重新制作新的镜像。开放 8080 端口，最后将 entrypoint 设定为 jar 包启动命令即可\n最终部署的目录树如下mvn package -D\"maven.test.skip=true\"\nversion: '3'\nservices:\n  nginx:\n    image: nginx:latest\n    container_name: library-nginx\n    environment:\n      TZ: Asia/Shanghai\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./nginx/html:/etc/nginx/html\n      - ./nginx/nginx.conf:/etc/nginx/nginx.conf\n      - ./nginx/cert:/etc/nginx/cert\n    privileged: true # 容 器 内 的 root 有 真 正 的 root 权 限 ， 否 则 只 是 一 个 普 通 用 户 ， 没 有 .conf 文 件 权 限\n  main:\n    image: openjdk:17\n    container_name: library-main\n    environment:\n      TZ: Asia/Shanghai\n    ports:\n      - \"8080:8080\"\n    volumes:\n      - ./library-main-0.0.1-SNAPSHOT.jar:/main.jar\n    entrypoint: [\"java\",\"-jar\",\"/main.jar\",\"--spring.profiles.active=prod\"]\ndeploy\n├─.env\n├─docker-compose.yml\n├─library-main-0.0.1-SNAPSHOT.jar └─nginx\n   ├─nginx.conf\n   ├─cert\n   │   ├─7998226_example.com.key\n   │   └─7998226_example.com.pem\n   └─html\n       ├─favicon.ico\n       ├─index.html\n       ├─css\n       │  ├─app.e91ad99c.css\n       │  └─chunk-vendors.84b717c5.css\n       ├─img\n       │  ├─background1.c0704357.jpg\n       │  ├─img.eeed2fa6.png\n       │  ├─img_1.95baee9e.png\n       │  ├─img_2.568571b8.png\n       │  ├─img_3.77932214.png\n       │  └─img_4.637f0cc9.png\n       └─js\n          ├─app.4ac23e3c.js\n          ├─app.4ac23e3c.js.map\n          ├─chunk-vendors.0ff34ea2.js\n          └─chunk-vendors.0ff34ea2.js.map"}, {"role": "assistant", "content": "最终打分：7.9 (范围0-10分)\n1. 结构完整性得分：8.5, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8.5, 占比20%\n4. 内容独特性和创新性得分：8.5, 占比20%\n5. 参考文献规范性得分：3, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "Road Sign Recognition and Backdoor Attacks of\nBaidu Pilotless Automobile\nShiwen CAO\n12113024Xuanyu LIU\n12110408Tianyou SONG\n12112519Fanzhong WANG\n11911824Jinlong ZHANG\n12112528\nAbstract —Traffic image recognition is an indispensable func-\ntion in the field of automatic driving. It requires that the\ncomputer can accurately and quickly identify the traffic signs\nin the captured information. YOLOv5 is an algorithm that can\neffectively detect the far and small targets with high speed and\naccuracy. Backdoor attack is an operation aimed at malicious\nprocessing of data sets, and it is difficult for human eyes to detect\nthe differences in images, However, after receiving the backdoor\nattack, many characteristic values of the data have changed,\nwhich will affect the judgment of the recognition algorithm.\nThis paper will use the method of adding hidden watermark\nto simulate the backdoor attack to detect whether the backdoor\nattack has an impact on the accuracy of YOLOv5.\nIndex Terms —Backdoor Attack, Watermark, Yolov5\nI. I NTRODUCTION\nA. Background\nWith the development and application of artificial intelli-\ngence technology in license plate detection and driver-less\ndriving. Due to the characteristics of its sliding window model,\nthe traditional target detection algorithm has a certain degree\nof singularity in feature extraction and matching, and its\naccuracy and detection speed are relatively poor. Therefore, the\ndeep learn-based target detection algorithm has the advantages\nof higher efficiency and accuracy, rapidly overtaking the\ntraditional target detection algorithm and becoming the most\nmainstream target detection algorithm. It is mainly divided\ninto two development directions: two-stage model and one-\nstage model.\nB. Backdoor Attack\nData poisoning attack refers to that the attacker adds a small\namount of carefully constructed poison data to the training set\nof the model, so that the model cannot be used normally in the\ntest stage or assists the attacker to invade the model without\ndamaging the accuracy of the model. The former destroys the\navailability of the model and is a no-target attack. The latter\ndestroys the integrity of the model for targeted attacks. Data\npoisoning attack was first proposed by Dalvi, who used this\nattack to evade detection of spam classifiers. Poisoning attacks\nthat destroy integrity have strong concealment: the poisoned\nmodel shows normal prediction ability for clean data, andonly outputs wrong results for the target data selected by\nthe attacker. Such attacks that cause the AI model to output\nincorrect results on certain data can cause significant damage\nand in some critical scenarios, serious security failures.\nC. Motivation\nThrough the computer to realize the human visual function,\ngrasp and understand the image of the scene, identify and\nlocate the target, determine their structure, spatial arrangement\nand distribution as well as the mutual relationship between the\ntarget and so on, according to the perceived image of the actual\ntarget and scene in the objective world to make a meaningful\njudgment.\nII. E XPERIMENT\nA. Backdoor Attack\n1) earlier stage: Through the operation of the frequency\ndomain of the picture, the invisible watermark is added to the\npicture. The Algorithm used is invisible-watermark [1]. The\nresulting image is obviously different from the original image\nand fails to achieve the invisible watermark effect we expected.\nThe reason may be that this method is suitable for large size\npictures, and it is difficult to hide the watermark in the picture\nwithout being discovered for small size pictures.\nFig. 1. The invisible watermark images in earlier stage\n2) later stage: Eventually the watermark adding methods\nis FTrojan [2], concrete process is: the parts of the need to add\nthe watermark from the original cutting, part of the invisible\nwatermark image, and will be processed images put back to\nthe original image.\nB. Model Training\n1) Yolov5: In 2016, Redmon J proposed a new target detec-\ntion algorithm, YOLO (You Only Look Once). Different from\nthe target detection algorithm based on classification, whichFig. 2. The invisible watermark images in later stage\nuses classifiers to perform detection, the YOLO algorithm\ntreats the target detection framework as a spatial regression\nproblem. A single neural network can obtain the prediction of\nboundary frame and category probability from the complete\nimage after one operation, which is conducive to the end-\nto-end optimization of detection performance. Compared to\nprevious networks, YOLOv5 uses Pytorch for the first time,\nwhich provides simpler support and easier deployment, and\nimproves image reasoning time and detection average accuracy\nwithout any loss of accuracy.\nFig. 3. Different target detection algorithms on COCO datasets\nFig. 4. GPU speed of four Yolov5 models\n2) Training Result: AP (Average precision) is the main\nevaluation standard of target detection model. In the field of\ntarget detection, suppose we have a set of pictures containing\nseveral targets to be detected, Precision represents how many\ndozen targets can be detected by our model, and Recall\nrepresents how many percentage of all real targets can be\ndetected by our model.\nFig. 5. The test results without poisoning\nThe possible causes of misidentification are:\n•Less similar data, can not train more accurate weight;\n•Part type characteristic value of similar images, bring\ncertain difficulty to algorithm identification;\n•Datasets with poor quality, image distance is far and less\ninformation.\nFig. 6. The test results with poisoning\nThere are two conjectures as to why the poisoning effect\nis not obvious. One is that, yolov5 has powerful function,\nwhich can effectively filter poisoning information. Second\nis, Poisoned by the information is not obvious in the image\ncharacteristics, has not been identified.Explain the error in identification after poisoning:\n•Less training data, can not train more effective parame-\nters;\n•Picture is too small, can identify the characteristics of\nless;\n•Information may be poisoning affects judgment.\n"}, {"role": "assistant", "content": "最终打分：7.9 (范围0-10分)\n1. 结构完整性得分：8, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：7, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "CS103人工智能导论项目报告\n一、项目信息\n组号：2\n成员：刘一郴:12112609方嘉玮:12110804 徐建辉:12112807\n题目：大象机器人——物块识别分类\n文前致谢 ： 感谢深圳市大象机器人科技有限公司提供包括机械臂在内的人工智能\n套装以及丰富齐全可移植性高的配套基础代码。 同时感谢行业导师不吝赐教， 热\n心地帮助解决各种软硬件问题。\n成果简介 ： 实现了让机械臂自动将物块按照颜色等分类标准进行分类，并通过机\n械臂分拣至各个置物桶。\n总体收获：真正体验认识到计算机系作为工科的特点-需要兼顾软件与硬件。通\n过这次project，学习并运用ROS，六轴机械臂运动原理，计算机视觉，以及将\n其对接成完整项目的各个工科手段。 熟悉并完成了一个需要软硬件结合的工科项\n目的流程。\n二、前期准备 （国庆期间完成）\n1.套装搭建\n首先将套装组合，有说明书的情况下很快完成。2.机械臂固件烧录\n向机械臂的内部上下两块开发板烧录相应固件。在这一步中遇到的问题是 ： 由于\n机械臂为该型号最新一批次产品， 相应的用户手册没有及时更新， 且应该烧录的\n对应固件并没有及时上线。这就使得正常按照手册操作，机械臂未能正常运行，\n让我们的项目陷入困惑并停滞， 在三天时间内多次与公司确认后， 公司发现并帮\n助解决了这一问题。\n3.对接其他功能模块\n将包括摄像头，吸泵在内的各个模块对接至相应引脚，并且对接至PC端。这一\n步遇到的问题是吸泵在按照手册正常对接后未能正常工作。 该问题同样是机械臂\n产品批次过新造成。 在经过徐建辉同学两天的拆解观察机械臂开发板引脚， 查看\n底层代码后，找到了正确的吸泵引脚接口。\n三、视频演示\n//https://github.com/cunlidaniang/CS103introduction-to-AI-project-elephant-\nrobot\n四、技术实现\n1.机械臂定位\n为机械臂的运动设计一系列的点，如机械臂的初始化点、待抓取点。功能中机械\n臂的初始位置和置物桶的位置信息是我们提前预设好的， 具体获得方法是通过公\n司提供的软件rviz将机械臂移动至指定位置， 记录并保存当前机械臂六个轴的各\n自转动角度。\n2.物块识别\n首先是得到物体的坐标，方法是通过ArUco码获得三维坐标信息，以此来推算\n出所要抓取物块的位置。\n然后是分类算法，也是该项目中唯一涉及人工智能的部分。这里分两种情况，一\n是颜色识别，由于物块颜色有限，所以不需要智能算法，只需要对接收到的图片\n进行色度转换处理，根据自定义类初始化的HSV设置颜色识别范围。将转换后\n的图片进行腐蚀、膨胀等操作，加深图片颜色对比程度。通过过滤以及检查轮廓\n等操作，识别物块颜色并对其进行定位。 （手册操作）\n二是图像识别分类，公司提供的配套库丰富齐全，有自带的用OpenCV加载\ntensorflow训练出来的图片识别模型，所以在使用的时候只需要将图片丢进\nself.net，取置信度最高的结果，就可以直接该图像的分类结果。\n3.机械臂移动\n为了机械臂运行时不会碰到障碍物， 我们并没有直接采用公司提供的pub_angles，\npub_coord这种直接输入坐标就运行至指定位置的移动方法， 而是采用了机械臂\n逐步移动的方法，每移动一步，根据当前坐标与置物桶位置，进行一系列判断条\n件，来判断是否能进行下一步移动。\n五、反思\n1.在物块的定位方面，是有aruco码来提供三维信息，但是只能计算出物块的xy\n值， 对于物块堆放时的高度信息无法获取， 也就是当物品重叠时无法很好的处理。\n2.在机械臂移动方面，我们为了避免碰撞，编写了自己的移动方式，但实际发现\n效果并不好。原因在于，一是由于多步移动与睡眠，物块从拾取到放置的时间大\n大延长。 二是由于我们的判断条件完全根据机械臂头的位置坐标与置物桶的坐标\n来判断， 无法有效考虑到外来障碍与机械臂本身碰撞体积， 是一种效率低下且意\n义不大的移动方式。 三是由于我们机械臂本身六轴各个轴都有限位角度， 每一步\n的移动都会收到限位的影响，边界条件判断复杂，容易出现不可控奇怪的错误。\n移动时出错几率较大。\n六、收获\n本次项目重点并不在于人工智能的训练， 而是在于人工智能技术的实际应用， 在\n于实际工程项目中软硬件的对接与部署， 是一次软件硬件技术的很好的训练机会。\n这次项目让我们认识到了在实际工程项目中， 需要同时具备一定软硬件各方面知\n识， 有较强的学习能力来阅读手册学习所需非本专业知识。 初步掌握了ROS， 空\n间定位，机器人运动规划，计算视觉等知识。\n七、致谢\n最后再次感谢深圳市大象机器人科技有限公司提供包括机械臂在内的人工智能\n套装以及丰富齐全可移植性高的配套基础代码。 感谢行业导师不吝赐教， 热心地\n帮助解决各种软硬件问题。让我们收获这次宝贵的软硬件结合项目经历。"}, {"role": "assistant", "content": "最终打分：7.6 (范围0-10分)\n1. 结构完整性得分：8, 占比20%\n2. 逻辑清晰度得分：8, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：8.5, 占比20%\n5. 参考文献规范性得分：3, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "                                                                                                                                                            \n                                                                                                                                                           http://www.jsjkx.com                                                                                                   \n        世界屋脊上的湖泊-对遥感图像中湖泊识别处理的应用与分\n析 \n白楚焓1  张凌瑞1   张子越1   彭宇科1   赵亮迪1                 \n1 南方科技大学 广东省 深圳市 518000\n摘 要 湖泊在自然环境和人类活动中都具有重要意义。湖泊在生态系统中扮演着重要的角色，对水文气象\n等环境因素产生显著影响，同时也是人类活动的重要场所。由于湖泊分布范围广，人迹罕至，传统的湖泊\n识别方法，如手动测绘和遥感分析，虽然有一定的效果，但是效率低下，难以满足日益增长的需求。为了\n解决这一问题，两种基于人工智能的湖泊识别方法被使用，即聚类算法和神经网络算法。为了测试算法，\n两个算法的模型被训练，并通过实验测试评估了方法的有效性。结果表明，使用聚类算法和神经网络算法\n都能达到识别湖泊的效果，方法在识别准确率和召回率方面都表现良好，而神经网络算法能够达到更高的\n湖泊识别准确率。因此，我们认为算法在识别湖泊方面具有较大的潜力，可以为农牧业生产，地图测绘，\n资源勘探等领域提供帮助。\n关键词：  人工智能；湖泊识别；遥感图像；聚类算法；神经网络\n中图分类号：TP391 \n \nLakes on the Roof of the World - Application and Analysis of Lake \nRecognition Processing in Remote Sensing Images \nChuhan Bai1 ，Lingrui Zhang 1 ，Ziyue Zhang 1 ，Yuke Peng 1 and  Liangdi Zhao 1 \n1 South University of Science and Technology, Shenzhen 518000, China\nAbstract   Lakes play an important role in both natural environment and human activities. Lakes play an important \nrole in the ecosystem, which has a significant impact on hydrometeorology and other environmental factors, and is \nalso an important place for human activities. As the lakes are widely distributed and rarely visited, traditional lake \nidentification methods, such as manual mapping and remote sensing analysis, although effective, are inefficient and \ndifficult to meet the growing demand. To solve this problem, two artificial intelligence-based lake recognition \nmethods are used, namely clustering algorithm and neural network algorithm. To test the algorithm, models of the \ntwo algorithms were trained and the effectiveness of the method was evaluated through experimental tests. The \nresults show that both the clustering algorithm and the neural network algorithm can achieve the effect of lake \nidentification, and the method has a good performance in recognition accuracy and recall rate, while the neural \nnetwork algorithm can achieve a higher accuracy of lake identification. Therefore, we believe that the algorithm has \ngreat potential in the identification of lakes, which can be helpful in agriculture and animal husbandry production, \nmapping, resource exploration and other fields.\nKeywords   Artificial intelligence, Lake identification, Remote sensing image, Clustering algorithm, Neural network                                                                                                                                                            \n                                                                                                                                                           http://www.jsjkx.com                                                                                                   \n        1引言\n湖泊是地理环境中的一种重要组成部分，在\n自然环境和人类活动中都具有重要意义。例如，\n湖泊是重要的水源，可以为人类提供饮用水和农\n业生产所需的水资源；湖泊也是重要的生态系\n统，可以为动物和植物提供生存条件；湖泊的分\n布也可以反映地理区域的演变历史和自然环境的\n变化。\n由于湖泊分布范围广，人迹罕至，因此传统\n的测绘手段难以实现。在近年来，随着遥感技术\n的不断发展，遥感图像在地理环境研究中发挥着\n越来越重要的作用。然而，由于遥感图像数据集\n通常拥有大量的信息，人工处理较为困难。因\n此，如何有效地识别遥感图像中的湖泊，并自动\n提取相关信息，成为了一个重要研究方向。\n为了解决这一问题，我们提出了使用人工智\n能的方法。在具体实现方面，我们考虑使用聚类\n算法[1]和神经网络算法[2]来提取图像的特征，并\n使用全连接层进行分类。我们选择这些方法的原\n因是，它们在处理遥感图像的应用中表现出较高\n的效率和准确性[3]。\n此外，我们还将结合其他地理数据，如等高\n图，来进一步提高模型的准确性。最后，我们将\n使用基于地理信息系统的方法来计算湖泊的面\n积，并根据湖泊的分布情况进行分析。\n本研究的意义在于，我们希望通过自动化的\n方式，为农牧业生产、地图测绘、资源勘探等领\n域提供有效的支持。同时，我们也希望通过本研\n究，为未来人工智能在遥感图像处理中的应用提\n供借鉴。\n2研究方法\n在本文中，我们使用了聚类算法和神经网络\n来实现湖泊识别。\n2.1聚类算法模型\n我们采用了一种有监督的学习算法，用于将\n像素点分成不同的组。我们使用聚类算法来找到\n湖泊的RGB值，即在图像中提取可能为湖泊的颜\n色信息。在识别图像时，聚类算法可能会将图像中的相似颜色或纹理归为一个类，从而实现对图\n像的分类。因此，聚类算法产生的图片可能是将\n图像分为若干类的结果。具体来说，可能会将图\n像中的不同颜色、纹理、物体等分开，每个分类\n用不同的颜色标注。\n我们使用像素级别的聚类算法，利用label\n得出湖泊的纹理信息，在得到所有纹理消息后利\n用高斯分布得到纹理的分布，之后通过迭代不断\n调整聚类中心来将像素点分成不同的组。再利用\n高斯图和阈值得到目标区域。最后，我们将可能\n为湖泊的类值和非湖泊的类进行黑白转换，形成\n灰度图或黑白图。得到湖泊的大致区域。之后，\n我们还通过轮廓检测算法提取湖泊的轮廓。结合\n我们之前的得出的纹理消息。形成新的mask来\n和原图像进行矩阵乘积操作，最后得到湖泊的图\n像。\n2.2神经网络模型\n神经网络算法是一种机器学习算法，它通过\n训练得到输入和输出之间的映射关系。在识别图\n像时，神经网络算法可以通过对图像中的特征进\n行提取，并输出对应的label来进行识别。因\n此，神经网络算法产生的图片可能是将图像分为\n不同类别的结果。具体来说，可能会将图像中的\n不同物体、场景、表情等分开，并使用不同的颜\n色标注。同时，神经网络算法还可能输出图像中\n物体的分割图，即将图像中的物体以轮廓线的形\n式标出来。\n我们尝试构建神经网络，对图像的特征进行\n提取。具体来说，我们使用卷积神经网络 (CNN) \n来提取图像的特征。首先，我们使用了8个\npadding和stride为1的卷积层，最大池和一\n个全连接层来提取图像的特征，在第1，2，4，\n6，8个卷积层后添加最大池来保留特征。每一\n个卷积层后都使用了relu函数来保留特征。最\n后，我们使用全连接层来得出结果，我们取了学\n习率为1e-4，利用ADAM来做optimizer。我们\n还使用了 dropout 层来减少过拟合的风险。\n2.3语义分割框架白楚焓等：世界屋脊上的湖泊-对遥感图像中湖泊识别处理的应用与分析                                              3\n在深度学习研究领域中，语义分割\n（Semantic Segmentation）被广泛运用于自动\n驾驶、遥感测绘等领域。2.2中便是其中一种语\n义分割的实现。在科研领域，有许多成熟且高效\n的框架可供我们进行模型的实验和快速迭代。此\n次我们使用了MMSegmentation来进行训练，并\n且分析模型的准确度。\n本次实验采用了PSPNet网络处理分类任\n务，利用其金字塔池化模块的特性，整合全局上\n下文信息，有效地在场景语义中获得高质量的结\n果。\n网络架构如图所示，使用了带空洞卷积的\nResNet作为特征提取网络，并且在金字塔池化\n（Pyramid Pooling）中融合了四种不同比例的\n特征，最后将不同level的特征拼接起来作为金\n字塔池化的全局特征。\nPSPNet在遥感数据集上有着不错的表现。\n经实测，backbone为ResNet50的网络在ISPRS \nPotsdam数据集上可以达到78.98的mIoU。在我\n们的湖泊数据集中达到了96.2的mIoU以及\n98.24的平均准确度。\n3研究结果\n我们在真实的遥感图像数据集上进行了实\n验，并使用准确率和召回率来评估我们的方法的\n有效性。实验结果表明，我们的方法在识别湖泊\n方面具有较高的准确性和效率。如表一所示，我\n们的聚类算法在准确率方面取得了79.21%的结\n果，在召回率方面取得了74.40%的结果。而我\n们的神经网络算法在准确率方面取得了 98.15% \n的结果，在召回率方面取得了 92.97% 的结果。\n综合来看，神经网络算法性能优于聚类算法。\n表1   两种算法的统计结果\nTable 1  Result\nAlgorithm Recall Rate /% Accuracy Rate /%\nCluster 74.40 79.21Neural Network 92.97 98.15\n4研究分析\n总的来说，聚类算法和神经网络算法在识别\n图像时，都可能产生的图片是将图像分为若干类\n的结果。聚类算法的结果可能是将图像中的相似\n颜色或纹理归为一个类，并使用不同的颜色标\n注；而神经网络算法的结果可能是将图像中的不\n同物体、场景、表情等分开，或者输出图像中物\n体的分割图。如图1所示，聚类算法和神经网络\n算法大体上都能完成识别任务，聚类算法识别出\n来的图像会带一些毛刺和噪点。对一些比较极端\n的图像，如图2，聚类算法在识别湖泊时的处理\n效果不是非常好， 判断区域错误较明显。\n综上所述，我们的研究表明，通过使用人工\n智能技术，我们能够有效识别和分析遥感图像中\n的湖泊。这项研究为我们更好地利用遥感图像，\n更精确地测绘湖泊，并为进一步的研究提供了基\n础。\n5结束语\n本文提出了两种基于人工智能的湖泊识别\n方法并做了对比。在文中，我们对遥感图像中湖\n泊的识别处理进行了应用与分析。我们使用了聚\n类算法和神经网络算法，并对网络进行了优化以\n提高准确率。我们在真实的遥感图像数据集上进\n行了实验。实验结果表明，我们的方法在识别湖\n泊方面具有较高的准确性和效率，神经网络算法\n能够准确识别95%以上的湖泊。\n通过此模型，我们可以为农牧业生产、地图\n测绘、资源勘探等领域提供有价值的信息，同时\n也为我们更好地了解高原湖泊的分布情况提供了\n有力的技术支持。\n但是，本文仍有一些问题需要进一步探究。\n例如，我们尚未尝试其他算法，并且准确率也不\n是完全满意的。因此，未来的工作将继续探究其\n他算法，以及如何更加精确地识别湖泊。同时，\nComputer Science   计算机科学                                                                                   4\n我们还将继续对算法进行更多的优化，以提高准\n确率。\n \n(a) Origin                          (b) Label\n \n(c) Cluster                (d) Neural Network\n图1  原图，label以及两种算法识别出来的图像\nFig.1  The original image, the label and the image identified \nby the two algorithms\n \n(a) Origin                          (b) Label\n \n(c) Cluster                (d) Neural Network\n图2  原图，label以及两种算法识别出来的图像\nFig.2  The original image, the label and the image identified \nby the two algorithms\n\n"}, {"role": "assistant", "content": "最终打分：9 (范围0-10分)\n1. 结构完整性得分：9.5, 占比20%\n2. 逻辑清晰度得分：9, 占比20%\n3. 语言连贯性得分：9, 占比20%\n4. 内容独特性和创新性得分：9, 占比20%\n5. 参考文献规范性得分：8, 占比10%\n6. 课程知识掌握度得分：9, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "D4LCN: Learning Depth-Guided Convolutionsfor Monocular 3D Object Detection (CVPR2022) Requirements Cuda & Cudnn & Python & PytorchThis project is tested with CUDA 8.0/9.0, Python 3, Pytorch 0.4/1.0/1.1, NVIDIA Tesla V100/TITANX GPU. And almost all the packages we use are covered by Anaconda.Please install proper CUDA and CUDNN version, and then install Anaconda3 and Pytorch.My settings\nData preparation Download and unzip the full KITTI detection dataset to the folder /path/to/kitti/. Then place a softlink (or the actual data) in data/kitti/. There are two widely used training/validation set splits for the KITTI dataset. Here we only show the setting of split1, you can set split2 accordingly.Our method uses DORN (or other monocular depth models) to extract depth maps for all images. You can download and unzip the depth maps extracted by DORN here and put them (or softlink) to the folder data/kitti/depth_2/. (You can also change the path in the scripts setup_depth.py)Then use the following scripts to extract the data splits, which use softlinks to the above directory for efficient storage.source ~/anaconda3/bin/activate (python 3.6.5)(base)  pip listtorch                              1.1.0torchfile                          0.1.0torchvision                        0.3.0numpy                              1.14.3numpydoc                           0.8.0numba                              0.38.0visdom                             0.1.8.9opencv-python                      4.1.0.25easydict                           1.9Shapely                            1.6.4.post2\ncd D4LCNln -s /path/to/kitti data/kittiln -s /path/to/kitti/testing data/kitti_split1/testing\nNo. 1 / 2Next, build the KITTI devkit eval for split1.Lastly, build the nms modulesTraining We use visdom for visualization and graphs. Optionally, start the server by command lineThe port can be customized in config files. The training monitor can be viewed at http://localhost:9891. You can change the batch_size according to the number of GPUs, default: 4 GPUs with batch_size = 8.If you want to utilize the resnet backbone pre-trained on the COCO dataset, it can be downloaded from git or Google Drive, default: ImageNet pretrained pytorch model. You can also set use_corner and corner_in_3d to False for quick training.See the configurations in scripts/config/depth_guided_config and scripts/train.py for details. Testing We provide the weights, model and config file on the val1 data split available to download.Testing requires paths to the configuration file and model weights, exposed variables near the top scripts/test.py. To test a configuration and model, simply update the variables and run the test file as below.  python data/kitti_split1/setup_split.pypython data/kitti_split1/setup_depth.pysh data/kitti_split1/devkit/cpp/build.shcd lib/nmsmake\nsh visdom.sh\nsh train.sh\nsh test.sh\nNo. 2 / 2"}, {"role": "assistant", "content": "最终打分：6.5 (范围0-10分)\n1. 结构完整性得分：6, 占比20%\n2. 逻辑清晰度得分：6, 占比20%\n3. 语言连贯性得分：6.5, 占比20%\n4. 内容独特性和创新性得分：8, 占比20%\n5. 参考文献规范性得分：4, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
{"messages": [{"role": "system", "content": "你是一位教授人工智能导论课程的大学教师，需要对学生提交的课程项目报告进行评估。\n请评估以下<报告文本/总结报告文本>在描述<结构完整性>，<逻辑清晰度>， <语言连贯性>， <内容独特性和创新性>， <参考文献规范性>，<课程知识掌握度>方面的表现，\n并根据各指标<占比比例>进行打分与点评，打分范围0-10分。并最终按照<打分模版>给出学生报告打分结果与评价”。打分模板如下：“最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>”\n请严格按照以下格式返回结果，最终打分一行、6个维度各自一行、修改意见一行，不要擅自添加换行：\n最终打分：<> (范围0-10分)\n1. 结构完整性得分：<>, 占比20%，原因如下：<>\n2. 逻辑清晰度得分：<>, 占比20%，原因如下：<>\n3. 语言连贯性得分：<>, 占比20%，原因如下：<>\n4. 内容独特性和创新性得分：<>, 占比20%，原因如下：<>\n5. 参考文献规范性得分：<>, 占比10%，原因如下：<>\n6. 课程知识掌握度得分：<>, 占比10%，原因如下：<>\n修改意见：<>"}, {"role": "user", "content": "在智能医疗中的深度估计算法的实现与优化\n邓远桥 邹逸轩\n自我介绍\n老师同学们大家好\n我们项目的主题是智能医疗中的深度估计，由于我们组均为大一新生，所以此项目以了解，理解为主，并\n在此的基础上提出一些自己的看法。首先我们需要了解的就是【智能医疗】和【深度估计】的基础概念。  \n何为【智能医疗】？“智能医疗是通过打造健康档案区域医疗信息平台，利用最先进的 物联网技术，实现\n患者与医务人员、医疗机构、医疗设备之间的互动，逐步达到信息化。”  而通过对一些文献的阅读，我们\n自己的理解是：所谓智能医疗，就是利用当下不断进步的技术（物联网，云计算等技术）与医疗结合，来\n实现更高效，强力，精准的医疗服务（疾病治疗，就医手续等） 。通过继续深入了解发现，现在智能医疗可\n以分为三个阶段：1.0-医疗信息化——通过计算机、宽带网络等技术实现医院信息共享和区域医疗信息共\n享；2.0-联网医疗——借助可穿戴设备、4G网络、云计算、大数据等技术，实现以在线导流、问诊为主要\n模式的互联网医疗以及医院内部融合医保的全流程移动；3.0智能医疗——人工智能技术全面融入医疗健\n康全环节，借助医疗机器人、虚拟现实、增强现实、5G网络、人工智能等技术，实现人工智能辅助诊断、\n远程手术等业务模式，实现医疗健康全流程智能化。而当下，正是从联网医疗2.0到智能医疗3.0的过渡\n阶段，智能医疗的实现是时代的大趋势。而【深度估计】又是什么呢，简单来说，就是利用一张或唯一、\n多个视角下的RGB图像，估计图像中每个像素相对拍摄源的距离。为什么我们想研究深度估计呢，因为它\n是一个很实用， 能给人们带来便利的东西。 例如， 在科技发展能通过CT等手段拍摄出人体内部图像的今天，\n一个能做到对图像的准确深度估计很有可能让医生做出更加合理的诊疗方案，甚至对病症部位的准确估计\n能极大的提高一场手术的成功率与成功度。例如肿瘤切除手术中通过深度估计技术的运用来对肿瘤位置进\n行准确判断便能让医生做出更合理，有针对性的诊疗方案，减少失败风险从而提高整个手术的成功率。    \n于是乎，我们项目的主要内容就是对深度估计的学习与理解：单目深度估计的研究现状，基于线索或机器\n学习的传统方法，以及基于无监督的深度学习方法，计算机视觉基础，相关网络模型，常用的损失函数，\n深度估计的评价方法以及指标等等\n   算法\n1.监督算法：以输入图像，输出深度图真实值作为训练。主要有回归和分类俩类。\n（1）线性回归算法\n线性回归算法通常用来构建一个预测模型。例如，根据房子年限、房屋面积这两个特性，来预测房屋\n价格，就可以构建一个线性回归算法。利用已有的数据训练模型，再用训练好的模型预测新的房价。\n如果有一个输入x， 对应一个输出y， 就可以构建一个一维现行回归模型， 通常表现为一条直线或取现，\n当给定一个x值时，对应可以求出y值。当有多个特征共同影响y值时，就可以构建一个多元的线性回归\n模型。\n（2）逻辑回归算法\n逻辑回归算法用来进行对数据的分类。它和线性回归算法类似，不同之处在于，输出值y是给定的几\n类。最常见的是分成两类，如好和坏。使用逻辑回归算法，可以根据输入数据的特征，判断该条数据的输\n出是哪一类的。\n2.无监督算法：仅使用俩个摄像机采集的双目图像数据进行联合训练。其中双目数据可彼此预测对方，\n从而获得相应的视差数据，在根据视差与深度的关系进行演化亦或是将双目图像中各个像素点的对应\n问题看作是立体匹配问题进行训练。因深度数据的获取难度高，所以大量算法都基于无监督模型。\n（1）Kmeans聚类\n聚类是一种非监督学习，它和分类的不同之处在于，分类是有标签的，而聚类是无标签的。分类的结果是\n知道哪个好哪个坏，而聚类是根据特性，将相似的事物聚集到一起，不考虑它们的好坏。\nKmeas算法是聚类算法中的一种，可以根据输入的特性，将一些数据聚集为成任意多个类别。Kmeas算法使用距离的远近来聚集一类数据。\n（2）降维\n有的时候，一条数据的输入特性可能有很多。比如，在预测房价的问题上，可能会输入房间面积、年限、\n地理位置相关信息等近百个属性。使用这么多属性进行分析，会为分析过程带来麻烦。降维，就是将这些D\n个维度的输入属性，缩小成d个维度的输入属性。它将一些输入特性合并或进行某些操作，来减少变量的\n数量。\n（3）异常检测\n异常检测用来判断某一个数据，其输入特性是否异常。例如，有多个属性来描述一个飞机的发动机，在大\n量的数据输入中，通过异常检测，来发现哪条数据记录存在异常。\n3.基于视频的深度估计：既包含了单帧视频的单目深度估计，也包含了多帧间视频帧的像素的立体匹配，\n从而近似获取多视角图像，对相机位姿进行估计。\n深度学习\n深度算法的研究离不开深度学习，特别是深度学习领域的最新进展，对医学图像的识别、分类和量化的发\n展起到了促进作用。\n深度学习正迅速成为一种最先进的学习方式，在各种医学应用场景中表现突出。\n在过去几十年中，如CT,MRI,PET等医学成像技术已被广泛用于疾病的早期发现,诊断和治疗。这些解释大\n多是由医生进行的，因为病理上的差异以及医生的潜在疲劳，所以效率不高。但随着机器学习技术的引进，\n这种情况被不断改善。\n深度学习取得成功离不开以下因素：\n1.高性能的CPU和GPU\n2.大数据的可行性\n3.算法的发展\n在大量的样本数量下，深度学习的方法是非常有效的但在医学应用中图像却成了问题，为克服图像过少的\n问题。研究人员设计了以下几种策略：\n1.将二维(2D)或三维(3D)图像块(image patches)而不是全尺寸图像作为输入，以降低输入维数并减少\n模型构建所需参数；\n2.通过仿射变换人工生成样本(即数据增强)来扩展数据集，然后用增强的数据集从头开始训练它们的网\n络；\n3.使用在计算机视觉中通过对大量自然图像训练生成的深度模型作为“现成”特征提取器，用目标任务\n样本训练最终分类器或输出层；\n4.使用来自非医学或自然图像的预训练模型的模型参数初始化模型参数，然后用与任务有关的样本微调\n网络参数；\n5.通过将全连接层中的权重转换为卷积核，对任意大小的输入使用用小尺寸图像作为输入训练出的模型\n我们研究的基于视频深度估计则是研究显微镜的内外参数以及基于显微镜单目图像进行深度估计（模型，猪眼实验，病人手术图像及视频）\n由于双目图像需要利用立体匹配进行像素点对应和视差计算，所以计算复杂度也较高，尤其是对于低纹理\n场景的匹配效果不好。相较于双目，单目深度估计成本更低也跟容易普及。\n单目深度估计即利用一张或唯一视角下的RGB图像，估计图像中每个像素相对拍摄源的距离。通过人眼需\n要从图像中提取出大量深度信息的例子得出，单目深度估计不仅需要从二维图像中学会客观的深度信息，\n还要提取一些经验信息。\n完善\n我们认为单目深度估计的发展仍将集中在提高精度，可传输性和实时性上\n精度：新的函数或网路框架。更有效的几何约束来改进无监督算法\n可传输性：同一网络在不同摄像机，场景和数据集的性能。但因为领域或摄像机的不同可能会导致性能的\n下降\n实时性：更快的推理速度，保证实时性的条件下提高精度\n思考\n‘深度估计’，顾名思义，它是一种估计，只要是估计就一定会有误差，而我们要做的就是通过手段不断地\n去缩小这个误差。那么为了实现这个目的就需要找到RGB图和深度图像之间存在的映射关系，并以此设计\n合理的算法。介于刚读大一知识面有限，在三种常见算法方向（基于几何，基于传感器，基于深度学习）\n中我们选择了在课堂上有所了解的深度学习者一方向来继续深入学习研究。那么是不是可以采用CNN来进\n行各种医学图像的训练，包括放射线医学，病理学，皮肤病学和眼科医学等数据，并通过BP反馈来不断调\n整优化算法函数呢？\n深度估计的应用是否能运用到其他领域呢?例如无人驾驶汽车，海底探测等等\n"}, {"role": "assistant", "content": "最终打分：7.6 (范围0-10分)\n1. 结构完整性得分：7, 占比20%\n2. 逻辑清晰度得分：7, 占比20%\n3. 语言连贯性得分：8, 占比20%\n4. 内容独特性和创新性得分：7, 占比20%\n5. 参考文献规范性得分：10, 占比10%\n6. 课程知识掌握度得分：8, 占比10%\n"}]}
